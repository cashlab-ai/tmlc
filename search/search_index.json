{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Text Multi-label classification project","text":"<p>This site provides documentation for the template Text Multi-label classification project. Use the navigation links on the left to browse the documentation.</p>"},{"location":"#about-the-project","title":"About the Project","text":"<p>Multi-label classification is a machine learning method where each data point can have multiple labels or categories. In compliance message contexts, it can classify messages by compliance topics like financial regulations, data privacy laws, or ethical guidelines.</p> <p>To create a multi-label classification model, techniques such as binary relevance, classifier chains, or label powerset can be used. The training process involves preprocessing text data into numerical formats like word embeddings or bag-of-words representations. Machine learning algorithms like logistic regression, random forests, or neural networks can then be applied to train the model.</p> <p>Once trained, the model can predict compliance topics for new messages, outputting a list of predicted labels based on the message's features. These labels can help prioritize and triage messages based on compliance risk or severity. Multi-label classification can thus aid compliance teams in automating classification and triage processes, enhancing efficiency and accuracy while reducing the risk of violations.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>For more detailed instructions, see the Quickstart page.</p>"},{"location":"installation/","title":"Installation","text":"<p>This guide provides the steps to install the TextMultiLabelClassificationModel project, which is a Poetry project. Poetry is a tool for dependency management and packaging in Python.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before you start, make sure you have Python installed on your system. You also need to install Poetry if you haven't already. You can install Poetry by following the instructions provided in the official documentation.</p> <p>Steps to install the project</p> <ol> <li>Clone the TextMultiLabelClassificationModel repository from Git:</li> </ol> <pre><code>git clone https://&lt;&gt;/TextMultiLabelClassificationModel.git\n</code></pre> <p>Replace yourusername with the appropriate username.</p> <ol> <li>Change to the project directory:</li> </ol> <pre><code>cd TextMultiLabelClassificationModel\n</code></pre> <ol> <li>Install the project dependencies using Poetry:</li> </ol> <pre><code>poetry install\n</code></pre> <p>This command will create a virtual environment and install the necessary dependencies.</p> <ol> <li>Activate the virtual environment:</li> </ol> <pre><code>poetry shell\n</code></pre> <p>Congratulations! The TextMultiLabelClassificationModel project is now installed and ready for use.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This quickstart guide provides an overview of the basic usage of the TextMultiLabelClassificationModel project.</p>"},{"location":"quickstart/#train-a-model","title":"Train a model","text":"<p>To train a model, you need to create a configuration file (in YAML format) with the necessary hyperparameters and settings. You can find an example configuration file in the model documentation.</p> <p>After creating the configuration file, you can run the training script:</p> <pre><code>python train.py --file-path path/to/your/config.yaml\n</code></pre> <p>Replace path/to/your/config.yaml with the path to your configuration file.</p>"},{"location":"quickstart/#evaluate-a-model","title":"Evaluate a model","text":"<p>To evaluate a model, you can use the scoring script. First, you need to register the trained model with MLflow. You can find the model name and version in the training script's output.</p> <p>Run the scoring script with the registered model name and version:</p> <pre><code>python score.py --model-name &lt;registered_model_name&gt; --version &lt;model_version&gt; --texts \"Text1|Text2|Text3\"\n</code></pre> <p>Replace  with the name of the registered model and  with the desired model version. The input texts should be separated by the '|' character."},{"location":"quickstart/#next-steps","title":"Next steps","text":"<p>For more detailed information on the project components and usage, refer to the user guides provided in the documentation:</p> <p>Introduction Dataset Requirements Model Training Model Evaluation Model Wrapper</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>app<ul> <li>app</li> <li>schemas</li> </ul> </li> <li>components<ul> <li>calculate_best_thresholds</li> <li>get_data</li> <li>load_data</li> <li>loss</li> <li>metrics</li> <li>predictions</li> <li>process_data</li> <li>split_data</li> </ul> </li> <li>configclasses<ul> <li>data_module_config</li> <li>dataset_config</li> <li>lightning_module_config</li> <li>mlflow_config</li> <li>model_config</li> <li>partial_function_config</li> <li>pretrained_config</li> <li>tokenizer_config</li> <li>trainer_config</li> </ul> </li> <li>dataclasses<ul> <li>datamodule</li> <li>dataset</li> <li>message</li> </ul> </li> <li>eda<ul> <li>configclasses</li> <li>datapreparation</li> <li>pretraining</li> <li>render</li> <li>visualization</li> </ul> </li> <li>exceptions</li> <li>scripts<ul> <li>eda</li> <li>score</li> <li>train</li> <li>utils</li> </ul> </li> </ul>"},{"location":"reference/exceptions/","title":"exceptions","text":""},{"location":"reference/exceptions/#tmlc.exceptions.PartialFunctionError","title":"<code>PartialFunctionError</code>","text":"<p>         Bases: <code>ValueError</code></p> <p>An exception is raised when there is an error loading a partial function.</p> Source code in <code>tmlc/exceptions.py</code> <pre><code>class PartialFunctionError(ValueError):\n\"\"\"\n    An exception is raised when there is an error loading a partial function.\n    \"\"\"\n</code></pre>"},{"location":"reference/app/","title":"app","text":""},{"location":"reference/app/app/","title":"app","text":""},{"location":"reference/app/app/#tmlc.app.app.get_metric","title":"<code>get_metric(request)</code>","text":"<p>Returns the value of a specified metric for a specified model run.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>MetricRequest</code> <p>An instance of the MetricRequest model containing the request parameters.</p> required <p>Returns:</p> Name Type Description <code>MetricResponse</code> <p>An instance of the MetricResponse model containing the response data.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the model or metric is not found, or there is an error with retrieving the metric value.</p> Source code in <code>tmlc/app/app.py</code> <pre><code>@app.get(\"/metric\", response_model=MetricResponse)\ndef get_metric(request: MetricRequest):\n\"\"\"\n    Returns the value of a specified metric for a specified model run.\n\n    Args:\n        request (MetricRequest): An instance of the MetricRequest model\n            containing the request parameters.\n\n    Returns:\n        MetricResponse: An instance of the MetricResponse model containing\n            the response data.\n\n    Raises:\n        HTTPException: If the model or metric is not found, or there is an error\n            with retrieving the metric value.\n    \"\"\"\n    # Load the run\n    run = _get_run(request.model_name, request.run_id, request.experiment_id)\n    if not run:\n        raise HTTPException(status_code=404, detail=\"Run not found\")\n\n    # Get the metric value\n    try:\n        metric = run.data.metrics[request.metric_name]\n    except KeyError:\n        raise HTTPException(status_code=404, detail=f\"Metric '{request.metric_name}' not found\")\n\n    # Return the response data\n    return MetricResponse(\n        model_name=request.model_name,\n        metric_name=request.metric_name,\n        metric_value=metric,\n        run_id=run.info.run_id,\n    )\n</code></pre>"},{"location":"reference/app/app/#tmlc.app.app.get_metrics","title":"<code>get_metrics(request)</code>","text":"<p>Returns the value of the specified metric for the specified model run.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the registered model to retrieve metric for.</p> required <code>run_id</code> <code>str</code> <p>The ID of the run to retrieve metric from. If not provided, searches for the latest run for the model.</p> required <code>experiment_id</code> <code>str</code> <p>The ID of the experiment to search for the run in. If not provided, searches across all experiments.</p> required <p>Returns:</p> Name Type Description <code>MetricsResponse</code> <p>A response object containing the model name, metrics dictionary, and run ID.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the model or metric is not found, or there is an error with retrieving the metric value.</p> Source code in <code>tmlc/app/app.py</code> <pre><code>@app.get(\"/metrics\", response_model=MetricsResponse)\ndef get_metrics(request: ModelLocation):\n\"\"\"\n    Returns the value of the specified metric for the specified model run.\n\n    Args:\n        model_name (str): The name of the registered model to retrieve metric for.\n        run_id (str, optional): The ID of the run to retrieve metric from. If not provided,\n            searches for the latest run for the model.\n        experiment_id (str, optional): The ID of the experiment to search for the run in.\n            If not provided, searches across all experiments.\n\n    Returns:\n        MetricsResponse: A response object containing the model name,\n            metrics dictionary, and run ID.\n\n    Raises:\n        HTTPException: If the model or metric is not found,\n            or there is an error with retrieving the metric value.\n    \"\"\"\n    # Load the run\n    run = _get_run(request.model_name, request.run_id, request.experiment_id)\n    if not run:\n        raise HTTPException(status_code=404, detail=\"Run not found\")\n\n    return MetricsResponse(model_name=request.model_name, metrics=run.data.metrics, run_id=run.info.run_id)\n</code></pre>"},{"location":"reference/app/app/#tmlc.app.app.get_model_location","title":"<code>get_model_location(model_name)</code>","text":"<p>Returns the location information of the specified model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the registered model to retrieve location for.</p> required <p>Returns:</p> Name Type Description <code>ModelLocation</code> <p>A Pydantic model containing the model name, run ID, and experiment ID.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the model is not found.</p> Source code in <code>tmlc/app/app.py</code> <pre><code>@app.get(\"/model/{model_name}\", response_model=ModelLocation)\ndef get_model_location(model_name: str):\n\"\"\"\n    Returns the location information of the specified model.\n\n    Args:\n        model_name (str): The name of the registered model to retrieve location for.\n\n    Returns:\n        ModelLocation: A Pydantic model containing the model name, run ID,\n            and experiment ID.\n\n    Raises:\n        HTTPException: If the model is not found.\n    \"\"\"\n    # Load the model\n    model = _get_model(model_name)\n    # Get the run and experiment information\n    experiment_id = model._metadata.experiment_id\n    run_id = model._metadata.run_id\n\n    return ModelLocation(model_name=model_name, run_id=run_id, experiment_id=experiment_id)\n</code></pre>"},{"location":"reference/app/app/#tmlc.app.app.health","title":"<code>health()</code>","text":"<p>Checks if MLflow is available and returns a status response.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the status response.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If MLflow is not available.</p> Source code in <code>tmlc/app/app.py</code> <pre><code>@app.get(\"/health\")\ndef health():\n\"\"\"\n    Checks if MLflow is available and returns a status response.\n\n    Returns:\n        dict: A dictionary containing the status response.\n\n    Raises:\n        HTTPException: If MLflow is not available.\n    \"\"\"\n    # Check if MLflow is available\n    try:\n        mlflow.get_tracking_uri()\n    except Exception as e:\n        app.logger.error(e)\n        raise HTTPException(status_code=500, detail=\"MLflow not available\")\n\n    return {\"status\": \"ok\"}\n</code></pre>"},{"location":"reference/app/app/#tmlc.app.app.logits","title":"<code>logits(request)</code>","text":"<p>Predicts the probabilities of the input texts for each class label using the specified model.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>LogitsRequest</code> <p>The request body containing the model name, input texts, and optional model version.</p> required <p>Returns:</p> Name Type Description <code>LogitsResponse</code> <code>List[str]</code> <p>The predicted logits for each input text as a list of strings.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the input parameters are invalid or there is an error with loading the model or making predictions.</p> Source code in <code>tmlc/app/app.py</code> <pre><code>@app.post(\"/logits\", response_model=LogitsResponse, responses={500: {\"description\": \"Failed to make logits\"}})\ndef logits(request: LogitsRequest) -&gt; List[str]:\n\"\"\"\n    Predicts the probabilities of the input texts for each class label using the specified model.\n\n    Args:\n        request (LogitsRequest): The request body containing the model name,\n            input texts, and optional model version.\n\n    Returns:\n        LogitsResponse: The predicted logits for each input text as a list of strings.\n\n    Raises:\n        HTTPException: If the input parameters are invalid or there is an error\n            with loading the model or making predictions.\n    \"\"\"\n\n    model = _get_model(request.model_name, request.model_version)\n\n    # Split the input texts by the '|' character\n    messages = request.texts.split(\"|\")\n\n    # Make logits\n    try:\n        logits = model.predict_logits(messages)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=\"Failed to make logits\") from e\n\n    # Log the request and response\n    app.logger.info(f\"Returned response: logits={logits}\")\n    # Return predictions as JSON response\n    return LogitsResponse(logits=logits)\n</code></pre>"},{"location":"reference/app/app/#tmlc.app.app.predict","title":"<code>predict(request)</code>","text":"<p>Predicts the class labels of the input texts using the specified model.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>PredictRequest</code> <p>The request body containing the model name, input texts, and optional model version.</p> required <p>Returns:</p> Name Type Description <code>PredictResponse</code> <code>List[str]</code> <p>The predicted class labels for each input text as a list of strings.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the input parameters are invalid or there is an error with loading the model or making predictions.</p> Source code in <code>tmlc/app/app.py</code> <pre><code>@app.post(\n    \"/predict\", response_model=PredictResponse, responses={500: {\"description\": \"Failed to make predictions\"}}\n)\ndef predict(request: PredictRequest) -&gt; List[str]:\n\"\"\"\n    Predicts the class labels of the input texts using the specified model.\n\n    Args:\n        request (PredictRequest): The request body containing the model name,\n            input texts, and optional model version.\n\n    Returns:\n        PredictResponse: The predicted class labels for each input text as a list of strings.\n\n    Raises:\n        HTTPException: If the input parameters are invalid or there is an error with loading\n            the model or making predictions.\n    \"\"\"\n\n    model = _get_model(request.model_name, request.model_version)\n\n    # Split the input texts by the '|' character\n    messages = request.texts.split(\"|\")\n\n    # Make predictions\n    try:\n        predictions = model.predict(messages)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=\"Failed to make predictions\") from e\n\n    # Log the request and response\n    app.logger.info(f\"Returned response: predictions={predictions}\")\n    # Return predictions as JSON response\n    return predictions\n</code></pre>"},{"location":"reference/app/schemas/","title":"schemas","text":""},{"location":"reference/app/schemas/#tmlc.app.schemas.LogitsRequest","title":"<code>LogitsRequest</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Request model for getting logits from a model.</p> Source code in <code>tmlc/app/schemas.py</code> <pre><code>class LogitsRequest(BaseModel):\n\"\"\"\n    Request model for getting logits from a model.\n    \"\"\"\n\n    model_name: str = Field(..., example=\"my_model\")\n    texts: str = Field(..., example=\"example text\")\n    model_version: Optional[int] = Field(None, example=1)\n</code></pre>"},{"location":"reference/app/schemas/#tmlc.app.schemas.LogitsResponse","title":"<code>LogitsResponse</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response model for getting logits from a model.</p> Source code in <code>tmlc/app/schemas.py</code> <pre><code>class LogitsResponse(BaseModel):\n\"\"\"\n    Response model for getting logits from a model.\n    \"\"\"\n\n    logits: List[float] = Field(..., example=[\"0.5\", \"0.7\", \"0.2\"])\n</code></pre>"},{"location":"reference/app/schemas/#tmlc.app.schemas.MetricRequest","title":"<code>MetricRequest</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Request model for getting a specific metric for a model run.</p> Source code in <code>tmlc/app/schemas.py</code> <pre><code>class MetricRequest(BaseModel):\n\"\"\"\n    Request model for getting a specific metric for a model run.\n    \"\"\"\n\n    model_name: str = Field(..., example=\"my_model\")\n    metric_name: str = Field(..., example=\"accuracy\")\n    run_id: Optional[str] = Field(None, example=\"run_123\")\n    experiment_id: Optional[str] = Field(None, example=\"exp_456\")\n</code></pre>"},{"location":"reference/app/schemas/#tmlc.app.schemas.MetricResponse","title":"<code>MetricResponse</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response model for getting a specific metric for a model run.</p> Source code in <code>tmlc/app/schemas.py</code> <pre><code>class MetricResponse(BaseModel):\n\"\"\"\n    Response model for getting a specific metric for a model run.\n    \"\"\"\n\n    model_name: str = Field(..., example=\"my_model\")\n    metric_name: str = Field(..., example=\"f1_score\")\n    metric_value: float = Field(..., example=0.92)\n    run_id: str = Field(..., example=\"run_123\")\n</code></pre>"},{"location":"reference/app/schemas/#tmlc.app.schemas.MetricsResponse","title":"<code>MetricsResponse</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response model for getting all metrics for a model run.</p> Source code in <code>tmlc/app/schemas.py</code> <pre><code>class MetricsResponse(BaseModel):\n\"\"\"\n    Response model for getting all metrics for a model run.\n    \"\"\"\n\n    model_name: str = Field(..., example=\"my_model\")\n    metrics: dict = Field(..., example={\"accuracy\": 0.95, \"f1_score\": 0.92})\n    run_id: str = Field(..., example=\"run_123\")\n</code></pre>"},{"location":"reference/app/schemas/#tmlc.app.schemas.ModelLocation","title":"<code>ModelLocation</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Request model for getting the location of a model.</p> Source code in <code>tmlc/app/schemas.py</code> <pre><code>class ModelLocation(BaseModel):\n\"\"\"\n    Request model for getting the location of a model.\n    \"\"\"\n\n    model_name: str = Field(..., example=\"my_model\")\n    run_id: str = Field(..., example=\"run_123\")\n    experiment_id: str = Field(..., example=\"exp_456\")\n</code></pre>"},{"location":"reference/app/schemas/#tmlc.app.schemas.PredictRequest","title":"<code>PredictRequest</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Request model for making predictions using a model.</p> Source code in <code>tmlc/app/schemas.py</code> <pre><code>class PredictRequest(BaseModel):\n\"\"\"\n    Request model for making predictions using a model.\n    \"\"\"\n\n    model_name: str = Field(..., example=\"my_model\")\n    texts: str = Field(..., example=\"example text\")\n    model_version: Optional[int] = Field(None, example=1)\n</code></pre>"},{"location":"reference/app/schemas/#tmlc.app.schemas.PredictResponse","title":"<code>PredictResponse</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response model for making predictions using a model.</p> Source code in <code>tmlc/app/schemas.py</code> <pre><code>class PredictResponse(BaseModel):\n\"\"\"\n    Response model for making predictions using a model.\n    \"\"\"\n\n    predictions: List[str] = Field(..., example=[True, False, True])\n</code></pre>"},{"location":"reference/components/","title":"components","text":""},{"location":"reference/components/calculate_best_thresholds/","title":"calculate_best_thresholds","text":""},{"location":"reference/components/calculate_best_thresholds/#tmlc.components.calculate_best_thresholds.calculate_best_thresholds","title":"<code>calculate_best_thresholds(probabilities, labels, vmin, vmax, step, metric=None, maximize=True)</code>","text":"<p>Calculate the optimal threshold for each class based on predicted probabilities and true labels.</p> <p>Parameters:</p> Name Type Description Default <code>probabilities</code> <code>torch.Tensor</code> <p>Tensor of predicted probabilities for each class, of shape (n_samples, num_classes).</p> required <code>labels</code> <code>torch.Tensor</code> <p>Tensor of true labels, of shape (n_samples, num_classes).</p> required <code>vmin</code> <code>float</code> <p>The minimum threshold value to try. Must be within the range (0, 1).</p> required <code>vmax</code> <code>float</code> <p>The maximum threshold value to try. Must be within the range (0, 1).</p> required <code>step</code> <code>float</code> <p>The step size to use between vmin and vmax.</p> required <code>metric</code> <code>Optional[Callable[[torch.Tensor, torch.Tensor], float]]</code> <p>The scoring metric to use when evaluating the performance of each threshold. Must take two arguments: the true labels and the predicted labels. Defaults to the F1 score metric from the TorchMetrics library.</p> <code>None</code> <code>maximize</code> <code>bool</code> <p>If True, the metric is maximized; otherwise, it is minimized.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If vmin or vmax are not within the range (0, 1), or if vmax &lt;= vmin.</p> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>torch.Tensor: A tensor of the optimal threshold values for each class, in order.</p> <p>Calculates the optimal threshold for each class given the predicted probabilities and true labels, by evaluating the performance of each threshold using the specified metric.</p> <p>The function returns a torch tensor containing the optimal threshold value for each class, in the order they appear in the input.</p> Example <p>probs = torch.Tensor([[0.6, 0.4], [0.7, 0.3], [0.8, 0.2]]) labels = torch.Tensor([[1, 0], [0, 1], [1, 1]]) thresholds = calculate_best_thresholds(probs, labels, 0.1, 0.9, 0.1) print(thresholds) tensor([0.5000, 0.2000])</p> <p>In this example, there are 3 samples and 2 classes. The predicted probabilities for the first class are [0.6, 0.7, 0.8], while for the second class they are [0.4, 0.3, 0.2]. The true labels are [1, 0] for the first sample, [0, 1] for the second sample, and [1, 1] for the third sample. The function is called with vmin=0.1, vmax=0.9, step=0.1, and the default F1 score metric from the TorchMetrics library. The function returns the optimal threshold for the first class as 0.5, and for the second class as 0.2.</p> Source code in <code>tmlc/components/calculate_best_thresholds.py</code> <pre><code>def calculate_best_thresholds(\n    probabilities: torch.Tensor,\n    labels: torch.Tensor,\n    vmin: float,\n    vmax: float,\n    step: float,\n    metric: Optional[Callable[[torch.Tensor, torch.Tensor], float]] = None,\n    maximize: bool = True,\n) -&gt; torch.Tensor:\n\"\"\"\n    Calculate the optimal threshold for each class based on predicted probabilities and true labels.\n\n    Args:\n        probabilities (torch.Tensor): Tensor of predicted probabilities for each class, of shape\n            (n_samples, num_classes).\n        labels (torch.Tensor): Tensor of true labels, of shape (n_samples, num_classes).\n        vmin (float): The minimum threshold value to try. Must be within the range (0, 1).\n        vmax (float): The maximum threshold value to try. Must be within the range (0, 1).\n        step (float): The step size to use between vmin and vmax.\n        metric (Optional[Callable[[torch.Tensor, torch.Tensor], float]]):\n            The scoring metric to use when evaluating the performance of\n            each threshold. Must take two arguments:\n            the true labels and the predicted labels. Defaults to the F1 score metric\n            from the TorchMetrics library.\n        maximize (bool): If True, the metric is maximized; otherwise, it is minimized.\n\n    Raises:\n        ValueError: If vmin or vmax are not within the range (0, 1), or if vmax &lt;= vmin.\n\n    Returns:\n        torch.Tensor: A tensor of the optimal threshold values for each class, in order.\n\n    Calculates the optimal threshold for each class given the predicted probabilities and true\n    labels, by evaluating the performance of each threshold using the specified metric.\n\n    The function returns a torch tensor containing the optimal threshold value for each class,\n    in the order they appear in the input.\n\n    Example:\n        &gt;&gt;&gt; probs = torch.Tensor([[0.6, 0.4], [0.7, 0.3], [0.8, 0.2]])\n        &gt;&gt;&gt; labels = torch.Tensor([[1, 0], [0, 1], [1, 1]])\n        &gt;&gt;&gt; thresholds = calculate_best_thresholds(probs, labels, 0.1, 0.9, 0.1)\n        &gt;&gt;&gt; print(thresholds)\n        tensor([0.5000, 0.2000])\n\n    In this example, there are 3 samples and 2 classes. The predicted probabilities for the\n    first class are [0.6, 0.7, 0.8], while for the second class they are [0.4, 0.3, 0.2].\n    The true labels are [1, 0] for the first sample, [0, 1] for the second sample, and [1, 1]\n    for the third sample. The function is called with vmin=0.1, vmax=0.9, step=0.1, and\n    the default F1 score metric from the TorchMetrics library. The function returns the\n    optimal threshold for the first class as 0.5, and for the second class as 0.2.\n    \"\"\"\n\n    metric = metric or torchmetrics.classification.BinaryF1Score()\n\n    # Validate input\n    if not 0 &lt; vmin &lt; 1:\n        raise ValueError(f\"vmin must be within the range (0, 1), but got {vmin}\")\n    if not 0 &lt; vmax &lt; 1:\n        raise ValueError(f\"vmax must be within the range (0, 1), but got {vmax}\")\n    if vmax &lt;= vmin:\n        raise ValueError(f\"vmax ({vmax}) must be greater than vmin ({vmin})\")\n\n    # Find the optimal threshold for each class\n    _, num_classes = probabilities.shape\n    best_thresholds = torch.zeros(num_classes)\n\n    if maximize:\n        selection = torch.argmax\n    else:\n        selection = torch.argmin\n\n    for i in range(num_classes):\n        scores = []\n        thresholds = torch.arange(vmin, vmax, step)\n        for threshold in thresholds:\n            predictions = (probabilities[:, i] &gt; threshold).int()\n            score = metric(labels[:, i], predictions)\n            scores.append(score)\n        best_thresholds[i] = thresholds[selection(torch.tensor(scores))]\n\n    return best_thresholds\n</code></pre>"},{"location":"reference/components/get_data/","title":"get_data","text":""},{"location":"reference/components/get_data/#tmlc.components.get_data.get_data","title":"<code>get_data(file_path)</code>","text":"<p>Load data from a file and preprocess it.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file containing the data.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: Containing the raw data.</p> Source code in <code>tmlc/components/get_data.py</code> <pre><code>def get_data(file_path: str) -&gt; pd.DataFrame:\n\"\"\"\n    Load data from a file and preprocess it.\n\n    Args:\n        file_path (str): The path to the file containing the data.\n\n    Returns:\n        pd.DataFrame: Containing the raw data.\n    \"\"\"\n\n    return pd.read_csv(file_path)\n</code></pre>"},{"location":"reference/components/load_data/","title":"load_data","text":""},{"location":"reference/components/load_data/#tmlc.components.load_data.load_data","title":"<code>load_data(file_path)</code>","text":"<p>Load data from a file for the datamodule.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file containing the data.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: Containing the raw data.</p> Source code in <code>tmlc/components/load_data.py</code> <pre><code>def load_data(file_path: str) -&gt; pd.DataFrame:\n\"\"\"\n    Load data from a file for the datamodule.\n\n    Args:\n        file_path (str): The path to the file containing the data.\n\n    Returns:\n        pd.DataFrame: Containing the raw data.\n    \"\"\"\n\n    return pd.read_csv(file_path)\n</code></pre>"},{"location":"reference/components/loss/","title":"loss","text":""},{"location":"reference/components/loss/#tmlc.components.loss.bceloss_inverse_frequency_weighted","title":"<code>bceloss_inverse_frequency_weighted(labels)</code>","text":"<p>Compute a weighted binary cross-entropy loss based on the frequency of each class.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>torch.Tensor</code> <p>A tensor of shape (batch_size, num_classes) representing the true labels.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>torch.Tensor: A tensor representing the binary cross-entropy loss.</p> <p>The function first counts the number of samples in each class, then calculates the frequency of each class in the dataset. The weight for each class is then calculated as the inverse of the class frequency, where the frequency is computed as follows: (number of samples in the class + 1) / (total number of samples in the dataset + 1). Adding 1 to both the numerator and denominator is a form of smoothing that avoids division by zero errors in case a class has zero samples. In this case, we are assuming that the class has appeared once in the dataset. Finally, the class weights are converted to a tensor and passed as the <code>pos_weight</code> argument to the <code>torch.nn.BCEWithLogitsLoss</code> function, which computes the binary cross-entropy loss.</p> Example <p>import torch from my_module import bceloss_inverse_frequency_weighted labels = torch.tensor([[1, 1, 0], [0, 1, 1], [1, 1, 0], [0, 0, 1]]) loss_fn = bceloss_inverse_frequency_weighted(labels) loss = loss_fn(logits, labels)</p> <p>In this example, the input <code>labels</code> has shape (4, 3) and contains 4 samples with 3 classes each. The first class appears twice, the second class appears three times, and the third class appears twice. The frequency of each class is [0.5, 0.75, 0.5], calculated as (2 + 1) / (4 + 1), (3 + 1) / (4 + 1), and (2 + 1) / (4 + 1), respectively. The weight for each class is [1.3333, 1.25, 1.3333], calculated as 1.0 / 0.6, 1.0 / 0.8, and 1.0 / 0.6, respectively. These weights are then passed as the <code>pos_weight</code> argument to the <code>torch.nn.BCEWithLogitsLoss</code> function, which computes the binary cross-entropy loss.</p> Source code in <code>tmlc/components/loss.py</code> <pre><code>def bceloss_inverse_frequency_weighted(labels: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"\n    Compute a weighted binary cross-entropy loss based on the frequency of each class.\n\n    Args:\n        labels (torch.Tensor): A tensor of shape (batch_size, num_classes) representing the true labels.\n\n    Returns:\n        torch.Tensor: A tensor representing the binary cross-entropy loss.\n\n    The function first counts the number of samples in each class, then calculates the frequency\n    of each class in the dataset. The weight for each class is then calculated as the inverse of\n    the class frequency, where the frequency is computed as follows:\n    (number of samples in the class + 1) / (total number of samples in the dataset + 1).\n    Adding 1 to both the numerator and denominator is a form of smoothing that avoids division\n    by zero errors in case a class has zero samples. In this case, we are assuming that the\n    class has appeared once in the dataset.\n    Finally, the class weights are converted to a tensor and passed as the `pos_weight` argument\n    to the `torch.nn.BCEWithLogitsLoss` function, which computes the binary cross-entropy loss.\n\n    Example:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from my_module import bceloss_inverse_frequency_weighted\n        &gt;&gt;&gt; labels = torch.tensor([[1, 1, 0], [0, 1, 1], [1, 1, 0], [0, 0, 1]])\n        &gt;&gt;&gt; loss_fn = bceloss_inverse_frequency_weighted(labels)\n        &gt;&gt;&gt; loss = loss_fn(logits, labels)\n    In this example, the input `labels` has shape (4, 3) and contains 4 samples with 3 classes each.\n    The first class appears twice, the second class appears three times, and the third class appears\n    twice. The frequency of each class is [0.5, 0.75, 0.5], calculated as (2 + 1) / (4 + 1),\n    (3 + 1) / (4 + 1), and (2 + 1) / (4 + 1), respectively.\n    The weight for each class is [1.3333, 1.25, 1.3333], calculated as 1.0 / 0.6, 1.0 / 0.8,\n    and 1.0 / 0.6, respectively.\n    These weights are then passed as the `pos_weight` argument to the `torch.nn.BCEWithLogitsLoss`\n    function, which computes the binary cross-entropy loss.\n    \"\"\"\n\n    # count the number of samples in each class\n    class_count = torch.sum(labels, axis=0)\n\n    # calculate the total number of samples\n    num_samples, _ = labels.shape\n\n    # calculate the frequency of each class in the dataset\n    class_freq = (class_count + 1) / (num_samples + 1)\n\n    # calculate the weight for each class\n    class_weights = 1.0 / class_freq\n\n    # convert the class_weights to a tensor\n    class_weights = torch.as_tensor(class_weights, dtype=torch.float32)\n\n    return torch.nn.BCEWithLogitsLoss(pos_weight=class_weights)\n</code></pre>"},{"location":"reference/components/metrics/","title":"metrics","text":""},{"location":"reference/components/metrics/#tmlc.components.metrics.bootstrap_rates","title":"<code>bootstrap_rates(y_true, y_pred, n_iterations=1000, percentile=2.5)</code>","text":"<p>Estimate average and error bars for true positive rate (tpr), false positive rate (fpr), false negative rate (fnr), and true negative rate (tnr) using bootstrapping.</p> <p>Bootstrapping is a resampling technique that estimates the sampling distribution of an estimator by generating multiple \"bootstrap samples\" from the original data. In this function, we generate multiple bootstrap samples of the true labels and predicted labels, and then calculate the tpr, fpr, fnr, and tnr for each sample. Finally, we estimate the average and error bars for each rate using the bootstrap samples.</p> <p>To generate a bootstrap sample, we randomly sample the original data with replacement, resulting in a new dataset of the same size as the original, but with some samples potentially repeated and others left out. By repeatedly generating these bootstrap samples, we can estimate the distribution of the estimator, and use this distribution to estimate the average and error bars.</p> <p>The benefit of bootstrapping is that it allows us to estimate the distribution of an estimator without making assumptions about the underlying distribution of the data. This can be particularly useful in cases where the sample size is small or the underlying distribution is unknown.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like</code> <p>True labels, of shape (n_samples,)</p> required <code>y_pred</code> <code>array-like</code> <p>Predicted labels, of shape (n_samples,)</p> required <code>n_iterations</code> <code>int</code> <p>Number of bootstrap iterations.</p> <code>1000</code> <code>percentile</code> <code>float</code> <p>Percentile for error bar calculation.</p> <code>2.5</code> <p>Returns:</p> Type Description <p>Dict[str, Dict[str, float]]: A dictionary containing the average values and error bars</p> <p>for tpr, fpr, fnr, tnr, n_samples.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If percentile is outside the range of (0, 100).</p> Example <p>For single-label classification:</p> <p>import numpy as np np.random.seed(123) y_true = np.random.randint(0, 2, size=100) y_pred = np.random.rand(100) results = bootstrap_rates(y_true, y_pred) set(results.keys()) {'tpr', 'fpr', 'fnr', 'tnr'} results['tpr'].keys() dict_keys(['avg', 'error_bars']) results'tpr' 0.4734470264099684 results'tpr' (0.36764705882352944, 0.5789473684210527)</p> <p>For multi-label classification:</p> <p>import numpy as np np.random.seed(123) y_true = np.random.randint(0, 2, size=(100, 3)) y_pred = np.random.rand(100, 3) results = bootstrap_rates(y_true, y_pred) set(results.keys()) {'tpr', 'fpr', 'fnr', 'tnr'} results['tpr'].keys() dict_keys(['avg', 'error_bars']) results'tpr' array([0.47186573, 0.42935668, 0.44356321]) results'tpr' (array([0.3761165 , 0.31934306, 0.33100715]),  array([0.57416268, 0.5308642 , 0.56390977]))</p> Source code in <code>tmlc/components/metrics.py</code> <pre><code>def bootstrap_rates(y_true, y_pred, n_iterations=1000, percentile=2.5):\n\"\"\"\n    Estimate average and error bars for true positive rate (tpr), false positive rate (fpr), false\n    negative rate (fnr), and true negative rate (tnr) using bootstrapping.\n\n    Bootstrapping is a resampling technique that estimates the sampling distribution of an estimator\n    by generating multiple \"bootstrap samples\" from the original data. In this function, we generate\n    multiple bootstrap samples of the true labels and predicted labels, and then calculate the tpr,\n    fpr, fnr, and tnr for each sample. Finally, we estimate the average and error bars for each rate\n    using the bootstrap samples.\n\n    To generate a bootstrap sample, we randomly sample the original data with replacement, resulting\n    in a new dataset of the same size as the original, but with some samples potentially repeated and\n    others left out. By repeatedly generating these bootstrap samples, we can estimate the distribution\n    of the estimator, and use this distribution to estimate the average and error bars.\n\n    The benefit of bootstrapping is that it allows us to estimate the distribution of an estimator without\n    making assumptions about the underlying distribution of the data. This can be particularly useful in\n    cases where the sample size is small or the underlying distribution is unknown.\n\n    Args:\n        y_true (array-like): True labels, of shape (n_samples,)\n        y_pred (array-like): Predicted labels, of shape (n_samples,)\n        n_iterations (int): Number of bootstrap iterations.\n        percentile (float): Percentile for error bar calculation.\n\n    Returns:\n        Dict[str, Dict[str, float]]: A dictionary containing the average values and error bars\n        for tpr, fpr, fnr, tnr, n_samples.\n\n    Raises:\n        ValueError: If percentile is outside the range of (0, 100).\n\n    Example:\n        For single-label classification:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; np.random.seed(123)\n        &gt;&gt;&gt; y_true = np.random.randint(0, 2, size=100)\n        &gt;&gt;&gt; y_pred = np.random.rand(100)\n        &gt;&gt;&gt; results = bootstrap_rates(y_true, y_pred)\n        &gt;&gt;&gt; set(results.keys())\n        {'tpr', 'fpr', 'fnr', 'tnr'}\n        &gt;&gt;&gt; results['tpr'].keys()\n        dict_keys(['avg', 'error_bars'])\n        &gt;&gt;&gt; results['tpr']['avg']\n        0.4734470264099684\n        &gt;&gt;&gt; results['tpr']['error_bars']\n        (0.36764705882352944, 0.5789473684210527)\n\n        For multi-label classification:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; np.random.seed(123)\n        &gt;&gt;&gt; y_true = np.random.randint(0, 2, size=(100, 3))\n        &gt;&gt;&gt; y_pred = np.random.rand(100, 3)\n        &gt;&gt;&gt; results = bootstrap_rates(y_true, y_pred)\n        &gt;&gt;&gt; set(results.keys())\n        {'tpr', 'fpr', 'fnr', 'tnr'}\n        &gt;&gt;&gt; results['tpr'].keys()\n        dict_keys(['avg', 'error_bars'])\n        &gt;&gt;&gt; results['tpr']['avg']\n        array([0.47186573, 0.42935668, 0.44356321])\n        &gt;&gt;&gt; results['tpr']['error_bars']\n        (array([0.3761165 , 0.31934306, 0.33100715]),\n         array([0.57416268, 0.5308642 , 0.56390977]))\n    \"\"\"\n    # Raise an error if percentile is outside the range of (0, 100).\n    if (percentile &gt;= 100) or (percentile &lt;= 0):\n        raise ValueError(f\"Percentile needs to be between (0, 100); it is {percentile}.\")\n\n    # Set the number of samples to the length of y_true.\n    n_samples = len(y_true)\n\n    # Initialize a dictionary to store errors for each rate.\n    errors = {\"tpr\": [], \"fpr\": [], \"fnr\": [], \"tnr\": []}\n\n    # Iterate over the number of bootstrap iterations.\n    for _ in range(n_iterations):\n\n        # Generate bootstrap samples of true labels and predicted labels.\n        idx = np.random.choice(n_samples, n_samples, replace=True)\n        y_true_bootstrap = y_true[idx]\n        y_pred_bootstrap = y_pred[idx]\n\n        # Evaluate rates for the bootstrap sample.\n        eval_bootstrap = evaluate_rates(y_true_bootstrap, y_pred_bootstrap)\n\n        # Append errors to the errors dictionary.\n        for key in errors.keys():\n            errors[key].extend(eval_bootstrap[key])\n\n    # Initialize a dictionary to store the output.\n    output = {}\n\n    # Calculate the average and error bars for each rate.\n    for key in errors.keys():\n        output[f\"{key}_avg\"] = np.mean(errors[key])\n        output[f\"{key}_error_bars_lower\"] = np.percentile(errors[key], percentile)\n        output[f\"{key}_error_bars_upper\"] = np.percentile(errors[key], 100 - percentile)\n    output[\"number_samples\"] = float(n_samples)\n    return output\n</code></pre>"},{"location":"reference/components/metrics/#tmlc.components.metrics.calculate_metrics","title":"<code>calculate_metrics(labels, predictions, element, n_iterations=1000, percentile=2.5)</code>","text":"<p>Calculates evaluation metrics given the true labels and predicted probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>torch.Tensor</code> <p>True labels, of shape (n_samples, num_classes).</p> required <code>predictions</code> <code>torch.Tensor</code> <p>Predicted probabilities, of shape (n_samples, num_classes).</p> required <code>element</code> <code>str</code> <p>Name of the element to include in metric names, such as \"train\" or \"test\".</p> required <code>n_iterations</code> <code>int</code> <p>Number of bootstrap iterations to use for calculating rates. Defaults to 1000.</p> <code>1000</code> <code>percentile</code> <code>float</code> <p>Percentile for the bootstrap confidence interval. Defaults to 2.5.</p> <code>2.5</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the following evaluation metrics: - element_f1 (float): Macro-average F1-score. - element_precision (float): Macro-average precision score. - element_recall (float): Macro-average recall score. - element_tpr (float): True positive rate. - element_fpr (float): False positive rate. - element_fnr (float): False negative rate. - element_tnr (float): True negative rate.</p> Example <p>Here's an example of how to use the <code>calculate_metrics</code> function:</p> <p>import torch from sklearn.metrics import f1_score, precision_score, recall_score from tmlc.metrics import calculate_metrics</p> Source code in <code>tmlc/components/metrics.py</code> <pre><code>def calculate_metrics(\n    labels: torch.Tensor,\n    predictions: torch.Tensor,\n    element: str,\n    n_iterations: int = 1000,\n    percentile: float = 2.5,\n) -&gt; Dict[str, Any]:\n\"\"\"\n    Calculates evaluation metrics given the true labels and predicted probabilities.\n\n    Args:\n        labels (torch.Tensor): True labels, of shape (n_samples, num_classes).\n        predictions (torch.Tensor): Predicted probabilities, of shape (n_samples, num_classes).\n        element (str): Name of the element to include in metric names, such as \"train\" or \"test\".\n        n_iterations (int, optional): Number of bootstrap iterations to use for calculating rates.\n            Defaults to 1000.\n        percentile (float, optional): Percentile for the bootstrap confidence interval. Defaults to 2.5.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the following evaluation metrics:\n            - element_f1 (float): Macro-average F1-score.\n            - element_precision (float): Macro-average precision score.\n            - element_recall (float): Macro-average recall score.\n            - element_tpr (float): True positive rate.\n            - element_fpr (float): False positive rate.\n            - element_fnr (float): False negative rate.\n            - element_tnr (float): True negative rate.\n\n    Example:\n        Here's an example of how to use the `calculate_metrics` function:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from sklearn.metrics import f1_score, precision_score, recall_score\n        &gt;&gt;&gt; from tmlc.metrics import calculate_metrics\n\n        &gt;&gt;&gt; # Create a tensor of true labels and predicted probabilities\n        &gt;&gt;&gt; true_labels = torch.tensor([[1, 0, 1], [0, 1, 0], [1, 1, 1], [0, 1, 1]])\n        &gt;&gt;&gt; predicted_probs = torch.tensor(\n        ... [[0.8, 0.1, 0.9], [0.3, 0.7, 0.4], [0.7, 0.6, 0.8], [0.2, 0.9, 0.7]])\n\n        &gt;&gt;&gt; # Calculate the evaluation metrics\n        &gt;&gt;&gt; metrics = calculate_metrics(\n        ... true_labels, predicted_probs, element=\"test\", n_iterations=100, percentile=5.0)\n\n        &gt;&gt;&gt; # Print the evaluation metrics\n        &gt;&gt;&gt; print(metrics)\n        {'test_f1': 0.717948717948718,\n        'test_precision': 0.6666666666666666,\n        'test_recall': 0.8,\n        'test_tpr': 0.8,\n        'test_fpr': 0.16666666666666666,\n        'test_fnr': 0.2,\n        'test_tnr': 0.8333333333333334}\n    \"\"\"\n    # Convert input tensors to numpy arrays\n    labels, predictions = labels.detach().numpy(), predictions.detach().numpy()\n\n    # Calculate macro-average F1-score, precision, recall, and accuracy\n    metrics = {\n        f\"{element}_f1\": f1_score(labels, predictions, average=\"macro\"),\n        f\"{element}_precision\": precision_score(labels, predictions, average=\"macro\", zero_division=0),\n        f\"{element}_recall\": recall_score(labels, predictions, average=\"macro\"),\n    }\n\n    # Calculate bootstrap estimates for true positive rate (tpr), false positive rate (fpr),\n    # false negative rate (fnr), and true negative rate (tnr) using the calculate_rates function\n    rates = bootstrap_rates(labels, predictions, n_iterations=n_iterations, percentile=percentile)\n\n    # Add bootstrap estimates to the metrics dictionary\n    for key, value in rates.items():\n        metrics.update({f\"{element}_{key}\": value})\n\n    # Return the metrics dictionary\n    return metrics\n</code></pre>"},{"location":"reference/components/metrics/#tmlc.components.metrics.calculate_metrics--create-a-tensor-of-true-labels-and-predicted-probabilities","title":"Create a tensor of true labels and predicted probabilities","text":"<p>true_labels = torch.tensor([[1, 0, 1], [0, 1, 0], [1, 1, 1], [0, 1, 1]]) predicted_probs = torch.tensor( ... [[0.8, 0.1, 0.9], [0.3, 0.7, 0.4], [0.7, 0.6, 0.8], [0.2, 0.9, 0.7]])</p>"},{"location":"reference/components/metrics/#tmlc.components.metrics.calculate_metrics--calculate-the-evaluation-metrics","title":"Calculate the evaluation metrics","text":"<p>metrics = calculate_metrics( ... true_labels, predicted_probs, element=\"test\", n_iterations=100, percentile=5.0)</p>"},{"location":"reference/components/metrics/#tmlc.components.metrics.calculate_metrics--print-the-evaluation-metrics","title":"Print the evaluation metrics","text":"<p>print(metrics) {'test_f1': 0.717948717948718, 'test_precision': 0.6666666666666666, 'test_recall': 0.8, 'test_tpr': 0.8, 'test_fpr': 0.16666666666666666, 'test_fnr': 0.2, 'test_tnr': 0.8333333333333334}</p>"},{"location":"reference/components/metrics/#tmlc.components.metrics.calculate_rates","title":"<code>calculate_rates(cm)</code>","text":"<p>Calculate true positive rate (tpr), false positive rate (fpr), false negative rate (fnr), and true negative rate (tnr) given a confusion matrix.</p> <p>A confusion matrix is a table used to evaluate the performance of a classification algorithm. The rows of the matrix represent the true labels, while the columns represent the predicted labels. Each cell of the matrix represents the number of samples that fall into a particular category.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>array-like</code> <p>Confusion matrix, of shape (num_classes, num_classes).</p> required <p>Returns:</p> Type Description <p>Tuple[array-like]: A tuple containing the tpr, fpr, fnr, and tnr.</p> <p>The tpr, fpr, fnr, and tnr can be calculated from the confusion matrix as follows:</p> <p>tpr = tp / (tp + fn) fpr = fp / (fp + tn) fnr = fn / (fn + tp) tnr = tn / (tn + fp)</p> <p>where tp, fn, fp, and tn are the numbers of true positives, false negatives, false positives, and true negatives, respectively.</p> Example <p>import numpy as np cm = np.array([[50, 10], [20, 70]]) tpr, fpr, fnr, tnr = calculate_rates(cm) tpr array([0.83333333, 0.77777778]) fpr array([0.22222222, 0.16666667]) fnr array([0.16666667, 0.22222222]) tnr array([0.77777778, 0.83333333])</p> Source code in <code>tmlc/components/metrics.py</code> <pre><code>def calculate_rates(cm):\n\"\"\"\n    Calculate true positive rate (tpr), false positive rate (fpr), false negative rate (fnr), and\n    true negative rate (tnr) given a confusion matrix.\n\n    A confusion matrix is a table used to evaluate the performance of a classification algorithm.\n    The rows of the matrix represent the true labels, while the columns represent the predicted labels.\n    Each cell of the matrix represents the number of samples that fall into a particular category.\n\n    Args:\n        cm (array-like): Confusion matrix, of shape (num_classes, num_classes).\n\n    Returns:\n        Tuple[array-like]: A tuple containing the tpr, fpr, fnr, and tnr.\n\n    The tpr, fpr, fnr, and tnr can be calculated from the confusion matrix as follows:\n\n    tpr = tp / (tp + fn)\n    fpr = fp / (fp + tn)\n    fnr = fn / (fn + tp)\n    tnr = tn / (tn + fp)\n\n    where tp, fn, fp, and tn are the numbers of true positives, false negatives, false positives,\n    and true negatives, respectively.\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; cm = np.array([[50, 10], [20, 70]])\n        &gt;&gt;&gt; tpr, fpr, fnr, tnr = calculate_rates(cm)\n        &gt;&gt;&gt; tpr\n        array([0.83333333, 0.77777778])\n        &gt;&gt;&gt; fpr\n        array([0.22222222, 0.16666667])\n        &gt;&gt;&gt; fnr\n        array([0.16666667, 0.22222222])\n        &gt;&gt;&gt; tnr\n        array([0.77777778, 0.83333333])\n    \"\"\"\n\n    eps = 0.00000001\n\n    # cm is the confusion matrix\n    num_classes = cm.shape[0]\n\n    # Initialize arrays to store true positives, false negatives, false positives,\n    # and true negatives for each class\n    tp = np.zeros(num_classes)\n    fn = np.zeros(num_classes)\n    fp = np.zeros(num_classes)\n    tn = np.zeros(num_classes)\n\n    # Calculate true positives, false negatives, false positives, and true negatives\n    # for each class\n    for i in range(num_classes):\n        tp[i] = cm[i, i]\n        fn[i] = np.sum(cm[i, :]) - tp[i]\n        fp[i] = np.sum(cm[:, i]) - tp[i]\n        tn[i] = np.sum(cm) - tp[i] - fn[i] - fp[i]\n\n    # Calculate true positive rate, false positive rate, false negative rate, and true\n    # negative rate for each class\n    tpr = tp / (tp + fn + eps)\n    fpr = fp / (fp + tn + eps)\n    fnr = fn / (fn + tp + eps)\n    tnr = tn / (tn + fp + eps)\n\n    # Return arrays containing tpr, fpr, fnr, and tnr for each class\n    return tpr, fpr, fnr, tnr\n</code></pre>"},{"location":"reference/components/metrics/#tmlc.components.metrics.evaluate_rates","title":"<code>evaluate_rates(y_true, y_pred)</code>","text":"<p>Calculate true positive rate (tpr), false positive rate (fpr), false negative rate (fnr), and true negative rate (tnr) given the true labels and predicted labels.</p> <p>The function first calculates the confusion matrix using scikit-learn's <code>confusion_matrix</code> function. It then uses the <code>calculate_rates</code> function to calculate the tpr, fpr, fnr, and tnr. The output is returned in a dictionary containing the confusion matrix and each of the rates.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array-like</code> <p>True labels, of shape (n_samples,).</p> required <code>y_pred</code> <code>array-like</code> <p>Predicted labels, of shape (n_samples,).</p> required <p>Returns:</p> Type Description <p>Dict[str, array-like]: A dictionary containing the confusion matrix and each of the rates.</p> Example <p>y_true = [1, 0, 1, 1, 0, 1] y_pred = [0, 0, 1, 1, 0, 1] evaluate_rates(y_true, y_pred) {     \"confusion_matrix\": array([[2, 1],                             [1, 3]]),     \"tpr\": 0.75,     \"fpr\": 0.3333333333333333,     \"fnr\": 0.25,     \"tnr\": 0.6666666666666666 }</p> Source code in <code>tmlc/components/metrics.py</code> <pre><code>def evaluate_rates(y_true, y_pred):\n\"\"\"\n    Calculate true positive rate (tpr), false positive rate (fpr), false negative rate (fnr), and\n    true negative rate (tnr) given the true labels and predicted labels.\n\n    The function first calculates the confusion matrix using scikit-learn's `confusion_matrix` function.\n    It then uses the `calculate_rates` function to calculate the tpr, fpr, fnr, and tnr. The output is\n    returned in a dictionary containing the confusion matrix and each of the rates.\n\n    Args:\n        y_true (array-like): True labels, of shape (n_samples,).\n        y_pred (array-like): Predicted labels, of shape (n_samples,).\n\n    Returns:\n        Dict[str, array-like]: A dictionary containing the confusion matrix and each of the rates.\n\n    Example:\n        &gt;&gt;&gt; y_true = [1, 0, 1, 1, 0, 1]\n        &gt;&gt;&gt; y_pred = [0, 0, 1, 1, 0, 1]\n        &gt;&gt;&gt; evaluate_rates(y_true, y_pred)\n        {\n            \"confusion_matrix\": array([[2, 1],\n                                    [1, 3]]),\n            \"tpr\": 0.75,\n            \"fpr\": 0.3333333333333333,\n            \"fnr\": 0.25,\n            \"tnr\": 0.6666666666666666\n        }\n    \"\"\"\n\n    cm = confusion_matrix(y_true, y_pred)\n    tpr, fpr, fnr, tnr = calculate_rates(cm)\n    return {\n        \"confusion_matrix\": cm,\n        \"tpr\": tpr,\n        \"fpr\": fpr,\n        \"fnr\": fnr,\n        \"tnr\": tnr,\n    }\n</code></pre>"},{"location":"reference/components/predictions/","title":"predictions","text":""},{"location":"reference/components/predictions/#tmlc.components.predictions.calculate_predictions","title":"<code>calculate_predictions(probabilities, thresholds)</code>","text":"<p>Generate predictions for a multi-class classification task.</p> <p>Parameters:</p> Name Type Description Default <code>probabilities</code> <code>torch.Tensor</code> <p>A tensor of shape (batch_size, num_classes) containing model predictions.</p> required <code>thresholds</code> <code>torch.Tensor</code> <p>A list of length num_classes containing the optimal thresholds for each class.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>A tensor of shape (batch_size, num_classes) with predictions.</p> Example <p>Example A:</p> <p>import torch probabilities = torch.tensor([[0.3, 0.7, 0.1], [0.1, 0.2, 0.9], [0.7, 0.3, 0.4]]) thresholds = [0.5, 0.3, 0.8] calculate_predictions(probabilities, thresholds) tensor([[0., 1., 0.],         [0., 0., 1.],         [1., 0., 0.]])</p> <p>Example B:</p> <p>probabilities = torch.tensor([[0.3, 0.7], [0.1, 0.2], [0.7, 0.3], [0.4, 0.6]]) thresholds = [0.4, 0.6] calculate_predictions(probabilities, thresholds) tensor([[1., 1.],         [0., 0.],         [1., 0.],         [1., 0.]])</p> Source code in <code>tmlc/components/predictions.py</code> <pre><code>def calculate_predictions(probabilities: torch.Tensor, thresholds: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"\n    Generate predictions for a multi-class classification task.\n\n    Args:\n        probabilities: A tensor of shape (batch_size, num_classes) containing model predictions.\n        thresholds: A list of length num_classes containing the optimal thresholds for each class.\n\n    Returns:\n        A tensor of shape (batch_size, num_classes) with predictions.\n\n    Example:\n        Example A:\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; probabilities = torch.tensor([[0.3, 0.7, 0.1], [0.1, 0.2, 0.9], [0.7, 0.3, 0.4]])\n        &gt;&gt;&gt; thresholds = [0.5, 0.3, 0.8]\n        &gt;&gt;&gt; calculate_predictions(probabilities, thresholds)\n        tensor([[0., 1., 0.],\n                [0., 0., 1.],\n                [1., 0., 0.]])\n\n        Example B:\n        &gt;&gt;&gt; probabilities = torch.tensor([[0.3, 0.7], [0.1, 0.2], [0.7, 0.3], [0.4, 0.6]])\n        &gt;&gt;&gt; thresholds = [0.4, 0.6]\n        &gt;&gt;&gt; calculate_predictions(probabilities, thresholds)\n        tensor([[1., 1.],\n                [0., 0.],\n                [1., 0.],\n                [1., 0.]])\n    \"\"\"\n    # create a copy of the tensor to store predictions\n    predictions = probabilities.clone()\n    # get the number of classes from the tensor\n    _, num_classes = probabilities.shape\n    for i in range(num_classes):\n        # iterate through the classes and assign 1 or 0 to the predictions depending on the threshold\n        predictions[:, i] = (probabilities[:, i] &gt; thresholds[i]).float()\n    return predictions\n</code></pre>"},{"location":"reference/components/process_data/","title":"process_data","text":""},{"location":"reference/components/process_data/#tmlc.components.process_data.process_data","title":"<code>process_data(data, text_column, labels_columns)</code>","text":"<p>Processes the input data by converting it to a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>The data to preprocess.</p> required <code>text_column</code> <code>str</code> <p>The name of the column containing the message text.</p> required <code>labels_columns</code> <code>List[str]</code> <p>The names of the columns containing the labels.</p> required <p>Returns:</p> Name Type Description <code>Messages</code> <code>Messages</code> <p>A Messages object containing the preprocessed messages.</p> Source code in <code>tmlc/components/process_data.py</code> <pre><code>def process_data(data: pd.DataFrame, text_column: str, labels_columns: List[str]) -&gt; Messages:\n\"\"\"\n    Processes the input data by converting it to a list of messages.\n\n    Args:\n        data (pd.DataFrame): The data to preprocess.\n        text_column (str): The name of the column containing the message text.\n        labels_columns (List[str]): The names of the columns containing the labels.\n\n    Returns:\n        Messages: A Messages object containing the preprocessed messages.\n    \"\"\"\n    messages = data.apply(\n        lambda row: Message(row[text_column], row[labels_columns].tolist()), axis=1\n    ).tolist()\n    return Messages(messages, labels_names=labels_columns)\n</code></pre>"},{"location":"reference/components/split_data/","title":"split_data","text":""},{"location":"reference/components/split_data/#tmlc.components.split_data.combine_frames","title":"<code>combine_frames(X_train, X_val, X_test, y_train, y_val, y_test, labels_columns, feature_columns)</code>","text":"<p>Combines the train, validation, and test data into a single dataframe for each dataset.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>np.ndarray</code> <p>Array of training data.</p> required <code>X_val</code> <code>np.ndarray</code> <p>Array of validation data.</p> required <code>X_test</code> <code>np.ndarray</code> <p>Array of testing data.</p> required <code>y_train</code> <code>np.ndarray</code> <p>Array of training labels.</p> required <code>y_val</code> <code>np.ndarray</code> <p>Array of validation labels.</p> required <code>y_test</code> <code>np.ndarray</code> <p>Array of testing labels.</p> required <code>labels_columns</code> <code>List[str]</code> <p>List of columns containing the labels.</p> required <code>feature_columns</code> <code>List[str]</code> <p>List of columns containing the features.</p> required <p>Returns:</p> Type Description <code>Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]</code> <p>Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: A tuple containing the combined train, validation, and test DataFrames.</p> Example <p>import numpy as np import pandas as pd X_train = np.array([[1, 2], [3, 4], [5, 6]]) X_val = np.array([[7, 8]]) X_test = np.array([[9, 10], [11, 12]]) y_train = np.array([[0], [1], [0]]) y_val = np.array([[1]]) y_test = np.array([[0], [1]]) labels_columns = [\"label\"] feature_columns = [\"feature_1\", \"feature_2\"] train_data, val_data, test_data = combine_frames( ... X_train, X_val, X_test, y_train, y_val, y_test, labels_columns, feature_columns) print(\"Train data shape:\", train_data.shape) print(\"Train data:\") print(train_data) print(\"Validation data shape:\", val_data.shape) print(\"Validation data:\") print(val_data) print(\"Test data shape:\", test_data.shape) print(\"Test data:\") print(test_data) Train data shape: (3, 3) Train data: feature_1  feature_2  label 0          1          2      0 1          3          4      1 2          5          6      0 Validation data shape: (1, 3) Validation data: feature_1  feature_2  label 0          7          8      1 Test data shape: (2, 3) Test data: feature_1  feature_2  label 0          9         10      0 1         11         12      1</p> Source code in <code>tmlc/components/split_data.py</code> <pre><code>def combine_frames(\n    X_train: np.ndarray,\n    X_val: np.ndarray,\n    X_test: np.ndarray,\n    y_train: np.ndarray,\n    y_val: np.ndarray,\n    y_test: np.ndarray,\n    labels_columns: List[str],\n    feature_columns: List[str],\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    Combines the train, validation, and test data into a single dataframe for each dataset.\n\n    Args:\n        X_train (np.ndarray): Array of training data.\n        X_val (np.ndarray): Array of validation data.\n        X_test (np.ndarray): Array of testing data.\n        y_train (np.ndarray): Array of training labels.\n        y_val (np.ndarray): Array of validation labels.\n        y_test (np.ndarray): Array of testing labels.\n        labels_columns (List[str]): List of columns containing the labels.\n        feature_columns (List[str]): List of columns containing the features.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: A tuple containing the combined train, validation,\n            and test DataFrames.\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; X_train = np.array([[1, 2], [3, 4], [5, 6]])\n        &gt;&gt;&gt; X_val = np.array([[7, 8]])\n        &gt;&gt;&gt; X_test = np.array([[9, 10], [11, 12]])\n        &gt;&gt;&gt; y_train = np.array([[0], [1], [0]])\n        &gt;&gt;&gt; y_val = np.array([[1]])\n        &gt;&gt;&gt; y_test = np.array([[0], [1]])\n        &gt;&gt;&gt; labels_columns = [\"label\"]\n        &gt;&gt;&gt; feature_columns = [\"feature_1\", \"feature_2\"]\n        &gt;&gt;&gt; train_data, val_data, test_data = combine_frames(\n        ... X_train, X_val, X_test, y_train, y_val, y_test, labels_columns, feature_columns)\n        &gt;&gt;&gt; print(\"Train data shape:\", train_data.shape)\n        &gt;&gt;&gt; print(\"Train data:\")\n        &gt;&gt;&gt; print(train_data)\n        &gt;&gt;&gt; print(\"Validation data shape:\", val_data.shape)\n        &gt;&gt;&gt; print(\"Validation data:\")\n        &gt;&gt;&gt; print(val_data)\n        &gt;&gt;&gt; print(\"Test data shape:\", test_data.shape)\n        &gt;&gt;&gt; print(\"Test data:\")\n        &gt;&gt;&gt; print(test_data)\n        Train data shape: (3, 3)\n        Train data:\n        feature_1  feature_2  label\n        0          1          2      0\n        1          3          4      1\n        2          5          6      0\n        Validation data shape: (1, 3)\n        Validation data:\n        feature_1  feature_2  label\n        0          7          8      1\n        Test data shape: (2, 3)\n        Test data:\n        feature_1  feature_2  label\n        0          9         10      0\n        1         11         12      1\n    \"\"\"\n    if not all(X.shape[0] == y.shape[0] for X, y in [(X_train, y_train), (X_val, y_val), (X_test, y_test)]):\n        raise ValueError(\"The number of rows in X and y must be the same for all sets.\")\n\n    # Convert arrays back to dataframes\n    train_df = pd.DataFrame(X_train, columns=feature_columns)\n    train_df[labels_columns] = y_train\n    val_df = pd.DataFrame(X_val, columns=feature_columns)\n    val_df[labels_columns] = y_val\n    test_df = pd.DataFrame(X_test, columns=feature_columns)\n    test_df[labels_columns] = y_test\n    return train_df, val_df, test_df\n</code></pre>"},{"location":"reference/components/split_data/#tmlc.components.split_data.split_data","title":"<code>split_data(data, labels_columns, train_ratio, val_ratio, test_ratio, random_state=42)</code>","text":"<p>Splits a given data set into training, validation, and test sets based on user-specified ratios.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>DataFrame containing features and labels.</p> required <code>labels_columns</code> <code>List[str]</code> <p>List of columns containing the labels.</p> required <code>train_ratio</code> <code>float</code> <p>Proportion of data to use for training.</p> required <code>val_ratio</code> <code>float</code> <p>Proportion of data to use for validation.</p> required <code>test_ratio</code> <code>float</code> <p>Proportion of data to use for testing.</p> required <code>random_state</code> <code>int</code> <p>Random state to use for reproducibility. Defaults to 42.</p> <code>42</code> <p>Returns:</p> Type Description <code>Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]</code> <p>Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: A tuple containing the train, validation, and test DataFrames.</p> Example <p>import pandas as pd data = pd.DataFrame({ ...     \"feature_1\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], ...     \"feature_2\": [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], ...     \"label\": [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1] ... }) labels_columns = [\"label\"] train_ratio = 0.6 val_ratio = 0.2 test_ratio = 0.2 train_data, val_data, test_data = split_data( ... data, labels_columns, train_ratio, val_ratio, test_ratio) print(\"Train data shape:\", train_data.shape) print(\"Train data:\") print(train_data) print(\"Validation data shape:\", val_data.shape) print(\"Validation data:\") print(val_data) print(\"Test data shape:\", test_data.shape) print(\"Test data:\") print(test_data) Train data shape: (8, 3) Train data:     feature_1  feature_2  label 0          1          6      0 2          3          8      0 3          4          9      1 5          6         11      1 7          8         13      1 9         10         15      1 10        11         16      0 12        13         18      0 Validation data shape: (2, 3) Validation data:     feature_1  feature_2  label 1          2          7      1 11        12         17      0 Test data shape: (4, 3) Test data:     feature_1  feature_2  label 4          5         10      0 6          7         12      0 8          9         14      0 13        14         19      1</p> Source code in <code>tmlc/components/split_data.py</code> <pre><code>def split_data(\n    data: pd.DataFrame,\n    labels_columns: List[str],\n    train_ratio: float,\n    val_ratio: float,\n    test_ratio: float,\n    random_state: int = 42,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    Splits a given data set into training, validation, and test sets based on user-specified ratios.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing features and labels.\n        labels_columns (List[str]): List of columns containing the labels.\n        train_ratio (float, optional): Proportion of data to use for training.\n        val_ratio (float, optional): Proportion of data to use for validation.\n        test_ratio (float, optional): Proportion of data to use for testing.\n        random_state (int, optional): Random state to use for reproducibility. Defaults to 42.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: A tuple containing the train, validation,\n            and test DataFrames.\n\n    Example:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.DataFrame({\n        ...     \"feature_1\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n        ...     \"feature_2\": [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n        ...     \"label\": [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n        ... })\n        &gt;&gt;&gt; labels_columns = [\"label\"]\n        &gt;&gt;&gt; train_ratio = 0.6\n        &gt;&gt;&gt; val_ratio = 0.2\n        &gt;&gt;&gt; test_ratio = 0.2\n        &gt;&gt;&gt; train_data, val_data, test_data = split_data(\n        ... data, labels_columns, train_ratio, val_ratio, test_ratio)\n        &gt;&gt;&gt; print(\"Train data shape:\", train_data.shape)\n        &gt;&gt;&gt; print(\"Train data:\")\n        &gt;&gt;&gt; print(train_data)\n        &gt;&gt;&gt; print(\"Validation data shape:\", val_data.shape)\n        &gt;&gt;&gt; print(\"Validation data:\")\n        &gt;&gt;&gt; print(val_data)\n        &gt;&gt;&gt; print(\"Test data shape:\", test_data.shape)\n        &gt;&gt;&gt; print(\"Test data:\")\n        &gt;&gt;&gt; print(test_data)\n        Train data shape: (8, 3)\n        Train data:\n            feature_1  feature_2  label\n        0          1          6      0\n        2          3          8      0\n        3          4          9      1\n        5          6         11      1\n        7          8         13      1\n        9         10         15      1\n        10        11         16      0\n        12        13         18      0\n        Validation data shape: (2, 3)\n        Validation data:\n            feature_1  feature_2  label\n        1          2          7      1\n        11        12         17      0\n        Test data shape: (4, 3)\n        Test data:\n            feature_1  feature_2  label\n        4          5         10      0\n        6          7         12      0\n        8          9         14      0\n        13        14         19      1\n    \"\"\"\n    np.random.seed(random_state)\n\n    # Check that ratios add up to 1.0\n    if abs(train_ratio + val_ratio + test_ratio - 1.0) &gt; 1e-6:\n        raise ValueError(\"train_ratio + val_ratio + test_ratio must add up to 1.0\")\n\n    # Check for missing values\n    if data.isnull().values.any():\n        raise ValueError(\"data contains missing values\")\n\n    # Check that labels_columns is not empty\n    if not labels_columns:\n        raise ValueError(\"labels_columns cannot be empty\")\n\n    # Split data into X and y\n    X = data.drop(columns=labels_columns)\n    y = data[labels_columns]\n\n    # Split into train and test\n    X_train, y_train, X_test, y_test = iterative_train_test_split(X.values, y.values, test_size=test_ratio)\n\n    # Split train into train and val\n    X_train, y_train, X_val, y_val = iterative_train_test_split(\n        X_train, y_train, test_size=val_ratio / (train_ratio + val_ratio)\n    )\n\n    return combine_frames(\n        X_train,\n        X_val,\n        X_test,\n        y_train,\n        y_val,\n        y_test,\n        labels_columns=labels_columns,\n        feature_columns=X.columns,\n    )\n</code></pre>"},{"location":"reference/configclasses/","title":"configclasses","text":""},{"location":"reference/configclasses/data_module_config/","title":"data_module_config","text":""},{"location":"reference/configclasses/data_module_config/#tmlc.configclasses.data_module_config.DataModuleConfig","title":"<code>DataModuleConfig</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Configuration class for a PyTorch Lightning DataModule.</p> <p>Attributes:</p> Name Type Description <code>state_file</code> <code>str</code> <p>Path to the file to store the state of the DataModule.</p> <code>dataset</code> <code>DatasetConfig</code> <p>Configuration for the dataset to use.</p> <code>load_data</code> <code>PartialFunctionConfig</code> <p>Configuration for the function to load the data.</p> <code>split</code> <code>PartialFunctionConfig</code> <p>Configuration for the function to split the dataset.</p> <code>process_data</code> <code>PartialFunctionConfig</code> <p>Configuration for the function to process the data.</p> Source code in <code>tmlc/configclasses/data_module_config.py</code> <pre><code>class DataModuleConfig(BaseModel):\n\"\"\"\n    Configuration class for a PyTorch Lightning DataModule.\n\n    Attributes:\n        state_file (str): Path to the file to store the state of the DataModule.\n        dataset (DatasetConfig): Configuration for the dataset to use.\n        load_data (PartialFunctionConfig): Configuration for the function to load the data.\n        split (PartialFunctionConfig): Configuration for the function to split the dataset.\n        process_data (PartialFunctionConfig): Configuration for the function to process the data.\n    \"\"\"\n\n    state_file: str\n    dataset: DatasetConfig\n    load_data: PartialFunctionConfig\n    split: PartialFunctionConfig\n    process_data: PartialFunctionConfig\n</code></pre>"},{"location":"reference/configclasses/dataset_config/","title":"dataset_config","text":""},{"location":"reference/configclasses/dataset_config/#tmlc.configclasses.dataset_config.DatasetConfig","title":"<code>DatasetConfig</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>A configuration class for a dataset.</p> <p>This class is used to store configuration information for a dataset. It includes a <code>tokenizer_config</code> attribute that specifies the configuration for the tokenizer to use.</p> <p>Attributes:</p> Name Type Description <code>tokenizer_config</code> <code>TokenizerConfig</code> <p>The configuration for the tokenizer to use.</p> <code>batch_size</code> <code>int</code> <p>The batch size to use when processing the dataset.</p> <code>kwargs</code> <code>Optional[dict]</code> <p>Additional keyword arguments to pass to the dataset class constructor.</p> Source code in <code>tmlc/configclasses/dataset_config.py</code> <pre><code>class DatasetConfig(BaseModel):\n\"\"\"\n    A configuration class for a dataset.\n\n    This class is used to store configuration information for a dataset. It includes a `tokenizer_config`\n    attribute that specifies the configuration for the tokenizer to use.\n\n    Attributes:\n        tokenizer_config (TokenizerConfig): The configuration for the tokenizer to use.\n        batch_size (int): The batch size to use when processing the dataset.\n        kwargs (Optional[dict]): Additional keyword arguments to pass to the dataset class constructor.\n    \"\"\"\n\n    tokenizer_config: TokenizerConfig\n    batch_size: int\n    kwargs: Optional[dict] = None\n\n    @property\n    def tokenizer(self):\n\"\"\"\n        Return the tokenizer from the tokenizer configuration.\n\n        Returns:\n            PreTrainedTokenizer: The tokenizer instance.\n        \"\"\"\n        return self.tokenizer_config\n</code></pre>"},{"location":"reference/configclasses/dataset_config/#tmlc.configclasses.dataset_config.DatasetConfig.tokenizer","title":"<code>tokenizer</code>  <code>property</code>","text":"<p>Return the tokenizer from the tokenizer configuration.</p> <p>Returns:</p> Name Type Description <code>PreTrainedTokenizer</code> <p>The tokenizer instance.</p>"},{"location":"reference/configclasses/lightning_module_config/","title":"lightning_module_config","text":""},{"location":"reference/configclasses/lightning_module_config/#tmlc.configclasses.lightning_module_config.LightningModuleConfig","title":"<code>LightningModuleConfig</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>A data class for storing configuration parameters to create a partial function object.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>The name of the model.</p> <code>model</code> <code>ModelConfig</code> <p>The configuration for the model to use.</p> <code>optimizer</code> <code>PartialFunctionConfig</code> <p>The configuration for the optimizer function.</p> <code>define_loss</code> <code>PartialFunctionConfig</code> <p>The configuration for the function to define the loss.</p> <code>predict</code> <code>PartialFunctionConfig</code> <p>The configuration for the function to generate predictions.</p> <code>calculate_best_thresholds</code> <code>PartialFunctionConfig</code> <p>The configuration for the function to calculate the best thresholds.</p> <code>calculate_metrics</code> <code>PartialFunctionConfig</code> <p>The configuration for the function to calculate the metrics.</p> Class Methods <p>from_yaml(file_path: str) -&gt; \"LightningModuleConfig\":     Class method to create a <code>LightningModuleConfig</code> object from a YAML file.</p> Source code in <code>tmlc/configclasses/lightning_module_config.py</code> <pre><code>class LightningModuleConfig(BaseModel):\n\"\"\"\n    A data class for storing configuration parameters to create a partial function object.\n\n    Attributes:\n        model_name (str): The name of the model.\n        model (ModelConfig): The configuration for the model to use.\n        optimizer (PartialFunctionConfig): The configuration for the optimizer function.\n        define_loss (PartialFunctionConfig): The configuration for the function to define the loss.\n        predict (PartialFunctionConfig): The configuration for the function to generate predictions.\n        calculate_best_thresholds (PartialFunctionConfig): The configuration for the function\n            to calculate the best thresholds.\n        calculate_metrics (PartialFunctionConfig): The configuration for the function to\n            calculate the metrics.\n\n    Class Methods:\n        from_yaml(file_path: str) -&gt; \"LightningModuleConfig\":\n            Class method to create a `LightningModuleConfig` object from a YAML file.\n    \"\"\"\n\n    model_name: str\n    model: ModelConfig\n    optimizer: PartialFunctionConfig\n    define_loss: PartialFunctionConfig\n    predict: PartialFunctionConfig\n    calculate_best_thresholds: PartialFunctionConfig\n    calculate_metrics: PartialFunctionConfig\n\n    @classmethod\n    def from_yaml(cls, file_path: str) -&gt; \"LightningModuleConfig\":\n        with open(file_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n\n        return cls(**config_dict[\"lightningmodule\"])\n</code></pre>"},{"location":"reference/configclasses/mlflow_config/","title":"mlflow_config","text":""},{"location":"reference/configclasses/mlflow_config/#tmlc.configclasses.mlflow_config.MlflowConfig","title":"<code>MlflowConfig</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Configuration class for logging a model and tokenizer with MLflow.</p> <p>Attributes:</p> Name Type Description <code>model_path</code> <code>str</code> <p>The path to the saved model.</p> <code>tokenizer_path</code> <code>str</code> <p>The path to the saved tokenizer.</p> <code>score_script_path</code> <code>Optional[str]</code> <p>The path to the Python script to use for scoring.</p> <code>description</code> <code>str</code> <p>A description of the model.</p> <code>tags</code> <code>Dict[str, Any]</code> <p>A dictionary of key-value pairs to use as tags when logging the model.</p> <code>artifact_folder</code> <code>str</code> <p>Path to the folder where artifacts will be stored.</p> Source code in <code>tmlc/configclasses/mlflow_config.py</code> <pre><code>class MlflowConfig(BaseModel):\n\"\"\"\n    Configuration class for logging a model and tokenizer with MLflow.\n\n    Attributes:\n        model_path (str): The path to the saved model.\n        tokenizer_path (str): The path to the saved tokenizer.\n        score_script_path (Optional[str]): The path to the\n            Python script to use for scoring.\n        description (str): A description of the model.\n        tags (Dict[str, Any]): A dictionary of key-value pairs\n            to use as tags when logging the model.\n        artifact_folder (str):\n            Path to the folder where artifacts will be stored.\n    \"\"\"\n\n    model_path: str\n    tokenizer_path: str\n    score_script_path: Optional[str] = None\n    description: str\n    tags: Dict[str, Any]\n    artifact_folder: str\n</code></pre>"},{"location":"reference/configclasses/model_config/","title":"model_config","text":""},{"location":"reference/configclasses/model_config/#tmlc.configclasses.model_config.ModelConfig","title":"<code>ModelConfig</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Configuration class for a PyTorch model.</p> <p>Attributes:</p> Name Type Description <code>pretrained_model</code> <code>PreTrainedConfig</code> <p>The configuration for a pre-trained model.</p> <code>dropout_prob</code> <code>float</code> <p>The dropout probability to use.</p> <code>hidden_size</code> <code>int</code> <p>The size of the hidden layer.</p> <code>num_classes</code> <code>int</code> <p>The number of output classes.</p> <code>calculate_predictions</code> <code>PartialFunctionConfig</code> <p>The configuration for the function to calculate predictions.</p> Source code in <code>tmlc/configclasses/model_config.py</code> <pre><code>class ModelConfig(BaseModel):\n\"\"\"\n    Configuration class for a PyTorch model.\n\n    Attributes:\n        pretrained_model (PreTrainedConfig): The configuration for a pre-trained model.\n        dropout_prob (float): The dropout probability to use.\n        hidden_size (int): The size of the hidden layer.\n        num_classes (int): The number of output classes.\n        calculate_predictions (PartialFunctionConfig): The configuration for the\n            function to calculate predictions.\n    \"\"\"\n\n    pretrained_model: PreTrainedConfig\n    dropout_prob: float\n    hidden_size: int\n    num_classes: int\n    calculate_predictions: PartialFunctionConfig\n</code></pre>"},{"location":"reference/configclasses/partial_function_config/","title":"partial_function_config","text":""},{"location":"reference/configclasses/partial_function_config/#tmlc.configclasses.partial_function_config.PartialFunctionConfig","title":"<code>PartialFunctionConfig</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Configuration class for creating a partial function object.</p> <p>Attributes:</p> Name Type Description <code>module</code> <code>str</code> <p>Name of the module that contains the function.</p> <code>func</code> <code>str</code> <p>Name of the function to create the partial object.</p> <code>args</code> <code>Optional[list]</code> <p>Arguments to pass to the function. Defaults to None.</p> <code>kwargs</code> <code>Optional[dict]</code> <p>Keyword arguments to pass to the function. Defaults to None.</p> Properties <p>partial (partial): A partial function object.</p> Source code in <code>tmlc/configclasses/partial_function_config.py</code> <pre><code>class PartialFunctionConfig(BaseModel):\n\"\"\"\n    Configuration class for creating a partial function object.\n\n    Attributes:\n        module (str): Name of the module that contains the function.\n        func (str): Name of the function to create the partial object.\n        args (Optional[list], optional): Arguments to pass to the function. Defaults to None.\n        kwargs (Optional[dict], optional): Keyword arguments to pass to the function.\n            Defaults to None.\n\n    Properties:\n        partial (partial): A partial function object.\n    \"\"\"\n\n    module: str\n    func: str\n    args: Optional[list] = None\n    kwargs: Optional[dict] = None\n\n    @property\n    def partial(self):\n\"\"\"\n        Creates and returns a partial function object.\n\n        Returns:\n            partial: A partial function object.\n        \"\"\"\n        return get_partial(self.module, self.func, self.args, self.kwargs)\n</code></pre>"},{"location":"reference/configclasses/partial_function_config/#tmlc.configclasses.partial_function_config.PartialFunctionConfig.partial","title":"<code>partial</code>  <code>property</code>","text":"<p>Creates and returns a partial function object.</p> <p>Returns:</p> Name Type Description <code>partial</code> <p>A partial function object.</p>"},{"location":"reference/configclasses/partial_function_config/#tmlc.configclasses.partial_function_config.get_partial","title":"<code>get_partial(module, func, args=None, kwargs=None)</code>","text":"<p>Create and return a partial function object.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>str</code> <p>Name of the module that contains the function to create the partial object.</p> required <code>func</code> <code>str</code> <p>Name of the function to create the partial object.</p> required <code>args</code> <code>Optional[Any]</code> <p>Arguments to pass to the function. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Keyword arguments to pass to the function. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If function object is not found.</p> <p>Returns:</p> Name Type Description <code>partial</code> <code>partial</code> <p>A partial function object.</p> Source code in <code>tmlc/configclasses/partial_function_config.py</code> <pre><code>def get_partial(\n    module: str, func: str, args: Optional[Any] = None, kwargs: Optional[Dict[str, Any]] = None\n) -&gt; partial:\n\"\"\"\n    Create and return a partial function object.\n\n    Args:\n        module (str): Name of the module that contains the function to create the partial object.\n        func (str): Name of the function to create the partial object.\n        args (Optional[Any], optional): Arguments to pass to the function. Defaults to None.\n        kwargs (Optional[Dict[str, Any]], optional): Keyword arguments to pass to the function.\n            Defaults to None.\n\n    Raises:\n        ValueError: If function object is not found.\n\n    Returns:\n        partial: A partial function object.\n    \"\"\"\n    try:\n        # Get the function object from the module and create a partial object\n        func = getattr(importlib.import_module(module), func)\n    except (AttributeError, ModuleNotFoundError):\n        raise ValueError(f\"Could not find function {func} in module {module}\")\n\n    args = args or []\n    kwargs = kwargs or {}\n    return partial(func, *args, **kwargs)\n</code></pre>"},{"location":"reference/configclasses/pretrained_config/","title":"pretrained_config","text":""},{"location":"reference/configclasses/pretrained_config/#tmlc.configclasses.pretrained_config.PreTrainedConfig","title":"<code>PreTrainedConfig</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>A configuration class for a pre-trained model.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path to the pre-trained model files.</p> <code>kwargs</code> <code>Optional[dict]</code> <p>A dictionary of additional keyword arguments to pass to the tokenizer.</p> <code>instance</code> <code>Optional[PreTrainedModel]</code> <p>Instance of the pre-trained model.</p> Example Source code in <code>tmlc/configclasses/pretrained_config.py</code> <pre><code>class PreTrainedConfig(BaseModel):\n\"\"\"\n    A configuration class for a pre-trained model.\n\n    Attributes:\n        path (str): The path to the pre-trained model files.\n        kwargs (Optional[dict]): A dictionary of additional keyword arguments to pass\n            to the tokenizer.\n        instance (Optional[PreTrainedModel]): Instance of the pre-trained model.\n\n    Example:\n        &gt;&gt;&gt; # create a configuration object\n        &gt;&gt;&gt; config = PreTrainedConfig(path='bert-base-uncased', kwargs={'return_dict': True})\n        &gt;&gt;&gt; # apply the pre-trained model on some input data\n        &gt;&gt;&gt; input_data = 'Hello world!'\n        &gt;&gt;&gt; output = config(input_data)\n        &gt;&gt;&gt; # print the output\n        &gt;&gt;&gt; print(output)\n        {'last_hidden_state': tensor([[[...]]]),\n         'pooler_output': tensor([[...]]),\n         'hidden_states': (tensor([[[...]]]), ...)}\n    \"\"\"\n\n    path: str\n    kwargs: Optional[dict] = None\n    instance: Optional[str] = None\n\n    @property\n    def model(self):\n\"\"\"\n        Initialize the Pre-Trained Model.\n        \"\"\"\n        if ~isinstance(self.instance, PreTrainedModel):\n            self.instance = AutoModel.from_pretrained(self.path)\n        return self.instance\n\n    def __call__(self, data):\n\"\"\"\n        Apply the Pre-Trained Model on the input data.\n        \"\"\"\n        kwargs = self.kwargs or {}\n        return self.model(data, **kwargs)\n</code></pre>"},{"location":"reference/configclasses/pretrained_config/#tmlc.configclasses.pretrained_config.PreTrainedConfig--create-a-configuration-object","title":"create a configuration object","text":"<p>config = PreTrainedConfig(path='bert-base-uncased', kwargs={'return_dict': True})</p>"},{"location":"reference/configclasses/pretrained_config/#tmlc.configclasses.pretrained_config.PreTrainedConfig--apply-the-pre-trained-model-on-some-input-data","title":"apply the pre-trained model on some input data","text":"<p>input_data = 'Hello world!' output = config(input_data)</p>"},{"location":"reference/configclasses/pretrained_config/#tmlc.configclasses.pretrained_config.PreTrainedConfig--print-the-output","title":"print the output","text":"<p>print(output) {'last_hidden_state': tensor([[[...]]]),  'pooler_output': tensor([[...]]),  'hidden_states': (tensor([[[...]]]), ...)}</p>"},{"location":"reference/configclasses/pretrained_config/#tmlc.configclasses.pretrained_config.PreTrainedConfig.model","title":"<code>model</code>  <code>property</code>","text":"<p>Initialize the Pre-Trained Model.</p>"},{"location":"reference/configclasses/pretrained_config/#tmlc.configclasses.pretrained_config.PreTrainedConfig.__call__","title":"<code>__call__(data)</code>","text":"<p>Apply the Pre-Trained Model on the input data.</p> Source code in <code>tmlc/configclasses/pretrained_config.py</code> <pre><code>def __call__(self, data):\n\"\"\"\n    Apply the Pre-Trained Model on the input data.\n    \"\"\"\n    kwargs = self.kwargs or {}\n    return self.model(data, **kwargs)\n</code></pre>"},{"location":"reference/configclasses/tokenizer_config/","title":"tokenizer_config","text":""},{"location":"reference/configclasses/tokenizer_config/#tmlc.configclasses.tokenizer_config.TokenizerConfig","title":"<code>TokenizerConfig</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>A configuration class for a tokenizer.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>The name of the pre-trained model to use.</p> <code>path</code> <code>str</code> <p>The path to the tokenizer files.</p> <code>max_length</code> <code>int</code> <p>The maximum length of input sequences.</p> <code>kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary of additional keyword arguments to pass to the tokenizer.</p> <code>instance</code> <code>Optional[Any]</code> <p>Instance of the pre-trained tokenizer.</p> <code>output_keys</code> <code>List[str]</code> <p>A list of output keys to include in the returned dictionary.</p> Example Source code in <code>tmlc/configclasses/tokenizer_config.py</code> <pre><code>class TokenizerConfig(BaseModel):\n\"\"\"\n    A configuration class for a tokenizer.\n\n    Attributes:\n        model_name (str): The name of the pre-trained model to use.\n        path (str): The path to the tokenizer files.\n        max_length (int): The maximum length of input sequences.\n        kwargs (Optional[Dict[str, Any]]): A dictionary of additional keyword arguments to pass\n            to the tokenizer.\n        instance (Optional[Any]): Instance of the pre-trained tokenizer.\n        output_keys (List[str]): A list of output keys to include in the returned dictionary.\n\n    Example:\n        &gt;&gt;&gt; # Initialize the tokenizer configuration object\n        &gt;&gt;&gt; tokenizer_config = TokenizerConfig(\n        ...     model_name='bert-base-uncased',\n        ...     path='bert-base-uncased',\n        ...     max_length=512,\n        ...     output_keys=['input_ids', 'attention_mask']\n        ... )\n        &gt;&gt;&gt; # Tokenize a list of sentences\n        &gt;&gt;&gt; sentences = ['Hello world!', 'This is a test.']\n        &gt;&gt;&gt; tokenized_data = tokenizer_config(sentences)\n        &gt;&gt;&gt; # Print the tokenized data\n        &gt;&gt;&gt; print(tokenized_data)\n        Output:\n        {\n            'input_ids': [[101, 7592, 2088, 999, 102], [101, 2023, 2003, 1037, 3231, 1012, 102]],\n            'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]\n        }\n    \"\"\"\n\n    model_name: str\n    path: str\n    max_length: int\n    output_keys: List[str]\n    kwargs: Optional[dict] = None\n    instance: Optional[str] = None\n\n    @property\n    def tokenizer(self):\n\"\"\"\n        Initialize the tokenizer.\n        \"\"\"\n        if ~isinstance(self.instance, PreTrainedTokenizer):\n            self.instance = AutoTokenizer.from_pretrained(self.path, model_name=self.model_name)\n        return self.instance\n\n    def __call__(self, data):\n\"\"\"\n        Tokenize the input data using the tokenizer.\n        \"\"\"\n        kwargs = self.kwargs or {}\n        output = self.tokenizer(data, **kwargs)\n        return {key: output[key] for key in self.output_keys}\n</code></pre>"},{"location":"reference/configclasses/tokenizer_config/#tmlc.configclasses.tokenizer_config.TokenizerConfig--initialize-the-tokenizer-configuration-object","title":"Initialize the tokenizer configuration object","text":"<p>tokenizer_config = TokenizerConfig( ...     model_name='bert-base-uncased', ...     path='bert-base-uncased', ...     max_length=512, ...     output_keys=['input_ids', 'attention_mask'] ... )</p>"},{"location":"reference/configclasses/tokenizer_config/#tmlc.configclasses.tokenizer_config.TokenizerConfig--tokenize-a-list-of-sentences","title":"Tokenize a list of sentences","text":"<p>sentences = ['Hello world!', 'This is a test.'] tokenized_data = tokenizer_config(sentences)</p>"},{"location":"reference/configclasses/tokenizer_config/#tmlc.configclasses.tokenizer_config.TokenizerConfig--print-the-tokenized-data","title":"Print the tokenized data","text":"<p>print(tokenized_data) Output: {     'input_ids': [[101, 7592, 2088, 999, 102], [101, 2023, 2003, 1037, 3231, 1012, 102]],     'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]] }</p>"},{"location":"reference/configclasses/tokenizer_config/#tmlc.configclasses.tokenizer_config.TokenizerConfig.tokenizer","title":"<code>tokenizer</code>  <code>property</code>","text":"<p>Initialize the tokenizer.</p>"},{"location":"reference/configclasses/tokenizer_config/#tmlc.configclasses.tokenizer_config.TokenizerConfig.__call__","title":"<code>__call__(data)</code>","text":"<p>Tokenize the input data using the tokenizer.</p> Source code in <code>tmlc/configclasses/tokenizer_config.py</code> <pre><code>def __call__(self, data):\n\"\"\"\n    Tokenize the input data using the tokenizer.\n    \"\"\"\n    kwargs = self.kwargs or {}\n    output = self.tokenizer(data, **kwargs)\n    return {key: output[key] for key in self.output_keys}\n</code></pre>"},{"location":"reference/configclasses/trainer_config/","title":"trainer_config","text":""},{"location":"reference/configclasses/trainer_config/#tmlc.configclasses.trainer_config.TrainerConfig","title":"<code>TrainerConfig</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Configuration class for the PyTorch Lightning trainer.</p> <p>Parameters:</p> Name Type Description Default <code>lightning_module_config</code> <code>LightningModuleConfig</code> <p>Configuration for the LightningModule.</p> required <code>data_module_config</code> <code>DataModuleConfig</code> <p>Configuration for the DataModule.</p> required <code>mlflow_config</code> <code>MlflowConfig</code> <p>Configuration for MLflow tracking.</p> required <code>callbacks</code> <code>List[PartialFunctionConfig]</code> <p>List of callbacks to use during training.</p> required <code>loggers</code> <code>List[PartialFunctionConfig]</code> <p>List of loggers to use during training.</p> required <code>kwargs</code> <code>Optional[dict]</code> <p>Additional keyword arguments for the trainer. Defaults to None.</p> required <code>seed</code> <code>Optional[int]</code> <p>The random seed to use. Defaults to 42.</p> required <code>config_path</code> <code>Optional[str]</code> <p>Path to the YAML file containing the configuration. Defaults to None.</p> required Source code in <code>tmlc/configclasses/trainer_config.py</code> <pre><code>class TrainerConfig(BaseModel):\n\"\"\"\n    Configuration class for the PyTorch Lightning trainer.\n\n    Args:\n        lightning_module_config (LightningModuleConfig):\n            Configuration for the LightningModule.\n        data_module_config (DataModuleConfig):\n            Configuration for the DataModule.\n        mlflow_config (MlflowConfig):\n            Configuration for MLflow tracking.\n        callbacks (List[PartialFunctionConfig]):\n            List of callbacks to use during training.\n        loggers (List[PartialFunctionConfig]):\n            List of loggers to use during training.\n        kwargs (Optional[dict], optional):\n            Additional keyword arguments for the trainer.\n            Defaults to None.\n        seed (Optional[int], optional):\n            The random seed to use.\n            Defaults to 42.\n        config_path (Optional[str], optional):\n            Path to the YAML file containing the configuration.\n            Defaults to None.\n    \"\"\"\n\n    lightning_module_config: LightningModuleConfig\n    data_module_config: DataModuleConfig\n    mlflow_config: MlflowConfig\n    callbacks: List[PartialFunctionConfig]\n    loggers: List[PartialFunctionConfig]\n    kwargs: Optional[dict] = None\n    seed: Optional[int] = 42\n    config_path: Optional[str] = None\n\n    def __post_init__(self) -&gt; None:\n\"\"\"\n        Initializes any optional parameters if they were not specified.\n        \"\"\"\n        self.kwargs = self.kwargs or {}\n\n    @classmethod\n    def from_yaml(cls, file_path: str) -&gt; \"TrainerConfig\":\n\"\"\"\n        Loads the configuration from a YAML file.\n\n        Args:\n            file_path (str): The path to the YAML file.\n\n        Returns:\n            TrainerConfig: The configuration object.\n        \"\"\"\n        with open(file_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n            config_dict[\"trainer_config\"][\"config_path\"] = file_path\n\n        return cls(**config_dict[\"trainer_config\"])\n</code></pre>"},{"location":"reference/configclasses/trainer_config/#tmlc.configclasses.trainer_config.TrainerConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Initializes any optional parameters if they were not specified.</p> Source code in <code>tmlc/configclasses/trainer_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"\n    Initializes any optional parameters if they were not specified.\n    \"\"\"\n    self.kwargs = self.kwargs or {}\n</code></pre>"},{"location":"reference/configclasses/trainer_config/#tmlc.configclasses.trainer_config.TrainerConfig.from_yaml","title":"<code>from_yaml(file_path)</code>  <code>classmethod</code>","text":"<p>Loads the configuration from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the YAML file.</p> required <p>Returns:</p> Name Type Description <code>TrainerConfig</code> <code>TrainerConfig</code> <p>The configuration object.</p> Source code in <code>tmlc/configclasses/trainer_config.py</code> <pre><code>@classmethod\ndef from_yaml(cls, file_path: str) -&gt; \"TrainerConfig\":\n\"\"\"\n    Loads the configuration from a YAML file.\n\n    Args:\n        file_path (str): The path to the YAML file.\n\n    Returns:\n        TrainerConfig: The configuration object.\n    \"\"\"\n    with open(file_path, \"r\") as f:\n        config_dict = yaml.safe_load(f)\n        config_dict[\"trainer_config\"][\"config_path\"] = file_path\n\n    return cls(**config_dict[\"trainer_config\"])\n</code></pre>"},{"location":"reference/dataclasses/","title":"dataclasses","text":""},{"location":"reference/dataclasses/datamodule/","title":"datamodule","text":""},{"location":"reference/dataclasses/datamodule/#tmlc.dataclasses.datamodule.DataModule","title":"<code>DataModule</code>","text":"<p>         Bases: <code>pl.LightningDataModule</code></p> <p>A PyTorch Lightning DataModule for loading and preparing data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DataModuleConfig</code> <p>A configuration object specifying data loading, preparation, and processing settings.</p> required <p>Attributes:</p> Name Type Description <code>config</code> <code>DataModuleConfig</code> <p>A configuration object specifying data loading, preparation, and processing settings.</p> <code>train_data</code> <code>Optional</code> <p>Training data.</p> <code>val_data</code> <code>Optional</code> <p>Validation data.</p> <code>test_data</code> <code>Optional</code> <p>Testing data.</p> <p>Examples:</p> <p>To use this DataModule, first create an instance of <code>DataModuleConfig</code> with the desired settings for data loading, preparation, and processing. Then, pass that instance to the <code>DataModule</code> constructor:</p> <pre><code>&gt;&gt;&gt; from tmlc.configclasses import DataModuleConfig\n&gt;&gt;&gt; config = DataModuleConfig()\n&gt;&gt;&gt; data_module = DataModule(config)\n</code></pre> <p>After creating the <code>DataModule</code> instance, you can pass it to a PyTorch Lightning <code>Trainer</code> instance to train your model:</p> <pre><code>&gt;&gt;&gt; from pytorch_lightning import Trainer\n&gt;&gt;&gt; trainer = Trainer(gpus=1)\n&gt;&gt;&gt; trainer.fit(model, datamodule=data_module)\n</code></pre> Source code in <code>tmlc/dataclasses/datamodule.py</code> <pre><code>class DataModule(pl.LightningDataModule):\n\"\"\"\n    A PyTorch Lightning DataModule for loading and preparing data.\n\n    Args:\n        config (DataModuleConfig): A configuration object specifying data loading, preparation,\n            and processing settings.\n\n    Attributes:\n        config (DataModuleConfig): A configuration object specifying data loading, preparation,\n            and processing settings.\n        train_data (Optional): Training data.\n        val_data (Optional): Validation data.\n        test_data (Optional): Testing data.\n\n    Examples:\n        To use this DataModule, first create an instance of `DataModuleConfig` with the desired settings\n        for data loading, preparation, and processing. Then, pass that instance to the `DataModule`\n        constructor:\n\n        &gt;&gt;&gt; from tmlc.configclasses import DataModuleConfig\n        &gt;&gt;&gt; config = DataModuleConfig()\n        &gt;&gt;&gt; data_module = DataModule(config)\n\n        After creating the `DataModule` instance, you can pass it to a PyTorch Lightning `Trainer`\n        instance to train your model:\n\n        &gt;&gt;&gt; from pytorch_lightning import Trainer\n        &gt;&gt;&gt; trainer = Trainer(gpus=1)\n        &gt;&gt;&gt; trainer.fit(model, datamodule=data_module)\n    \"\"\"\n\n    def __init__(self, config: DataModuleConfig):\n\"\"\"\n        Initialize a new instance of DataModule.\n\n        Args:\n            config (DataModuleConfig): A configuration object specifying data loading, preparation, and\n                processing settings.\n        \"\"\"\n        logger.info(f\"Initialize DataModule with config: {config}\")\n\n        super().__init__()\n        self.config = config\n        self.train_data = None\n        self.val_data = None\n        self.test_data = None\n\n    def prepare_data(self) -&gt; None:\n\"\"\"\n        A method that runs once at the very beginning of training. It is responsible for downloading\n        and processing data.\n\n        In this implementation, this method is not needed because the data is already preprocessed.\n        \"\"\"\n        pass\n\n    def setup(self, stage=None) -&gt; None:\n\"\"\"\n        A method that runs once per process at the beginning of training. It is responsible for\n        splitting data into training, validation, and test sets.\n\n        Args:\n            stage (Optional): If specified, this argument tells whether we're at the \"fit\" stage\n                or the \"test\" stage.\n        \"\"\"\n        # Load data\n        self._data = {}\n        if self.config.state_file:\n            try:\n                state_dict = torch.load(self.config.state_file)\n                self.config.dataset = state_dict[\"config\"].dataset\n                for element in [\"train\", \"val\", \"test\"]:\n                    self._data[element] = state_dict[f\"{element}_data\"]\n                logger.info(f\"Loaded datasets with config: {self.config}\")\n                return\n            except FileNotFoundError:\n                logger.debug(\"State file not found.\")\n                logger.info(\"Continuing with setup without load.\")\n            except Exception as e:\n                logger.debug(f\"Error loading state: {e}.\")\n                logger.info(\"Continuing with setup without load.\")\n\n        data = self.config.get_data.partial()\n        self._data[\"train\"], self._data[\"val\"], self._data[\"test\"] = self.config.split.partial(data)\n\n    def _dataloader(self, element: str) -&gt; DataLoader:\n\"\"\"\n        Helper method for creating a DataLoader from a given dataset.\n\n        Args:\n            element (str): One of \"train\", \"val\", or \"test\".\n\n        Returns:\n            DataLoader: A DataLoader object that can be used for iterating over the data.\n        \"\"\"\n        messages = self.config.process_data.partial(self._data[element])\n        dataset = Dataset(messages=messages, config=self.config.dataset)\n        kwargs = self.config.dataset.kwargs or {}\n        return DataLoader(dataset, batch_size=self.config.dataset.batch_size, **kwargs)\n\n    def train_dataloader(self) -&gt; DataLoader:\n\"\"\"\n        Method that returns a DataLoader for the training set.\n\n        Returns:\n            DataLoader: A DataLoader object that can be used for iterating over the training data.\n        \"\"\"\n        return self._dataloader(element=\"train\")\n\n    def val_dataloader(self) -&gt; DataLoader:\n\"\"\"\n        Method that returns a DataLoader for the validation set.\n\n        Returns:\n            DataLoader: A DataLoader object that can be used for iterating over the validation data.\n        \"\"\"\n        return self._dataloader(element=\"val\")\n\n    def test_dataloader(self) -&gt; DataLoader:\n\"\"\"\n        Method that returns a DataLoader for the test set.\n\n        Returns:\n            DataLoader: A DataLoader object that can be used for iterating over the test data.\n        \"\"\"\n        return self._dataloader(element=\"test\")\n</code></pre>"},{"location":"reference/dataclasses/datamodule/#tmlc.dataclasses.datamodule.DataModule.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize a new instance of DataModule.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DataModuleConfig</code> <p>A configuration object specifying data loading, preparation, and processing settings.</p> required Source code in <code>tmlc/dataclasses/datamodule.py</code> <pre><code>def __init__(self, config: DataModuleConfig):\n\"\"\"\n    Initialize a new instance of DataModule.\n\n    Args:\n        config (DataModuleConfig): A configuration object specifying data loading, preparation, and\n            processing settings.\n    \"\"\"\n    logger.info(f\"Initialize DataModule with config: {config}\")\n\n    super().__init__()\n    self.config = config\n    self.train_data = None\n    self.val_data = None\n    self.test_data = None\n</code></pre>"},{"location":"reference/dataclasses/datamodule/#tmlc.dataclasses.datamodule.DataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>A method that runs once at the very beginning of training. It is responsible for downloading and processing data.</p> <p>In this implementation, this method is not needed because the data is already preprocessed.</p> Source code in <code>tmlc/dataclasses/datamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n\"\"\"\n    A method that runs once at the very beginning of training. It is responsible for downloading\n    and processing data.\n\n    In this implementation, this method is not needed because the data is already preprocessed.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/dataclasses/datamodule/#tmlc.dataclasses.datamodule.DataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>A method that runs once per process at the beginning of training. It is responsible for splitting data into training, validation, and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional</code> <p>If specified, this argument tells whether we're at the \"fit\" stage or the \"test\" stage.</p> <code>None</code> Source code in <code>tmlc/dataclasses/datamodule.py</code> <pre><code>def setup(self, stage=None) -&gt; None:\n\"\"\"\n    A method that runs once per process at the beginning of training. It is responsible for\n    splitting data into training, validation, and test sets.\n\n    Args:\n        stage (Optional): If specified, this argument tells whether we're at the \"fit\" stage\n            or the \"test\" stage.\n    \"\"\"\n    # Load data\n    self._data = {}\n    if self.config.state_file:\n        try:\n            state_dict = torch.load(self.config.state_file)\n            self.config.dataset = state_dict[\"config\"].dataset\n            for element in [\"train\", \"val\", \"test\"]:\n                self._data[element] = state_dict[f\"{element}_data\"]\n            logger.info(f\"Loaded datasets with config: {self.config}\")\n            return\n        except FileNotFoundError:\n            logger.debug(\"State file not found.\")\n            logger.info(\"Continuing with setup without load.\")\n        except Exception as e:\n            logger.debug(f\"Error loading state: {e}.\")\n            logger.info(\"Continuing with setup without load.\")\n\n    data = self.config.get_data.partial()\n    self._data[\"train\"], self._data[\"val\"], self._data[\"test\"] = self.config.split.partial(data)\n</code></pre>"},{"location":"reference/dataclasses/datamodule/#tmlc.dataclasses.datamodule.DataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Method that returns a DataLoader for the test set.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>A DataLoader object that can be used for iterating over the test data.</p> Source code in <code>tmlc/dataclasses/datamodule.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"\n    Method that returns a DataLoader for the test set.\n\n    Returns:\n        DataLoader: A DataLoader object that can be used for iterating over the test data.\n    \"\"\"\n    return self._dataloader(element=\"test\")\n</code></pre>"},{"location":"reference/dataclasses/datamodule/#tmlc.dataclasses.datamodule.DataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Method that returns a DataLoader for the training set.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>A DataLoader object that can be used for iterating over the training data.</p> Source code in <code>tmlc/dataclasses/datamodule.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"\n    Method that returns a DataLoader for the training set.\n\n    Returns:\n        DataLoader: A DataLoader object that can be used for iterating over the training data.\n    \"\"\"\n    return self._dataloader(element=\"train\")\n</code></pre>"},{"location":"reference/dataclasses/datamodule/#tmlc.dataclasses.datamodule.DataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Method that returns a DataLoader for the validation set.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>A DataLoader object that can be used for iterating over the validation data.</p> Source code in <code>tmlc/dataclasses/datamodule.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n\"\"\"\n    Method that returns a DataLoader for the validation set.\n\n    Returns:\n        DataLoader: A DataLoader object that can be used for iterating over the validation data.\n    \"\"\"\n    return self._dataloader(element=\"val\")\n</code></pre>"},{"location":"reference/dataclasses/dataset/","title":"dataset","text":""},{"location":"reference/dataclasses/dataset/#tmlc.dataclasses.dataset.Dataset","title":"<code>Dataset</code>","text":"<p>         Bases: <code>Dataset</code></p> <p>A custom dataset class that can be used with PyTorch DataLoader to load email data.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>List[Message]</code> <p>A list of Message objects containing the email data.</p> <code>config</code> <code>DatasetConfig</code> <p>A DatasetConfig object containing configuration information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from tmlc.dataclasses import Message\n&gt;&gt;&gt; from tmlc.configclasses import DatasetConfig\n&gt;&gt;&gt; from tmlc.datasets import Dataset\n&gt;&gt;&gt; import transformers\n&gt;&gt;&gt; messages = [Message(text='This is a test email', labels=['spam']),\n...             Message(text='Another email for testing', labels=['not_spam'])]\n&gt;&gt;&gt; tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n&gt;&gt;&gt; dataset_config = DatasetConfig(tokenizer=tokenizer, batch_size=2, kwargs={'num_workers': 4})\n&gt;&gt;&gt; dataset = Dataset(messages, dataset_config)\n&gt;&gt;&gt; dataloader = torch.utils.data.DataLoader(dataset, batch_size=dataset_config.batch_size,\n...                                          num_workers=dataset_config.kwargs.get('num_workers', 0))\n&gt;&gt;&gt; for batch in dataloader:\n...     print(batch['input_ids'], batch['attention_mask'], batch['labels'])\ntensor([[  101,  2023,  2003,  1037,  3231,  2758,   102],\n        [ 2066,  2758,  2005,  5607,   102,     0,     0]])\ntensor([[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0]])\ntensor([[1., 0.], [0., 1.]])\n</code></pre> Source code in <code>tmlc/dataclasses/dataset.py</code> <pre><code>class Dataset(Dataset):\n\"\"\"\n    A custom dataset class that can be used with PyTorch DataLoader to load email data.\n\n    Attributes:\n        messages (List[Message]): A list of Message objects containing the email data.\n        config (DatasetConfig): A DatasetConfig object containing configuration information.\n\n    Examples:\n        &gt;&gt;&gt; from tmlc.dataclasses import Message\n        &gt;&gt;&gt; from tmlc.configclasses import DatasetConfig\n        &gt;&gt;&gt; from tmlc.datasets import Dataset\n        &gt;&gt;&gt; import transformers\n        &gt;&gt;&gt; messages = [Message(text='This is a test email', labels=['spam']),\n        ...             Message(text='Another email for testing', labels=['not_spam'])]\n        &gt;&gt;&gt; tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n        &gt;&gt;&gt; dataset_config = DatasetConfig(tokenizer=tokenizer, batch_size=2, kwargs={'num_workers': 4})\n        &gt;&gt;&gt; dataset = Dataset(messages, dataset_config)\n        &gt;&gt;&gt; dataloader = torch.utils.data.DataLoader(dataset, batch_size=dataset_config.batch_size,\n        ...                                          num_workers=dataset_config.kwargs.get('num_workers', 0))\n        &gt;&gt;&gt; for batch in dataloader:\n        ...     print(batch['input_ids'], batch['attention_mask'], batch['labels'])\n        tensor([[  101,  2023,  2003,  1037,  3231,  2758,   102],\n                [ 2066,  2758,  2005,  5607,   102,     0,     0]])\n        tensor([[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0]])\n        tensor([[1., 0.], [0., 1.]])\n    \"\"\"\n\n    def __init__(self, messages: List[Message], config: DatasetConfig):\n\"\"\"\n        Initializes the dataset.\n\n        Args:\n            messages (List[Message]): A list of Message objects containing the email data.\n            config (DatasetConfig): A DatasetConfig object containing configuration information.\n        \"\"\"\n        self.messages = messages\n        self.config = config\n\n    def __len__(self) -&gt; int:\n\"\"\"\n        Returns the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n        \"\"\"\n        return len(self.messages)\n\n    def __getitem__(self, index: int) -&gt; dict:\n\"\"\"\n        Gets an item from the dataset at the specified index.\n\n        Args:\n            index (int): The index of the item to get.\n\n        Returns:\n            dict: A dictionary containing the input_ids, attention_mask, and labels for the item.\n        \"\"\"\n\n        message = self.messages[index]\n        encoding = self.config.tokenizer(message.text)\n        output = {key: torch.tensor(value) for key, value in encoding.items()}\n\n        # Convert bool to int.\n        labels = list(map(int, message.labels))\n        output.update({\"labels\": torch.FloatTensor(labels)})\n        return output\n</code></pre>"},{"location":"reference/dataclasses/dataset/#tmlc.dataclasses.dataset.Dataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Gets an item from the dataset at the specified index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to get.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the input_ids, attention_mask, and labels for the item.</p> Source code in <code>tmlc/dataclasses/dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; dict:\n\"\"\"\n    Gets an item from the dataset at the specified index.\n\n    Args:\n        index (int): The index of the item to get.\n\n    Returns:\n        dict: A dictionary containing the input_ids, attention_mask, and labels for the item.\n    \"\"\"\n\n    message = self.messages[index]\n    encoding = self.config.tokenizer(message.text)\n    output = {key: torch.tensor(value) for key, value in encoding.items()}\n\n    # Convert bool to int.\n    labels = list(map(int, message.labels))\n    output.update({\"labels\": torch.FloatTensor(labels)})\n    return output\n</code></pre>"},{"location":"reference/dataclasses/dataset/#tmlc.dataclasses.dataset.Dataset.__init__","title":"<code>__init__(messages, config)</code>","text":"<p>Initializes the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>A list of Message objects containing the email data.</p> required <code>config</code> <code>DatasetConfig</code> <p>A DatasetConfig object containing configuration information.</p> required Source code in <code>tmlc/dataclasses/dataset.py</code> <pre><code>def __init__(self, messages: List[Message], config: DatasetConfig):\n\"\"\"\n    Initializes the dataset.\n\n    Args:\n        messages (List[Message]): A list of Message objects containing the email data.\n        config (DatasetConfig): A DatasetConfig object containing configuration information.\n    \"\"\"\n    self.messages = messages\n    self.config = config\n</code></pre>"},{"location":"reference/dataclasses/dataset/#tmlc.dataclasses.dataset.Dataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The length of the dataset.</p> Source code in <code>tmlc/dataclasses/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"\n    Returns the length of the dataset.\n\n    Returns:\n        int: The length of the dataset.\n    \"\"\"\n    return len(self.messages)\n</code></pre>"},{"location":"reference/dataclasses/message/","title":"message","text":""},{"location":"reference/dataclasses/message/#tmlc.dataclasses.message.Message","title":"<code>Message</code>  <code>dataclass</code>","text":"<p>A dataclass that holds a message text and its labels.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>The message text.</p> <code>labels</code> <code>List[int]</code> <p>The list of labels for the message.</p> Source code in <code>tmlc/dataclasses/message.py</code> <pre><code>@dataclass\nclass Message:\n\"\"\"\n    A dataclass that holds a message text and its labels.\n\n    Attributes:\n        text (str): The message text.\n        labels (List[int]): The list of labels for the message.\n    \"\"\"\n\n    text: str\n    labels: List[bool]\n</code></pre>"},{"location":"reference/dataclasses/message/#tmlc.dataclasses.message.Messages","title":"<code>Messages</code>  <code>dataclass</code>","text":"<p>A dataclass that holds a list of messages and their labels.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>List[Message]</code> <p>The list of messages.</p> <code>labels_names</code> <code>List[str]</code> <p>The list of label names for the messages.</p> Source code in <code>tmlc/dataclasses/message.py</code> <pre><code>@dataclass\nclass Messages:\n\"\"\"\n    A dataclass that holds a list of messages and their labels.\n\n    Attributes:\n        messages (List[Message]):\n            The list of messages.\n        labels_names (List[str]):\n            The list of label names for the messages.\n    \"\"\"\n\n    messages: List[Message]\n    labels_names: List[str]\n\n    def __len__(self):\n\"\"\"\n        Returns the number of messages in the dataset.\n\n        Returns:\n            int:\n                The number of messages.\n        \"\"\"\n        return len(self.messages)\n\n    def __getitem__(self, index: int) -&gt; dict:\n\"\"\"\n        Returns the message at the given index.\n\n        Args:\n            index (int):\n                The index of the message to return.\n\n        Returns:\n            Message:\n                The message at the given index.\n        \"\"\"\n        return self.messages[index]\n</code></pre>"},{"location":"reference/dataclasses/message/#tmlc.dataclasses.message.Messages.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns the message at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the message to return.</p> required <p>Returns:</p> Name Type Description <code>Message</code> <code>dict</code> <p>The message at the given index.</p> Source code in <code>tmlc/dataclasses/message.py</code> <pre><code>def __getitem__(self, index: int) -&gt; dict:\n\"\"\"\n    Returns the message at the given index.\n\n    Args:\n        index (int):\n            The index of the message to return.\n\n    Returns:\n        Message:\n            The message at the given index.\n    \"\"\"\n    return self.messages[index]\n</code></pre>"},{"location":"reference/dataclasses/message/#tmlc.dataclasses.message.Messages.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of messages in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The number of messages.</p> Source code in <code>tmlc/dataclasses/message.py</code> <pre><code>def __len__(self):\n\"\"\"\n    Returns the number of messages in the dataset.\n\n    Returns:\n        int:\n            The number of messages.\n    \"\"\"\n    return len(self.messages)\n</code></pre>"},{"location":"reference/eda/","title":"eda","text":""},{"location":"reference/eda/configclasses/","title":"configclasses","text":""},{"location":"reference/eda/datapreparation/","title":"datapreparation","text":""},{"location":"reference/eda/datapreparation/#tmlc.eda.datapreparation.calculate_co_occurrence_matrix","title":"<code>calculate_co_occurrence_matrix(data, unique_labels)</code>","text":"<p>Calculate the co-occurrence matrix of labels in a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>The input DataFrame with a 'labels' column containing lists of labels.</p> required <code>unique_labels</code> <code>List[str]</code> <p>A list of unique labels.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The co-occurrence matrix of size (label_count, label_count).</p> Source code in <code>tmlc/eda/datapreparation.py</code> <pre><code>def calculate_co_occurrence_matrix(data: pd.DataFrame, unique_labels: List[str]) -&gt; pd.DataFrame:\n\"\"\"\n    Calculate the co-occurrence matrix of labels in a pandas DataFrame.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with a 'labels' column containing lists of labels.\n        unique_labels (List[str]): A list of unique labels.\n\n    Returns:\n        pd.DataFrame: The co-occurrence matrix of size (label_count, label_count).\n    \"\"\"\n    co_occurrence_matrix = pd.DataFrame(index=unique_labels, columns=unique_labels)\n\n    for labels in data[\"labels\"]:\n        for label1 in labels:\n            if label1 not in unique_labels:\n                continue\n            for label2 in labels:\n                if label2 not in unique_labels:\n                    continue\n                co_occurrence_matrix.loc[label1, label2] += 1\n\n    return co_occurrence_matrix\n</code></pre>"},{"location":"reference/eda/pretraining/","title":"pretraining","text":""},{"location":"reference/eda/pretraining/#tmlc.eda.pretraining.evaluate_predictions","title":"<code>evaluate_predictions(clf, X_val_transformed, y_val, X_test_transformed, y_test)</code>","text":"<p>Evaluate the predictions of a classifier on the validation and test datasets.</p> <p>Parameters:</p> Name Type Description Default <code>clf</code> <code>Any</code> <p>A classifier object.</p> required <code>X_val_transformed</code> <code>np.ndarray</code> <p>The transformed validation input data.</p> required <code>y_val</code> <code>np.ndarray</code> <p>The validation output data.</p> required <code>X_test_transformed</code> <code>np.ndarray</code> <p>The transformed test input data.</p> required <code>y_test</code> <code>np.ndarray</code> <p>The test output data.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: A dictionary containing the F1 scores for the validation and test predictions.</p> Source code in <code>tmlc/eda/pretraining.py</code> <pre><code>def evaluate_predictions(clf, X_val_transformed, y_val, X_test_transformed, y_test) -&gt; Dict[str, float]:\n\"\"\"\n    Evaluate the predictions of a classifier on the validation and test datasets.\n\n    Args:\n        clf (Any): A classifier object.\n        X_val_transformed (np.ndarray): The transformed validation input data.\n        y_val (np.ndarray): The validation output data.\n        X_test_transformed (np.ndarray): The transformed test input data.\n        y_test (np.ndarray): The test output data.\n\n    Returns:\n        Dict[str, float]: A dictionary containing the F1 scores for the validation and\n            test predictions.\n    \"\"\"\n    val_predictions = clf.predict(X_val_transformed)\n    test_predictions = clf.predict(X_test_transformed)\n    return {\n        \"val_f1\": f1_score(y_val, val_predictions),\n        \"test_f1\": f1_score(y_test, test_predictions),\n    }\n</code></pre>"},{"location":"reference/eda/pretraining/#tmlc.eda.pretraining.find_best_classifier","title":"<code>find_best_classifier(clf, X_train, y_train, hyperparams)</code>","text":"<p>Find the best classifier and its hyperparameters using GridSearchCV.</p> <p>Parameters:</p> Name Type Description Default <code>clf</code> <code>Any</code> <p>A classifier object.</p> required <code>X_train</code> <code>np.ndarray</code> <p>A numpy array containing the training input data.</p> required <code>y_train</code> <code>np.ndarray</code> <p>A numpy array containing the training output data.</p> required <code>hyperparams</code> <code>Dict[str, List]</code> <p>A dictionary containing the hyperparameters for the classifier.</p> required <p>Returns:</p> Type Description <code>Tuple[Any, Dict[str, Any]]</code> <p>Tuple[Any, Dict[str, Any]]: A tuple containing the best classifier and the best hyperparameters.</p> Source code in <code>tmlc/eda/pretraining.py</code> <pre><code>def find_best_classifier(\n    clf: Any, X_train: np.ndarray, y_train: np.ndarray, hyperparams: Dict[str, List]\n) -&gt; Tuple[Any, Dict[str, Any]]:\n\"\"\"\n    Find the best classifier and its hyperparameters using GridSearchCV.\n\n    Args:\n        clf (Any): A classifier object.\n        X_train (np.ndarray): A numpy array containing the training input data.\n        y_train (np.ndarray): A numpy array containing the training output data.\n        hyperparams (Dict[str, List]): A dictionary containing the hyperparameters\n            for the classifier.\n\n    Returns:\n        Tuple[Any, Dict[str, Any]]: A tuple containing the best classifier and the\n            best hyperparameters.\n    \"\"\"\n    grid_search = GridSearchCV(clf, hyperparams, cv=5)\n    grid_search.fit(X_train, y_train)\n    return grid_search.best_estimator_, grid_search.best_params_\n</code></pre>"},{"location":"reference/eda/pretraining/#tmlc.eda.pretraining.get_input_and_attention_masks","title":"<code>get_input_and_attention_masks(X, tokenizer)</code>","text":"<p>Get the input IDs and attention masks for a given Pandas Series of text data and tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pd.Series</code> <p>A Pandas Series containing text data.</p> required <code>tokenizer</code> <code>Any</code> <p>A tokenizer object for a specific transformer model.</p> required <p>Returns:</p> Type Description <code>Tuple[List[torch.Tensor], List[torch.Tensor]]</code> <p>Tuple[List[torch.Tensor], List[torch.Tensor]]: A tuple containing two lists of torch tensors - input IDs and attention masks.</p> Source code in <code>tmlc/eda/pretraining.py</code> <pre><code>def get_input_and_attention_masks(\n    X: pd.Series, tokenizer: Any\n) -&gt; Tuple[List[torch.Tensor], List[torch.Tensor]]:\n\"\"\"\n    Get the input IDs and attention masks for a given Pandas Series of text data and tokenizer.\n\n    Args:\n        X (pd.Series): A Pandas Series containing text data.\n        tokenizer (Any): A tokenizer object for a specific transformer model.\n\n    Returns:\n        Tuple[List[torch.Tensor], List[torch.Tensor]]: A tuple containing two lists of\n            torch tensors - input IDs and attention masks.\n    \"\"\"\n    input_ids, attention_masks = [], []\n    for text in X:\n        encoded = tokenizer(text)\n        input_ids.append(encoded[\"input_ids\"])\n        attention_masks.append(encoded[\"attention_mask\"])\n\n    return input_ids, attention_masks\n</code></pre>"},{"location":"reference/eda/pretraining/#tmlc.eda.pretraining.get_transformer_embeddings","title":"<code>get_transformer_embeddings(transformer_model, X)</code>","text":"<p>Get the transformer embeddings for a given transformer model and Pandas Series of text data.</p> <p>Parameters:</p> Name Type Description Default <code>transformer_model</code> <code>TransformerModelConfig</code> <p>A TransformerModelConfig object containing the transformer model and tokenizer.</p> required <code>X</code> <code>pd.Series</code> <p>A Pandas Series containing text data.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: A numpy array containing the transformer embeddings for the input text data.</p> Source code in <code>tmlc/eda/pretraining.py</code> <pre><code>def get_transformer_embeddings(transformer_model: TransformerModelConfig, X: pd.Series) -&gt; np.ndarray:\n\"\"\"\n    Get the transformer embeddings for a given transformer model and Pandas Series of text data.\n\n    Args:\n        transformer_model (TransformerModelConfig): A TransformerModelConfig object\n            containing the transformer model and tokenizer.\n        X (pd.Series): A Pandas Series containing text data.\n\n    Returns:\n        np.ndarray: A numpy array containing the transformer embeddings for the input\n            text data.\n    \"\"\"\n    input_ids, attention_masks = get_input_and_attention_masks(X, transformer_model.tokenizer)\n\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    with torch.no_grad():\n        embeddings: torch.Tensor = transformer_model.pretrainedmodel.model(input_ids, attention_masks)[0]\n\n    embeddings_flat = embeddings.view(embeddings.shape[0], -1)\n    return embeddings_flat.numpy()\n</code></pre>"},{"location":"reference/eda/pretraining/#tmlc.eda.pretraining.train_and_evaluate","title":"<code>train_and_evaluate(transformer_model, classifiers, X_train, y_train, X_val, y_val, X_test, y_test)</code>","text":"<p>Train and evaluate multiple classifiers on the given data using a specific transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>transformer_model</code> <code>TransformerModelConfig</code> <p>A TransformerModelConfig object containing the transformer model and tokenizer.</p> required <code>classifiers</code> <code>List[SklearnClassifiersConfig]</code> <p>A list of SklearnClassifiersConfig objects containing classifiers and their hyperparameters.</p> required <code>X_train</code> <code>np.ndarray</code> <p>The training input data.</p> required <code>y_train</code> <code>np.ndarray</code> <p>The training output data.</p> required <code>X_val</code> <code>np.ndarray</code> <p>The validation input data.</p> required <code>y_val</code> <code>np.ndarray</code> <p>The validation output data.</p> required <code>X_test</code> <code>np.ndarray</code> <p>The test input data.</p> required <code>y_test</code> <code>np.ndarray</code> <p>The test output data.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the evaluation results for each classifier.</p> Source code in <code>tmlc/eda/pretraining.py</code> <pre><code>def train_and_evaluate(\n    transformer_model: TransformerModelConfig,\n    classifiers: List[SklearnClassifiersConfig],\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_val: np.ndarray,\n    y_val: np.ndarray,\n    X_test: np.ndarray,\n    y_test: np.ndarray,\n) -&gt; Dict[str, Any]:\n\"\"\"\n    Train and evaluate multiple classifiers on the given data using a specific transformer model.\n\n    Args:\n        transformer_model (TransformerModelConfig): A TransformerModelConfig object\n            containing the transformer model and tokenizer.\n        classifiers (List[SklearnClassifiersConfig]): A list of SklearnClassifiersConfig\n            objects containing classifiers and their hyperparameters.\n        X_train (np.ndarray): The training input data.\n        y_train (np.ndarray): The training output data.\n        X_val (np.ndarray): The validation input data.\n        y_val (np.ndarray): The validation output data.\n        X_test (np.ndarray): The test input data.\n        y_test (np.ndarray): The test output data.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the evaluation results for each classifier.\n    \"\"\"\n    transformer_embeddings = transform_data(transformer_model, X_train, X_val, X_test)\n    results = {}\n    for clf_config in classifiers:\n        best_estimator, metrics = train_and_evaluate_classifier(\n            clf_config,\n            transformer_embeddings[0],\n            y_train,\n            transformer_embeddings[1],\n            y_val,\n            transformer_embeddings[2],\n            y_test,\n        )\n        results[clf_config.clf.func] = {\"hyperparams\": best_estimator.get_params(), \"metrics\": metrics}\n\n    return results\n</code></pre>"},{"location":"reference/eda/pretraining/#tmlc.eda.pretraining.train_and_evaluate_classifier","title":"<code>train_and_evaluate_classifier(clf_config, X_train, y_train, X_val, y_val, X_test, y_test)</code>","text":"<p>Train and evaluate a classifier on the given data.</p> <p>Parameters:</p> Name Type Description Default <code>clf_config</code> <code>SklearnClassifiersConfig</code> <p>A SklearnClassifiersConfig object containing the classifier and its hyperparameters.</p> required <code>X_train</code> <code>np.ndarray</code> <p>The training input data.</p> required <code>y_train</code> <code>np.ndarray</code> <p>The training output data.</p> required <code>X_val</code> <code>np.ndarray</code> <p>The validation input data.</p> required <code>y_val</code> <code>np.ndarray</code> <p>The validation output data.</p> required <code>X_test</code> <code>np.ndarray</code> <p>The test input data.</p> required <code>y_test</code> <code>np.ndarray</code> <p>The test output data.</p> required <p>Returns:</p> Type Description <code>Tuple[Any, Dict[str, float]]</code> <p>Tuple[Any, Dict[str, float]]: A tuple containing the best classifier and a dictionary of evaluation metrics.</p> Source code in <code>tmlc/eda/pretraining.py</code> <pre><code>def train_and_evaluate_classifier(\n    clf_config: SklearnClassifiersConfig,\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_val: np.ndarray,\n    y_val: np.ndarray,\n    X_test: np.ndarray,\n    y_test: np.ndarray,\n) -&gt; Tuple[Any, Dict[str, float]]:\n\"\"\"\n    Train and evaluate a classifier on the given data.\n\n    Args:\n        clf_config (SklearnClassifiersConfig): A SklearnClassifiersConfig object\n            containing the classifier and its hyperparameters.\n        X_train (np.ndarray): The training input data.\n        y_train (np.ndarray): The training output data.\n        X_val (np.ndarray): The validation input data.\n        y_val (np.ndarray): The validation output data.\n        X_test (np.ndarray): The test input data.\n        y_test (np.ndarray): The test output data.\n\n    Returns:\n        Tuple[Any, Dict[str, float]]: A tuple containing the best classifier and\n            a dictionary of evaluation metrics.\n    \"\"\"\n    clf = clf_config.clf.partial()\n    best_estimator, best_params = find_best_classifier(clf, X_train, y_train, clf_config.hyperparams)\n    best_estimator.set_params(**best_params)\n    best_estimator.fit(X_train, y_train)\n    metrics = evaluate_predictions(best_estimator, X_val, y_val, X_test, y_test)\n    return best_estimator, metrics\n</code></pre>"},{"location":"reference/eda/pretraining/#tmlc.eda.pretraining.train_and_evaluate_classifiers","title":"<code>train_and_evaluate_classifiers(config, data)</code>","text":"<p>Train and evaluate classifiers using a specific configuration and data.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EDAClassifiersEvaluationConfig</code> <p>A EDAClassifiersEvaluationConfig object containing configuration settings.</p> required <code>data</code> <code>pd.DataFrame</code> <p>A DataFrame containing the input data.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the evaluation results for each classifier and transformer model.</p> Source code in <code>tmlc/eda/pretraining.py</code> <pre><code>def train_and_evaluate_classifiers(\n    config: EDAClassifiersEvaluationConfig, data: pd.DataFrame\n) -&gt; Dict[str, Any]:\n\"\"\"\n    Train and evaluate classifiers using a specific configuration and data.\n\n    Args:\n        config (EDAClassifiersEvaluationConfig):\n            A EDAClassifiersEvaluationConfig object containing configuration settings.\n        data (pd.DataFrame): A DataFrame containing the input data.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the evaluation results for each\n            classifier and transformer model.\n    \"\"\"\n    logger.info(\"Loading configuration and data\")\n    results = {}\n    data = config.get_data.partial()\n    train_df, val_df, test_df = config.split_data.partial(data)\n    X_train = train_df[config.message_column]\n    X_val = val_df[config.message_column]\n    X_test = test_df[config.message_column]\n\n    for transformer in config.transformer_models:\n        for label in config.labels_columns:\n            if label not in results.keys():\n                results[label] = {}\n            logger.info(\n                f\"Training and evaluating classifiers for label: {label}, \\\n                    transformer model: {transformer.pretrainedmodel.path}\"\n            )\n            model_results = train_and_evaluate(\n                transformer,\n                config.classifiers,\n                X_train,\n                train_df[label],\n                X_val,\n                val_df[label],\n                X_test,\n                test_df[label],\n            )\n            results[label].update({transformer.pretrainedmodel.path: model_results})\n            logger.info(model_results)\n\n    logger.info(\"Evaluation completed successfully!\")\n    return results\n</code></pre>"},{"location":"reference/eda/pretraining/#tmlc.eda.pretraining.transform_data","title":"<code>transform_data(transformer_model, *data)</code>","text":"<p>Transform multiple sets of text data into transformer embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>transformer_model</code> <code>Any</code> <p>A transformer model object.</p> required <code>*data</code> <code>np.ndarray</code> <p>One or more numpy arrays containing text data.</p> <code>()</code> <p>Returns:</p> Type Description <code>List[np.ndarray]</code> <p>List[np.ndarray]: A list of numpy arrays containing transformer embeddings for the input text data.</p> Source code in <code>tmlc/eda/pretraining.py</code> <pre><code>def transform_data(transformer_model: Any, *data: np.ndarray) -&gt; List[np.ndarray]:\n\"\"\"\n    Transform multiple sets of text data into transformer embeddings.\n\n    Args:\n        transformer_model (Any): A transformer model object.\n        *data (np.ndarray): One or more numpy arrays containing text data.\n\n    Returns:\n        List[np.ndarray]: A list of numpy arrays containing transformer embeddings\n            for the input text data.\n    \"\"\"\n    return [get_transformer_embeddings(transformer_model, d) for d in data]\n</code></pre>"},{"location":"reference/eda/render/","title":"render","text":""},{"location":"reference/eda/render/#tmlc.eda.render.render_eda_output","title":"<code>render_eda_output(figures, unique_labels, results, output_file)</code>","text":"<p>Renders an EDA report template with the provided figures, labels, and metrics.</p> <p>Parameters:</p> Name Type Description Default <code>figures</code> <code>Dict[str, str]</code> <p>A dictionary of figure names and paths to the corresponding image files.</p> required <code>unique_labels</code> <code>List[str]</code> <p>A list of unique labels in the dataset.</p> required <code>results</code> <code>Dict[str, Any]</code> <p>A dictionary of EDA results.</p> required <code>output_file</code> <code>str</code> <p>The path to the output file to save the rendered report to.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tmlc/eda/render.py</code> <pre><code>def render_eda_output(\n    figures: Dict[str, str],\n    unique_labels: List[str],\n    results: Dict[str, Any],\n    output_file: str,\n) -&gt; None:\n\"\"\"\n    Renders an EDA report template with the provided figures, labels, and metrics.\n\n    Args:\n        figures (Dict[str, str]):\n            A dictionary of figure names and paths to the corresponding image files.\n        unique_labels (List[str]):\n            A list of unique labels in the dataset.\n        results (Dict[str, Any]):\n            A dictionary of EDA results.\n        output_file (str):\n            The path to the output file to save the rendered report to.\n\n    Returns:\n        None\n    \"\"\"\n    template_file = os.path.join(os.path.dirname(__file__), \"template.md\")\n    # Load the Markdown template from a file\n\n    with open(template_file) as f:\n        template_str = f.read()\n\n    # Create a Jinja2 template object\n    env = Environment()\n    env.filters[\"to_json\"] = to_json\n    template = env.from_string(template_str)\n\n    # Render the template with the necessary variables\n    rendered_template = template.render(\n        figures=figures,\n        unique_labels=unique_labels,\n        results=results\n    )\n\n    # Save the rendered Markdown to a file\n    with open(output_file, \"w\") as f:\n        f.write(rendered_template)\n</code></pre>"},{"location":"reference/eda/render/#tmlc.eda.render.to_json","title":"<code>to_json(value)</code>","text":"<p>Converts a Python object to a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The Python object to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The JSON string representation of the object.</p> Source code in <code>tmlc/eda/render.py</code> <pre><code>def to_json(value):\n\"\"\"\n    Converts a Python object to a JSON string.\n\n    Args:\n        value (Any):\n            The Python object to convert.\n\n    Returns:\n        str:\n            The JSON string representation of the object.\n    \"\"\"\n    try:\n        return json.dumps(value)\n    except Exception as e:\n        logger.error(e)\n        raise e\n</code></pre>"},{"location":"reference/eda/visualization/","title":"visualization","text":""},{"location":"reference/eda/visualization/#tmlc.eda.visualization.plot_avg_sentence_length_distribution","title":"<code>plot_avg_sentence_length_distribution(data, label=None)</code>","text":"<p>Generate a histogram showing the distribution of average sentence lengths in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>The input DataFrame with average sentence length data.</p> required <code>label</code> <code>str</code> <p>The label to be analyzed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>plt.Figure</code> <p>matplotlib.figure.Figure: The histogram showing the average sentence length distribution.</p> Source code in <code>tmlc/eda/visualization.py</code> <pre><code>def plot_avg_sentence_length_distribution(data: pd.DataFrame, label: Optional[str] = None) -&gt; plt.Figure:\n\"\"\"\n    Generate a histogram showing the distribution of average sentence lengths in the dataset.\n\n    Parameters:\n        data (pd.DataFrame): The input DataFrame with average sentence length data.\n        label (str, optional): The label to be analyzed. Defaults to None.\n\n    Returns:\n        matplotlib.figure.Figure: The histogram showing the average sentence length distribution.\n    \"\"\"\n    if label:\n        data = data[data[\"labels\"].apply(lambda x: label in x)]\n        plt_title = f\"Average Sentence Length Distribution for Label '{label}'\"\n    else:\n        plt_title = \"Average Sentence Length Distribution\"\n\n    fig, ax = plt.subplots(figsize=(12, 6))\n    sns.histplot(data[\"avg_sent_length\"], kde=True, bins=50, ax=ax)\n    ax.set_title(plt_title)\n    ax.set_xlabel(\"Average Sentence Length\")\n    ax.set_ylabel(\"Frequency\")\n    return fig\n</code></pre>"},{"location":"reference/eda/visualization/#tmlc.eda.visualization.plot_label_co_occurrence_heatmap","title":"<code>plot_label_co_occurrence_heatmap(co_occurrence_matrix, unique_labels)</code>","text":"<p>Generate a heatmapshowing theco-occurrence of labels in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>co_occurrence_matrix</code> <code>np.ndarray</code> <p>The matrix containing co-occurrence data.</p> required <code>unique_labels</code> <code>list</code> <p>The list of unique labels.</p> required <p>Returns:</p> Type Description <code>plt.Figure</code> <p>matplotlib.figure.Figure: The heatmap showing the label co-occurrence.</p> Source code in <code>tmlc/eda/visualization.py</code> <pre><code>def plot_label_co_occurrence_heatmap(co_occurrence_matrix: np.ndarray, unique_labels: list) -&gt; plt.Figure:\n\"\"\"\n    Generate a heatmapshowing theco-occurrence of labels in the dataset.\n\n    Args:\n        co_occurrence_matrix (np.ndarray): The matrix containing co-occurrence data.\n        unique_labels (list): The list of unique labels.\n\n    Returns:\n        matplotlib.figure.Figure: The heatmap showing the label co-occurrence.\n    \"\"\"\n    fig = plt.figure(figsize=(12, 6))\n    co_occurrence_matrix = co_occurrence_matrix.fillna(0).astype(float)\n    sns.heatmap(\n        co_occurrence_matrix,\n        annot=True,\n        cmap=\"coolwarm\",\n        xticklabels=unique_labels,\n        yticklabels=unique_labels,\n    )\n    plt.title(\"Label Co-occurrence Matrix\")\n    plt.xlabel(\"Labels\")\n    plt.ylabel(\"Labels\")\n    return fig\n</code></pre>"},{"location":"reference/eda/visualization/#tmlc.eda.visualization.plot_label_correlations","title":"<code>plot_label_correlations(correlations, label)</code>","text":"<p>Generates a heatmap showing the correlations between variables.</p> <p>Parameters:</p> Name Type Description Default <code>correlations</code> <code>pd.DataFrame</code> <p>The DataFrame containing correlation data.</p> required <code>label</code> <code>str</code> <p>The label to be analyzed.</p> required <p>Returns:</p> Type Description <code>plt.Figure</code> <p>matplotlib.figure.Figure: The heatmap showing the correlations.</p> Source code in <code>tmlc/eda/visualization.py</code> <pre><code>def plot_label_correlations(correlations: pd.DataFrame, label: str) -&gt; plt.Figure:\n\"\"\"\n    Generates a heatmap showing the correlations between variables.\n\n    Args:\n        correlations (pd.DataFrame): The DataFrame containing correlation data.\n        label (str): The label to be analyzed.\n\n    Returns:\n        matplotlib.figure.Figure: The heatmap showing the correlations.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    sns.heatmap(correlations[label], annot=True, cmap=\"coolwarm\", ax=ax)\n    ax.set_title(f\"Correlations between Label '{label}' Properties\")\n    ax.set_xlabel(\"Properties\")\n    ax.set_ylabel(\"Properties\")\n    return fig\n</code></pre>"},{"location":"reference/eda/visualization/#tmlc.eda.visualization.plot_label_distribution","title":"<code>plot_label_distribution(label_freq)</code>","text":"<p>Genera te a barplot showing the distribution of labels in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>label_freq</code> <code>dict</code> <p>A dictionary containing the frequency of each label.</p> required <p>Returns:</p> Type Description <code>plt.Figure</code> <p>matplotlib.figure.Figure: The barplot showing the label distribution.</p> Source code in <code>tmlc/eda/visualization.py</code> <pre><code>def plot_label_distribution(label_freq: dict) -&gt; plt.Figure:\n\"\"\"\n    Genera te a barplot showing the distribution of labels in the dataset.\n\n    Args:\n        label_freq (dict): A dictionary containing the frequency of each label.\n\n    Returns:\n        matplotlib.figure.Figure: The barplot showing the label distribution.\n    \"\"\"\n    fig = plt.figure(figsize=(12, 6))\n    sns.barplot(x=list(label_freq.keys()), y=list(label_freq.values()))\n    plt.title(\"Label Distribution\")\n    plt.xlabel(\"Labels\")\n    plt.ylabel(\"Frequency\")\n    return fig\n</code></pre>"},{"location":"reference/eda/visualization/#tmlc.eda.visualization.plot_label_similarity_heatmap","title":"<code>plot_label_similarity_heatmap(similarity_matrix, unique_labels)</code>","text":"<p>Plots the heatmap of the similarity matrix for the given labels .</p> <p>Parameters:</p> Name Type Description Default <code>similarity_matrix</code> <code>np.ndarray</code> <p>A numpy array containing the similarity matrix.</p> required <code>unique_labels</code> <code>List[str]</code> <p>A list of unique labels.</p> required <p>Returns:</p> Name Type Description <code>fig</code> <code>plt.Figure</code> <p>The plotted figure.</p> Source code in <code>tmlc/eda/visualization.py</code> <pre><code>def plot_label_similarity_heatmap(similarity_matrix: np.ndarray, unique_labels: List[str]) -&gt; plt.Figure:\n\"\"\"\n    Plots the heatmap of the similarity matrix for the given labels .\n\n    Args:\n        similarity_matrix (np.ndarray): A numpy array containing the similarity matrix.\n        unique_labels (List[str]): A list of unique labels.\n\n    Returns:\n        fig (plt.Figure): The plotted figure.\n    \"\"\"\n    fig = plt.figure(figsize=(12, 6))\n    similarity_matrix = similarity_matrix.fillna(0).astype(float)\n    sns.heatmap(\n        similarity_matrix, annot=True, cmap=\"coolwarm\", xticklabels=unique_labels, yticklabels=unique_labels\n    )\n    plt.title(\"Label Similarity Matrix\")\n    plt.xlabel(\"Labels\")\n    plt.ylabel(\"Labels\")\n    return fig\n</code></pre>"},{"location":"reference/eda/visualization/#tmlc.eda.visualization.plot_lexical_diversity_distribution","title":"<code>plot_lexical_diversity_distribution(data, label=None)</code>","text":"<p>Plots the distribution of lexical diversity for the given data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>A pandas DataFrame containing the data.</p> required <code>label</code> <code>str</code> <p>Optional. The label associated with the data.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>The plotted figure.</p> Source code in <code>tmlc/eda/visualization.py</code> <pre><code>def plot_lexical_diversity_distribution(data: pd.DataFrame, label: Optional[str] = None) -&gt; plt.Figure:\n\"\"\"\n    Plots the distribution of lexical diversity for the given data.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the data.\n        label (str): Optional. The label associated with the data.\n\n    Returns:\n        fig (Figure): The plotted figure.\n    \"\"\"\n    if label:\n        data = data[data[\"labels\"].apply(lambda x: label in x)]\n        plt_title = f\"Lexical Diversity Distribution for Label '{label}'\"\n    else:\n        plt_title = \"Lexical Diversity Distribution\"\n\n    fig = plt.figure(figsize=(12, 6))\n    sns.histplot(data[\"lexical_diversity\"], kde=True, bins=50)\n    plt.title(plt_title)\n    plt.xlabel(\"Lexical Diversity\")\n    plt.ylabel(\"Frequency\")\n\n    return fig\n</code></pre>"},{"location":"reference/eda/visualization/#tmlc.eda.visualization.plot_message_length_distribution","title":"<code>plot_message_length_distribution(data, label=None)</code>","text":"<p>Generate a histogram showing the distribution of message lengths in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>The input DataFrame with message length data.</p> required <code>label</code> <code>str</code> <p>The label to be analyzed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>plt.Figure</code> <p>matplotlib.figure.Figure: The histogram showing the message length distribution.</p> Source code in <code>tmlc/eda/visualization.py</code> <pre><code>def plot_message_length_distribution(data: pd.DataFrame, label: Optional[str] = None) -&gt; plt.Figure:\n\"\"\"\n    Generate a histogram showing the distribution of message lengths in the dataset.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with message length data.\n        label (str, optional): The label to be analyzed. Defaults to None.\n\n    Returns:\n        matplotlib.figure.Figure: The histogram showing the message length distribution.\n    \"\"\"\n    if label:\n        data = data[data[\"labels\"].apply(lambda x: label in x)]\n        plt_title = f\"Message Length Distribution for Label '{label}'\"\n    else:\n        plt_title = \"Message Length Distribution\"\n\n    fig = plt.figure(figsize=(12, 6))\n    sns.histplot(data[\"message_length\"], kde=True, bins=50)\n    plt.title(plt_title)\n    plt.xlabel(\"Message Length\")\n    plt.ylabel(\"Frequency\")\n    return fig\n</code></pre>"},{"location":"reference/eda/visualization/#tmlc.eda.visualization.plot_ngram_distribution","title":"<code>plot_ngram_distribution(corpus, label=None, ngram_range=(1, 1), top_n=20)</code>","text":"<p>Plot the frequency distribution of n-grams for the given corpus.</p> <p>Parameters:</p> Name Type Description Default <code>corpus</code> <code>list</code> <p>A list of strings representing the corpus.</p> required <code>label</code> <code>str</code> <p>Optional. The label associated with the corpus.</p> <code>None</code> <code>ngram_range</code> <code>tuple</code> <p>Optional. A tuple containing the range of n-grams to consider. Default is (1, 1).</p> <code>(1, 1)</code> <code>top_n</code> <code>int</code> <p>Optional. The number of top n-grams to include in the plot. Default is 20.</p> <code>20</code> <p>Returns:</p> Type Description <code>plt.Figure</code> <p>matplotlib.figure.Figure: The plotted figure.</p> Source code in <code>tmlc/eda/visualization.py</code> <pre><code>def plot_ngram_distribution(\n    corpus: list, label: Optional[str] = None, ngram_range: tuple = (1, 1), top_n: int = 20\n) -&gt; plt.Figure:\n\"\"\"\n    Plot the frequency distribution of n-grams for the given corpus.\n\n    Parameters:\n        corpus (list): A list of strings representing the corpus.\n        label (str): Optional. The label associated with the corpus.\n        ngram_range (tuple): Optional. A tuple containing the range of n-grams to consider. Default is (1, 1).\n        top_n (int): Optional. The number of top n-grams to include in the plot. Default is 20.\n\n    Returns:\n        matplotlib.figure.Figure: The plotted figure.\n    \"\"\"\n    vectorizer = CountVectorizer(ngram_range=ngram_range)\n    X = vectorizer.fit_transform(corpus)\n    ngram_freq = dict(zip(vectorizer.get_feature_names_out(), X.sum(axis=0).tolist()[0]))\n    ngram_freq_sorted = dict(sorted(ngram_freq.items(), key=lambda x: x[1], reverse=True)[:top_n])\n\n    fig, ax = plt.subplots(figsize=(12, 6))\n    sns.barplot(x=list(ngram_freq_sorted.keys()), y=list(ngram_freq_sorted.values()), ax=ax)\n\n    if label:\n        plt_title = f\"{ngram_range[1]}-Gram Frequency Distribution for Label '{label}'\"\n    else:\n        plt_title = f\"{ngram_range[1]}-Gram Frequency Distribution\"\n\n    ax.set_title(plt_title)\n    ax.set_xlabel(\"N-grams\")\n    ax.set_ylabel(\"Frequency\")\n    plt.xticks(rotation=45)\n\n    return fig\n</code></pre>"},{"location":"reference/eda/visualization/#tmlc.eda.visualization.plot_pos_tag_distribution","title":"<code>plot_pos_tag_distribution(pos_tag_freq)</code>","text":"<p>Generate a barplot showing the distribution of POS tags in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>pos_tag_freq</code> <code>dict</code> <p>A dictionary containing the frequency of each POS tag.</p> required <p>Returns:</p> Type Description <code>plt.Figure</code> <p>matplotlib.figure.Figure: The barplot showing the POS tag distribution.</p> Source code in <code>tmlc/eda/visualization.py</code> <pre><code>def plot_pos_tag_distribution(pos_tag_freq: dict) -&gt; plt.Figure:\n\"\"\"\n    Generate a barplot showing the distribution of POS tags in the dataset.\n\n    Parameters:\n        pos_tag_freq (dict): A dictionary containing the frequency of each POS tag.\n\n    Returns:\n        matplotlib.figure.Figure: The barplot showing the POS tag distribution.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(12, 6))\n    sns.barplot(x=list(pos_tag_freq.keys()), y=list(pos_tag_freq.values()), ax=ax)\n    ax.set_title(\"POS Tag Distribution\")\n    ax.set_xlabel(\"POS Tags\")\n    ax.set_ylabel(\"Frequency\")\n    return fig\n</code></pre>"},{"location":"reference/eda/visualization/#tmlc.eda.visualization.plot_reduced_embeddings","title":"<code>plot_reduced_embeddings(reduced_embeddings, sample_labels, function_name)</code>","text":"<p>Generate a scatter plot showing the reduced BERT embeddings for a given function.</p> <p>Parameters:</p> Name Type Description Default <code>reduced_embeddings</code> <code>np.ndarray</code> <p>The reduced BERT embeddings.</p> required <code>sample_labels</code> <code>list</code> <p>The labels for each sample.</p> required <code>function_name</code> <code>str</code> <p>The name of the function being analyzed.</p> required <p>Returns:</p> Type Description <code>plt.Figure</code> <p>matplotlib.figure.Figure: The scatterplot showing the reduced BERT embeddings.</p> Source code in <code>tmlc/eda/visualization.py</code> <pre><code>def plot_reduced_embeddings(\n    reduced_embeddings: np.ndarray, sample_labels: list, function_name: str\n) -&gt; plt.Figure:\n\"\"\"\n    Generate a scatter plot showing the reduced BERT embeddings for a given function.\n\n    Args:\n        reduced_embeddings (np.ndarray): The reduced BERT embeddings.\n        sample_labels (list): The labels for each sample.\n        function_name (str): The name of the function being analyzed.\n\n    Returns:\n        matplotlib.figure.Figure: The scatterplot showing the reduced BERT embeddings.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    label_colors = {}\n    unique_labels = sorted(set([label for labels in sample_labels for label in labels]))\n\n    for i, label in enumerate(unique_labels):\n        label_colors[label] = plt.colormaps.get_cmap(\"tab10\")(i)\n\n    for i, (x, y) in enumerate(reduced_embeddings):\n        for label in sample_labels[i]:\n            ax.scatter(\n                x,\n                y,\n                color=label_colors[label],\n                label=label if label not in ax.get_legend_handles_labels()[1] else None,\n            )\n\n    ax.set_title(f\"Visualizing BERT Embeddings {function_name}\")\n    ax.set_xlabel(\"Dimension 1\")\n    ax.set_ylabel(\"Dimension 2\")\n    ax.legend()\n    return fig\n</code></pre>"},{"location":"reference/eda/visualization/#tmlc.eda.visualization.plot_sentiment_distribution","title":"<code>plot_sentiment_distribution(data, label=None)</code>","text":"<p>Plots the distribution of sentiment polarity and subjectivity for the given data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>A pandas DataFrame containing the data.</p> required <code>label</code> <code>str</code> <p>Optional. The label associated with the data.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>plt.Figure</code> <p>The plotted figure.</p> Source code in <code>tmlc/eda/visualization.py</code> <pre><code>def plot_sentiment_distribution(data: pd.DataFrame, label: Optional[str] = None) -&gt; plt.Figure:\n\"\"\"\n    Plots the distribution of sentiment polarity and subjectivity for the given data.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the data.\n        label (str): Optional. The label associated with the data.\n\n    Returns:\n        fig (plt.Figure): The plotted figure.\n    \"\"\"\n    if label:\n        data = data[data[\"labels\"].apply(lambda x: label in x)]\n        plt_title_prefix = f\"Sentiment Distribution for Label '{label}' -\"\n    else:\n        plt_title_prefix = \"Sentiment Distribution -\"\n\n    fig, axs = plt.subplots(nrows=2, figsize=(12, 12))\n    sns.histplot(data[\"sentiment_polarity\"], kde=True, bins=50, ax=axs[0])\n    axs[0].set_title(plt_title_prefix + \" Polarity\")\n    axs[0].set_xlabel(\"Sentiment Polarity\")\n    axs[0].set_ylabel(\"Frequency\")\n\n    sns.histplot(data[\"sentiment_subjectivity\"], kde=True, bins=50, ax=axs[1])\n    axs[1].set_title(plt_title_prefix + \" Subjectivity\")\n    axs[1].set_xlabel(\"Sentiment Subjectivity\")\n    axs[1].set_ylabel(\"Frequency\")\n\n    plt.tight_layout()\n\n    return fig\n</code></pre>"},{"location":"reference/eda/visualization/#tmlc.eda.visualization.visualize_label_embeddings","title":"<code>visualize_label_embeddings(reduced_embeddings)</code>","text":"<p>Visualizes the reduced label embeddings using PCA and TSNE.</p> <p>Parameters:</p> Name Type Description Default <code>reduced_embeddings</code> <code>dict</code> <p>A dictionary containing the reduced embeddings and sample labels.</p> required <p>Returns:</p> Name Type Description <code>figs</code> <code>Dict[str, plt.Figure]</code> <p>A dictionary of the plotted figures.</p> Source code in <code>tmlc/eda/visualization.py</code> <pre><code>def visualize_label_embeddings(reduced_embeddings: dict) -&gt; Dict[str, plt.Figure]:\n\"\"\"\n    Visualizes the reduced label embeddings using PCA and TSNE.\n\n    Args:\n        reduced_embeddings (dict): A dictionary containing the reduced embeddings and sample labels.\n\n    Returns:\n        figs (Dict[str, plt.Figure]): A dictionary of the plotted figures.\n    \"\"\"\n    figs = {}\n    for name, values in reduced_embeddings.items():\n        fig = plot_reduced_embeddings(values[\"reduced_embeddings\"], values[\"sample_labels\"], name)\n        fig_name = f\"{name}_Reduced_Label_Embeddings\"\n        fig_name = re.sub(r\"\\W+\", \"_\", fig_name)  # replace suboptimal characters with underscore\n        figs[fig_name] = fig\n    return figs\n</code></pre>"},{"location":"reference/eda/visualization/#tmlc.eda.visualization.visualize_multi_label_data","title":"<code>visualize_multi_label_data(data, unique_labels, label_freq, co_occurrence_matrix, correlations, similarity_matrix, reduced_embeddings)</code>","text":"<p>Visualizes the multi-label data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pd.DataFrame</code> <p>A pandas DataFrame containing the data.</p> required <code>unique_labels</code> <code>list</code> <p>A list of unique labels.</p> required <code>label_freq</code> <code>dict</code> <p>A dictionary containing the frequency of each label.</p> required <code>co_occurrence_matrix</code> <code>np.ndarray</code> <p>A numpy array containing the co-occurrence matrix.</p> required <code>correlations</code> <code>pd.DataFrame</code> <p>A pandas DataFrame containing the correlations between label properties.</p> required <code>similarity_matrix</code> <code>np.ndarray</code> <p>A numpy array containing the similarity matrix.</p> required <code>reduced_embeddings</code> <code>dict</code> <p>A dictionary containing the reduced embeddings and sample labels.</p> required <p>Returns:</p> Name Type Description <code>figs</code> <code>dict</code> <p>A dictionary of the plotted figures.</p> <p>The function visualizes the multi-label data using various methods such as co-occurrence matrix, correlation matrix, similarity matrix, and reduced embeddings. The unique_labels argument should contain all the unique labels present in the data, while the label_freq argument should contain a dictionary containing the frequency of each label. The co_occurrence_matrix argument should contain a numpy array representing the co-occurrence matrix of labels, whereas the correlations argument should contain a pandas DataFrame containing the correlations between label properties. The similarity_matrix argument should contain a numpy array representing the similarity matrix. The reduced_embeddings argument should contain a dictionary containing the reduced embeddings and sample labels.</p> Source code in <code>tmlc/eda/visualization.py</code> <pre><code>def visualize_multi_label_data(\n    data: pd.DataFrame,\n    unique_labels: list,\n    label_freq: dict,\n    co_occurrence_matrix: np.ndarray,\n    correlations: pd.DataFrame,\n    similarity_matrix: np.ndarray,\n    reduced_embeddings: dict,\n) -&gt; Dict[str, plt.Figure]:\n\"\"\"\n    Visualizes the multi-label data.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the data.\n        unique_labels (list): A list of unique labels.\n        label_freq (dict): A dictionary containing the frequency of each label.\n        co_occurrence_matrix (np.ndarray): A numpy array containing the co-occurrence matrix.\n        correlations (pd.DataFrame): A pandas DataFrame containing the correlations between label properties.\n        similarity_matrix (np.ndarray): A numpy array containing the similarity matrix.\n        reduced_embeddings (dict): A dictionary containing the reduced embeddings and sample labels.\n\n    Returns:\n        figs (dict): A dictionary of the plotted figures.\n\n    The function visualizes the multi-label data using various methods such as co-occurrence matrix,\n    correlation matrix, similarity matrix, and reduced embeddings. The unique_labels argument\n    should contain all the unique labels present in the data, while the label_freq argument\n    should contain a dictionary containing the frequency of each label. The co_occurrence_matrix\n    argument should contain a numpy array representing the co-occurrence matrix of labels,\n    whereas the correlations argument should contain a pandas DataFrame containing the correlations\n    between label properties. The similarity_matrix argument should contain a numpy array representing\n    the similarity matrix. The reduced_embeddings argument should contain a dictionary containing the\n    reduced embeddings and sample labels.\n    \"\"\"\n    figs = {}\n\n    # Plot label distribution and co-occurrence heatmap\n    fig = plot_label_distribution(label_freq)\n    figs[\"Label_Distribution\"] = fig\n\n    fig = plot_label_co_occurrence_heatmap(co_occurrence_matrix, unique_labels)\n    figs[\"Label_Co-Occurrence_Heatmap\"] = fig\n\n    # Plot label-specific distributions and analyses\n    for label in unique_labels:\n        label_data = data[data[\"labels\"].apply(lambda x: label in x)]\n\n        # Plot average sentence length distribution for the label\n        fig = plot_avg_sentence_length_distribution(label_data, label=label)\n        fig_name = f\"{label}_Avg_Sentence_Length_Distribution\"\n        fig_name = re.sub(r\"\\W+\", \"_\", fig_name)  # replace suboptimal characters with underscore\n        figs[fig_name] = fig\n\n        # Plot n-gram frequency distribution for the label\n        for ngram_range in [(1, 1), (2, 2), (3, 3)]:\n            plot_ngram_distribution(label_data[\"message\"], label=label, ngram_range=ngram_range)\n            fig_name = f\"{label}_{ngram_range[1]}-gram_Frequency_Distribution\"\n            fig_name = re.sub(r\"\\W+\", \"_\", fig_name)  # replace suboptimal characters with underscore\n            figs[fig_name] = fig\n\n        # Plot message length distribution for the label\n        fig = plot_message_length_distribution(label_data, label=label)\n        fig_name = f\"{label}_Message_Length_Distribution\"\n        fig_name = re.sub(r\"\\W+\", \"_\", fig_name)  # replace suboptimal characters with underscore\n        figs[fig_name] = fig\n\n        # Plot sentiment distribution for the label\n        fig = plot_sentiment_distribution(label_data, label=label)\n        fig_name = f\"{label}_Sentiment_Distribution\"\n        fig_name = re.sub(r\"\\W+\", \"_\", fig_name)  # replace suboptimal characters with underscore\n        figs[fig_name] = fig\n\n        # Plot lexical diversity distribution for the label\n        fig = plot_lexical_diversity_distribution(label_data, label=label)\n        fig_name = f\"{label}_Lexical_Diversity_Distribution\"\n        fig_name = re.sub(r\"\\W+\", \"_\", fig_name)  # replace suboptimal characters with underscore\n        figs[fig_name] = fig\n\n        # Plot correlation between label properties for the label\n        fig = plot_label_correlations(correlations, label)\n        fig_name = f\"{label}_Correlation_between_Properties\"\n        fig_name = re.sub(r\"\\W+\", \"_\", fig_name)  # replace suboptimal characters with underscore\n        figs[fig_name] = fig\n\n    # Plot correlation matrix between labels\n    fig = plot_label_similarity_heatmap(similarity_matrix, unique_labels)\n    figs[\"Label_Similarity_Matrix\"] = fig\n\n    # Visualize label embeddings using PCA and TSNE\n    figs.update(visualize_label_embeddings(reduced_embeddings))\n\n    return figs\n</code></pre>"},{"location":"reference/scripts/","title":"scripts","text":""},{"location":"reference/scripts/eda/","title":"eda","text":""},{"location":"reference/scripts/eda/#tmlc.scripts.eda.create_eda","title":"<code>create_eda(config_path)</code>","text":"<p>Generates an exploratory data analysis (EDA) report for multi-label text data. This function reads a YAML configuration file containing the paths to the input CSV file, the name of the column containing the text data, the list of column names containing the labels, the path to the output Markdown file, and the path to the folder where the output images will be saved. Then, it preprocesses the data, visualizes the label distributions and co- occurrences, generates embeddings using a pre-trained language model, trains and evaluates multiple classifiers using cross-validation, and renders a Markdown report with the results.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML configuration file.</p> required Example <p>To generate an EDA report for the \"toxic_multi_class.csv\" dataset with custom options, run the following command in the terminal:</p> <pre><code>python tmlc/scripts/eda.py --config_path \"eda.yaml\"\n</code></pre> Source code in <code>tmlc/scripts/eda.py</code> <pre><code>def create_eda(config_path):\n\"\"\"\n    Generates an exploratory data analysis (EDA) report for multi-label text data. This function\n    reads a YAML configuration file containing the paths to the input CSV file, the name of the\n    column containing the text data, the list of column names containing the labels, the path to the\n    output Markdown file, and the path to the folder where the output images will be saved. Then, it\n    preprocesses the data, visualizes the label distributions and co- occurrences, generates\n    embeddings using a pre-trained language model, trains and evaluates multiple classifiers using\n    cross-validation, and renders a Markdown report with the results.\n\n    Args:\n        config_path (str): Path to the YAML configuration file.\n\n    Example:\n        To generate an EDA report for the \"toxic_multi_class.csv\" dataset with custom options,\n        run the following command in the terminal:\n\n        ```\n        python tmlc/scripts/eda.py --config_path \"eda.yaml\"\n        ```\n    \"\"\"\n\n    logger.info(\"Loading configuration from YAML file...\")\n    config: EDAClassifiersEvaluationConfig = load_yaml_config(\n        config_path, EDAClassifiersEvaluationConfig\n    )\n\n    logger.info(\"Preprocessing data...\")\n    data = config.get_data.partial()\n    (\n        data,\n        unique_labels,\n        label_freq,\n        co_occurrence_matrix,\n        correlations,\n        similarity_matrix,\n        reduced_embeddings,\n    ) = preprocess_data(data, config.message_column, config.labels_columns)\n\n    logger.info(\"Creating data visualizations...\")\n    figs = visualize_multi_label_data(\n        data,\n        unique_labels,\n        label_freq,\n        co_occurrence_matrix,\n        correlations,\n        similarity_matrix,\n        reduced_embeddings,\n    )\n\n    figures = {}\n\n    if not os.path.exists(\"docs/imgs\"):\n        os.makedirs(\"docs/imgs\")\n\n    # Save each figure to a file with a unique name based on the dictionary key\n    for fig_name, fig in figs.items():\n        fig_file = f\"docs/imgs/{fig_name}.png\"\n        fig.savefig(fig_file, dpi=300, bbox_inches=\"tight\")\n        figures[fig_name] = f\"../imgs/{fig_name}.png\"\n\n    logger.info(\"Training and evaluating classifiers...\")\n    results = train_and_evaluate_classifiers(config, data)\n\n    metrics_keys = [\"val_f1\", \"test_f1\"]\n    logger.info(\"Rendering EDA output...\")\n\n    render_eda_output(\n        figures=figures,\n        unique_labels=unique_labels,\n        results=results,\n        output_file=config.output_file\n    )\n    return results\n</code></pre>"},{"location":"reference/scripts/score/","title":"score","text":""},{"location":"reference/scripts/score/#tmlc.scripts.score.score","title":"<code>score(model_name, version=None, texts=None)</code>","text":"<p>Loads the registered MLflow model by name and version, and makes predictions on the input texts.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <p>str Name of the registered MLflow model to use.</p> required <code>version</code> <p>int, optional Version of the registered MLflow model to use. If not specified, the latest version is used.</p> <code>None</code> <code>texts</code> <p>str, optional Texts to predict, separated by the '|' character. If not provided, the function returns an empty list.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str] Predictions made by the model on the input texts.</p> Example <p>To make predictions using a registered MLflow model, run the following command in the terminal:</p> <pre><code>python your_script.py score --model-name \"my_model\" --version 1             --texts \"text1|text2|text3\"\n</code></pre> <p>This will load the registered MLflow model with the name \"my_model\" and version 1, and make predictions on the texts \"text1\", \"text2\", and \"text3\". The predictions will be printed to the console.</p> Source code in <code>tmlc/scripts/score.py</code> <pre><code>def score(model_name: str, version: Optional[int] = None, texts: Optional[str] = None) -&gt; List[str]:\n\"\"\"\n    Loads the registered MLflow model by name and version, and makes predictions on the input texts.\n\n    Args:\n        model_name : str\n            Name of the registered MLflow model to use.\n        version : int, optional\n            Version of the registered MLflow model to use. If not specified, the\n            latest version is used.\n        texts : str, optional\n            Texts to predict, separated by the '|' character. If not provided, the function\n            returns an empty list.\n\n    Returns:\n        List[str]\n            Predictions made by the model on the input texts.\n\n    Example:\n        To make predictions using a registered MLflow model, run the following command\n        in the terminal:\n\n        ```\n        python your_script.py score --model-name \"my_model\" --version 1 \\\n            --texts \"text1|text2|text3\"\n        ```\n\n        This will load the registered MLflow model with the name \"my_model\" and version 1,\n        and make predictions on the texts \"text1\", \"text2\", and \"text3\".\n        The predictions will be printed to the console.\n    \"\"\"\n    model = mlflow.pytorch.load_model(model_name=model_name, version=version)\n\n    if texts is None:\n        logger.info(\"No texts provided for prediction\")\n        return []\n\n    # Split the input texts by the '|' character\n    messages = texts.split(\"|\")\n\n    # Make predictions\n    predictions = model.predict(messages)\n    logger.info(f\"Predictions: {predictions}\")\n    return predictions\n</code></pre>"},{"location":"reference/scripts/train/","title":"train","text":""},{"location":"reference/scripts/train/#tmlc.scripts.train.train","title":"<code>train(file_path, check_point=False)</code>","text":"<p>Trains the TextMultiLabelClassificationModel using the hyperparameters specified in the given YAML config file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the YAML config file or to the checkpoint if check-point flag</p> required <code>is</code> <code>True.check_point</code> <p>Flag to start from existing checkpoint.</p> required Example <p>To train the TextMultiLabelClassificationModel using a YAML config file, run the following command in the terminal:</p> <pre><code>python your_script.py train --file-path /path/to/config.yaml\n</code></pre> <p>To resume training from an existing checkpoint, set the check-point flag to True and provide the path to the checkpoint file:</p> <pre><code>python your_script.py train --file-path /path/to/checkpoint.pt --check-point True\n</code></pre> Source code in <code>tmlc/scripts/train.py</code> <pre><code>def train(file_path: str, check_point: bool = False) -&gt; None:\n\"\"\"\n    Trains the TextMultiLabelClassificationModel using the hyperparameters specified in the given\n    YAML config file.\n\n    Args:\n        file_path: Path to the YAML config file or to the checkpoint if check-point flag\n        is True. check_point: Flag to start from existing checkpoint.\n\n    Example:\n        To train the TextMultiLabelClassificationModel using a YAML config file, run the\n        following command in the terminal:\n\n        ```\n        python your_script.py train --file-path /path/to/config.yaml\n        ```\n\n        To resume training from an existing checkpoint, set the check-point flag to\n        True and provide the path to the checkpoint file:\n\n        ```\n        python your_script.py train --file-path /path/to/checkpoint.pt --check-point True\n        ```\n    \"\"\"\n\n    logger.info(f\"Loading model, data module, and trainer configuration from {file_path}\")\n    model, datamodule, config = load_model_data_trainer_config(file_path=file_path, check_point=check_point)\n    loggers = to_partial_functions_dictionary(config.loggers)\n    callbacks = to_partial_functions_dictionary(config.callbacks)\n\n    logger.info(\"Setting up trainer\")\n    trainer = setup_trainer(config=config, loggers=loggers, callbacks=callbacks)\n\n    with mlflow.start_run(experiment_id=loggers[\"MLFlowLogger\"].experiment_id) as run:\n        run_id = run.info.run_id\n        logger.info(f\"Run ID: {run_id}\")\n        loggers[\"MLFlowLogger\"].experiment.delete_run(loggers[\"MLFlowLogger\"]._run_id)\n        loggers[\"MLFlowLogger\"]._run_id = run_id\n\n        logger.info(\"Starting model training\")\n        trainer.fit(model, datamodule=datamodule)\n\n        logger.info(\"Starting model testing\")\n        trainer.test(datamodule=datamodule)\n\n        model_uri = register_model(model, config)\n        logger.info(f\"Model uri: {model_uri}\")\n</code></pre>"},{"location":"reference/scripts/utils/","title":"utils","text":""},{"location":"reference/scripts/utils/#tmlc.scripts.utils.add_suffix_to_filename","title":"<code>add_suffix_to_filename(filename, suffix)</code>","text":"<p>Add a suffix to the filename and return a Path object.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename to modify.</p> required <code>suffix</code> <code>str</code> <p>The suffix to add to the filename.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>A Path object with the modified filename.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If filename is empty or None.</p> Source code in <code>tmlc/scripts/utils.py</code> <pre><code>def add_suffix_to_filename(filename: str, suffix: str) -&gt; Path:\n\"\"\"\n    Add a suffix to the filename and return a Path object.\n\n    Args:\n        filename (str): The filename to modify.\n        suffix (str): The suffix to add to the filename.\n\n    Returns:\n        Path: A Path object with the modified filename.\n\n    Raises:\n        ValueError: If filename is empty or None.\n    \"\"\"\n    if not filename:\n        raise ValueError(\"filename cannot be empty or None\")\n    path = Path(filename)\n    new_name = f\"{path.stem}_{suffix}{path.suffix}\"\n    return path.with_name(new_name)\n</code></pre>"},{"location":"reference/scripts/utils/#tmlc.scripts.utils.create_artifacts","title":"<code>create_artifacts(model, config)</code>","text":"<p>Creates a dictionary of MLflow artifacts based on the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The PyTorch model to save as an artifact.</p> required <code>config</code> <code>TrainerConfig</code> <p>The configuration for the trainer.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary of MLflow artifacts.</p> Source code in <code>tmlc/scripts/utils.py</code> <pre><code>def create_artifacts(model: Any, config: TrainerConfig) -&gt; Dict[str, str]:\n\"\"\"\n    Creates a dictionary of MLflow artifacts based on the given configuration.\n\n    Args:\n        model (Any): The PyTorch model to save as an artifact.\n        config (TrainerConfig): The configuration for the trainer.\n\n    Returns:\n        Dict[str, str]: A dictionary of MLflow artifacts.\n    \"\"\"\n    # Create artifact folder if it doesn't exist\n    if not os.path.exists(config.mlflow_config.artifact_folder):\n        os.makedirs(config.mlflow_config.artifact_folder)\n\n    # Convert the PyTorch model to ONNX format and save it as an artifact\n    export_model_to_onnx(model, config)\n\n    # Save the tokenizer to a file\n    config.data_module_config.dataset.tokenizer.tokenizer.save_pretrained(config.mlflow_config.tokenizer_path)\n\n    # Create artifacts dictionary\n    artifacts = {\"model\": config.mlflow_config.model_path, \"tokenizer\": config.mlflow_config.tokenizer_path}\n\n    # Optionally include the score script in the artifacts\n    if config.mlflow_config.score_script_path:\n        artifacts.update({\"score\": config.mlflow_config.score_script_path})\n\n    return artifacts\n</code></pre>"},{"location":"reference/scripts/utils/#tmlc.scripts.utils.create_input_output_examples","title":"<code>create_input_output_examples()</code>","text":"<p>Creates example input and output dataframes for testing purposes.</p> <p>Returns:</p> Type Description <code>Tuple[pd.DataFrame, pd.DataFrame]</code> <p>Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing the input and output dataframes.</p> Source code in <code>tmlc/scripts/utils.py</code> <pre><code>def create_input_output_examples() -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    Creates example input and output dataframes for testing purposes.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing the input\n            and output dataframes.\n    \"\"\"\n    input_example = pd.DataFrame({\"input_text\": [\"example text\"]})\n    output_example = pd.DataFrame({\"output\": [True]})\n    return input_example, output_example\n</code></pre>"},{"location":"reference/scripts/utils/#tmlc.scripts.utils.export_model_to_onnx","title":"<code>export_model_to_onnx(model, config)</code>","text":"<p>Convert a PyTorch model to ONNX format and save it as an artifact.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>pl.LightningModule</code> <p>The trained PyTorch model.</p> required <code>config</code> <code>TrainerConfig</code> <p>A configuration object containing training settings.</p> required Source code in <code>tmlc/scripts/utils.py</code> <pre><code>def export_model_to_onnx(model: pl.LightningModule, config: TrainerConfig) -&gt; None:\n\"\"\"\n    Convert a PyTorch model to ONNX format and save it as an artifact.\n\n    Args:\n        model: The trained PyTorch model.\n        config: A configuration object containing training settings.\n    \"\"\"\n    # Set the model to inference mode\n    model.eval()\n\n    # Define dummy input\n    encoding = config.data_module_config.dataset.tokenizer([\"Hello, world!\"])\n    dummy_input = {\"data\": {key: torch.tensor(value) for key, value in encoding.items()}}\n    dynamic_axes = {\n        \"data\": {0: \"batch_size\", 1: \"sequence\"},\n        \"output\": {0: \"batch_size\", 1: \"sequence\"},\n    }\n    torch.onnx.export(\n        model,\n        dummy_input,\n        config.mlflow_config.model_path,\n        input_names=[\"data\"],\n        output_names=[\"output\"],\n        dynamic_axes=dynamic_axes,\n    )\n</code></pre>"},{"location":"reference/scripts/utils/#tmlc.scripts.utils.json_to_nested_tags","title":"<code>json_to_nested_tags(data)</code>","text":"<p>Converts a JSON object to a dictionary of nested tags using dot notation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>A JSON object.</p> required <p>Returns:</p> Type Description <p>A dictionary of nested tags.</p> Source code in <code>tmlc/scripts/utils.py</code> <pre><code>def json_to_nested_tags(data: Dict[str, Any]):\n\"\"\"\n    Converts a JSON object to a dictionary of nested tags using dot notation.\n\n    Args:\n        data: A JSON object.\n\n    Returns:\n        A dictionary of nested tags.\n    \"\"\"\n    if not isinstance(data, dict):\n        raise TypeError(\"Input data must be a dictionary.\")\n\n    tags = {}\n    stack = [(key, value) for key, value in data.items()]\n\n    while stack:\n        key, value = stack.pop()\n        if isinstance(value, dict):\n            stack.extend((f\"{key}.{sub_key}\", sub_value) for sub_key, sub_value in value.items())\n        else:\n            tags[key] = str(value)\n\n    return tags\n</code></pre>"},{"location":"reference/scripts/utils/#tmlc.scripts.utils.load_model_data_trainer_config","title":"<code>load_model_data_trainer_config(file_path, check_point=False)</code>","text":"<p>Load the model, data module, and trainer config from a YAML configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <code>check_point</code> <code>bool</code> <p>Whether to resume training from the checkpointed weights. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>TextMultiLabelClassificationModel</code> <p>A tuple of the TextMultiLabelClassificationModel, DataModule, and</p> <code>DataModule</code> <p>TrainerConfig objects.</p> Source code in <code>tmlc/scripts/utils.py</code> <pre><code>def load_model_data_trainer_config(\n    file_path: str, check_point: bool = False\n) -&gt; Tuple[TextMultiLabelClassificationModel, DataModule, TrainerConfig]:\n\"\"\"\n    Load the model, data module, and trainer config from a YAML configuration file.\n\n    Args:\n        file_path (str): Path to the YAML configuration file.\n        check_point (bool, optional): Whether to resume training from the\n            checkpointed weights. Defaults to False.\n\n    Returns:\n        A tuple of the TextMultiLabelClassificationModel, DataModule, and\n        TrainerConfig objects.\n    \"\"\"\n    config: TrainerConfig = load_yaml_config(config_path=file_path, basemodel=TrainerConfig)\n\n    if check_point:\n        # If you want to resume training from the checkpointed weights,\n        # load them back into the model\n        model = TextMultiLabelClassificationModel.load_from_checkpoint(file_path)\n    else:\n        # Initialize model\n        model = TextMultiLabelClassificationModel(config.lightning_module_config)\n\n    # Initialize data module\n    datamodule = DataModule(config.data_module_config)\n\n    return model, datamodule, config\n</code></pre>"},{"location":"reference/scripts/utils/#tmlc.scripts.utils.load_yaml_config","title":"<code>load_yaml_config(config_path, basemodel)</code>","text":"<p>Load a configuration file from a YAML file and validate it against a Pydantic BaseModel.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the YAML configuration file.</p> required <code>basemodel</code> <code>BaseModel</code> <p>The Pydantic BaseModel to validate the configuration against.</p> required <p>Returns:</p> Type Description <code>BaseModel</code> <p>A validated instance of the Pydantic BaseModel.</p> Source code in <code>tmlc/scripts/utils.py</code> <pre><code>def load_yaml_config(config_path: str, basemodel: BaseModel) -&gt; BaseModel:\n\"\"\"\n    Load a configuration file from a YAML file and validate it against a Pydantic BaseModel.\n\n    Args:\n        config_path (str): The path to the YAML configuration file.\n        basemodel (BaseModel): The Pydantic BaseModel to validate the\n            configuration against.\n\n    Returns:\n        A validated instance of the Pydantic BaseModel.\n    \"\"\"\n    try:\n        config = basemodel.from_yaml(file_path=config_path)\n    except FileNotFoundError as e:\n        logger.error(f\"Config file not found: {config_path}\")\n        raise e\n    except yaml.YAMLError as e:\n        logger.error(f\"Error parsing config file: {e}\")\n        raise e\n    return config\n</code></pre>"},{"location":"reference/scripts/utils/#tmlc.scripts.utils.prepare_model_path","title":"<code>prepare_model_path(config)</code>","text":"<p>Prepares the path to the model directory based on the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TrainerConfig</code> <p>The trainer configuration.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the model directory.</p> Source code in <code>tmlc/scripts/utils.py</code> <pre><code>def prepare_model_path(config: TrainerConfig) -&gt; str:\n\"\"\"\n    Prepares the path to the model directory based on the given configuration.\n\n    Args:\n        config (TrainerConfig): The trainer configuration.\n\n    Returns:\n        str: The path to the model directory.\n    \"\"\"\n    model_path = f\"{config.artifact_folder}{config.lightning_module_config.model_name}\"\n    try:\n        if os.path.exists(model_path):\n            shutil.rmtree(model_path)\n    except Exception as e:\n        logger.error(f\"Error removing directory {model_path}: {e}\")\n    return model_path\n</code></pre>"},{"location":"reference/scripts/utils/#tmlc.scripts.utils.register_model","title":"<code>register_model(model, config)</code>","text":"<p>Register a trained model in the MLflow model registry.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The trained PyTorch model.</p> required <code>config</code> <code>TrainerConfig</code> <p>A configuration object containing training settings.</p> required Source code in <code>tmlc/scripts/utils.py</code> <pre><code>def register_model(model, config: TrainerConfig):\n\"\"\"\n    Register a trained model in the MLflow model registry.\n\n    Args:\n        model: The trained PyTorch model.\n        config: A configuration object containing training settings.\n    \"\"\"\n    # Set tags in the MLflow UI from the configuration\n    set_mlflow_tags(config)\n\n    artifacts = create_artifacts(model, config)\n\n    model_uri = _register_model(config, artifacts, model)\n\n    logger.info(f\"model_uri: {model_uri}\")\n\n    return model_uri\n</code></pre>"},{"location":"reference/scripts/utils/#tmlc.scripts.utils.set_mlflow_tags","title":"<code>set_mlflow_tags(config)</code>","text":"<p>Set MLflow tags from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TrainerConfig</code> <p>A configuration object containing training settings.</p> required Source code in <code>tmlc/scripts/utils.py</code> <pre><code>def set_mlflow_tags(config: TrainerConfig) -&gt; None:\n\"\"\"\n    Set MLflow tags from configuration.\n\n    Args:\n        config: A configuration object containing training settings.\n    \"\"\"\n    logger.info(\"Set tags to MLFlowLogger\")\n    tags = json_to_nested_tags(config.dict())\n    mlflow.set_tags(tags)\n</code></pre>"},{"location":"reference/scripts/utils/#tmlc.scripts.utils.setup_trainer","title":"<code>setup_trainer(config, loggers, callbacks, tokenizers_parallelism=False)</code>","text":"<p>Initializes and returns a PyTorch Lightning Trainer object with the given configuration, loggers, and callbacks.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TrainerConfig</code> <p>The TrainerConfig object containing the training configuration parameters.</p> required <code>loggers</code> <code>Dict[str, callable]</code> <p>A dictionary of loggers, where keys are the logger names and values are the corresponding logger objects.</p> required <code>callbacks</code> <code>Dict[str, callable]</code> <p>A dictionary of callbacks, where keys are the callback names and values are the corresponding callback objects.</p> required <code>tokenizers_parallelism</code> <code>bool</code> <p>Whether to enable parallelism for tokenizers for huggingface False is expected. (default: False).</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[pl.Trainer, Dict[str, callable], Dict[str, callable]]</code> <p>A tuple containing the following objects: - A PyTorch Lightning Trainer object initialized with     the given configuration, loggers, and callbacks. - A dictionary of loggers, where keys are the logger names     and values are the corresponding logger callables. - A dictionary of callbacks, where keys are the callback names     and values are the corresponding callback callables.</p> Source code in <code>tmlc/scripts/utils.py</code> <pre><code>def setup_trainer(\n    config: TrainerConfig,\n    loggers: Dict[str, callable],\n    callbacks: Dict[str, callable],\n    tokenizers_parallelism: bool = False,\n) -&gt; Tuple[pl.Trainer, Dict[str, callable], Dict[str, callable]]:\n\"\"\"\n    Initializes and returns a PyTorch Lightning Trainer object with the given configuration,\n    loggers, and callbacks.\n\n    Args:\n        config: The TrainerConfig object containing the training configuration parameters.\n        loggers: A dictionary of loggers, where keys are the logger names\n            and values are the corresponding logger objects.\n        callbacks: A dictionary of callbacks, where keys are the callback\n            names and values are the corresponding callback objects.\n        tokenizers_parallelism: Whether to enable parallelism for tokenizers\n            for huggingface False is expected. (default: False).\n\n    Returns:\n        A tuple containing the following objects:\n            - A PyTorch Lightning Trainer object initialized with\n                the given configuration, loggers, and callbacks.\n            - A dictionary of loggers, where keys are the logger names\n                and values are the corresponding logger callables.\n            - A dictionary of callbacks, where keys are the callback names\n                and values are the corresponding callback callables.\n    \"\"\"\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = str(tokenizers_parallelism).lower()\n\n    # Set seed for reproducibility\n    pl.seed_everything(config.seed)\n\n    # Initialize trainer\n    trainer = pl.Trainer(logger=list(loggers.values()), callbacks=list(callbacks.values()), **config.kwargs)\n    return trainer\n</code></pre>"},{"location":"reference/scripts/utils/#tmlc.scripts.utils.to_partial_functions_dictionary","title":"<code>to_partial_functions_dictionary(partials)</code>","text":"<p>Converts a list of partial functions specified in the config to a dictionary of callables.</p> <p>Parameters:</p> Name Type Description Default <code>partials</code> <code>List[PartialFunctionConfig]</code> <p>A list of partial functions specified in the config.</p> required <p>Returns:</p> Type Description <code>Dict[str, Callable]</code> <p>A dictionary of partial functions as callables, with the keys being the</p> <code>Dict[str, Callable]</code> <p>function names.</p> <p>Raises:</p> Type Description <code>PartialFunctionError</code> <p>If any partial function fails to be converted to a callable.</p> Source code in <code>tmlc/scripts/utils.py</code> <pre><code>def to_partial_functions_dictionary(partials: List[PartialFunctionConfig]) -&gt; Dict[str, Callable]:\n\"\"\"\n    Converts a list of partial functions specified in the config to a dictionary of callables.\n\n    Args:\n        partials: A list of partial functions specified in the config.\n\n    Returns:\n        A dictionary of partial functions as callables, with the keys being the\n        function names.\n\n    Raises:\n        PartialFunctionError: If any partial function fails to be converted to a callable.\n    \"\"\"\n    out_partials: Dict[str, Callable] = {}\n    for partial in partials:\n        out_partials[partial.func] = _partial_to_callable(partial)\n    return out_partials\n</code></pre>"},{"location":"user_guide/dataset_requirements/","title":"Dataset Requirements","text":"<p>This documentation page provides an overview of the dataset requirements and structure for the <code>TextMultiLabelClassificationModel</code> project.</p>"},{"location":"user_guide/dataset_requirements/#overview","title":"Overview","text":"<p>The dataset used in this project is a CSV file containing text samples along with their respective multi-label classifications. Each sample can have multiple labels, and each label represents a different aspect of the text.</p>"},{"location":"user_guide/dataset_requirements/#structure","title":"Structure","text":"<p>The dataset should be structured as a CSV file with the following structure:</p> <ul> <li><code>id</code>: Unique identifier for each data point. Is optional</li> <li><code>comment_text</code>: The text of the comment or message.</li> <li>The remaining columns represent the labels for each sample. In the following example, the labels are: <code>toxic</code>, <code>severe_toxic</code>, <code>obscene</code>, <code>threat</code>, <code>insult</code>, <code>identity_hate</code>.</li> </ul> <p>Each label column should have binary values, with <code>1</code> indicating the presence of the label and <code>0</code> indicating its absence.</p>"},{"location":"user_guide/dataset_requirements/#example","title":"Example","text":"<p>Here is an example of the dataset structure:</p> <pre><code>\"id\",\"comment_text\",\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"\n\"1\",\"I really enjoyed the movie. It had great acting and a compelling plot.\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\"\n\"2\",\"This restaurant serves terrible food. I got food poisoning from eating here.\",\"1\",\"0\",\"1\",\"0\",\"1\",\"0\"\n\"3\",\"The speaker at the conference made several offensive comments about women and minorities.\",\"1\",\"0\",\"1\",\"1\",\"1\",\"1\"\n\"4\",\"I disagree with your opinion on this topic, but I respect your right to express it.\",\"0\",\"0\",\"0\",\"0\",\"0\",\"0\"\n\"5\",\"The graffiti on this building is a blight on the neighborhood.\",\"1\",\"0\",\"1\",\"0\",\"1\",\"0\"\n\"6\",\"I can't believe you would say something so racist. You should be ashamed of yourself.\",\"1\",\"0\",\"1\",\"0\",\"1\",\"1\"\n\"7\",\"I think the author of this article is completely wrong. Their arguments are flawed.\",\"0\",\"0\",\"0\",\"0\",\"1\",\"0\"\n\"8\",\"I'm sorry, but your behavior is unacceptable. Please stop harassing me.\",\"1\",\"1\",\"0\",\"1\",\"1\",\"1\"\n\"9\",\"This product is a ripoff. Don't waste your money on it.\",\"1\",\"0\",\"1\",\"0\",\"1\",\"0\"\n\"10\",\"I'm so sick of all the hate and vitriol on social media these days.\",\"1\",\"0\",\"1\",\"0\",\"1\",\"1\"\n</code></pre>"},{"location":"user_guide/dataset_requirements/#preprocessing","title":"Preprocessing","text":"<p>Before using the dataset for training and evaluation, it should be preprocessed to ensure the text data is properly tokenized and encoded. The <code>TextMultiLabelClassificationModel</code> project provides a data preprocessing pipeline to handle this process.</p>"},{"location":"user_guide/eda_validation/","title":"Build EDA","text":"<p>To generate an EDA report for the \"toxic_multi_class.csv\" dataset with custom options, run the following command in the terminal:</p> <pre><code>python your_script.py create_eda --filepath \"data/toxic_multi_class.csv\" \\\n    --message_col \"comment_text\" --label_cols \"toxic\" \"severe_toxic\" \\\n        --output_file \"output.md\" --img_folder \".\"\n</code></pre> <p>Check the page Validation/EDA to view the content output of the analysis, and the default resutls for the mock data.</p>"},{"location":"user_guide/eda_validation/#example-configuration","title":"Example Configuration","text":"<p>Here's an example configuration file for the <code>EDAClassifiersEvaluationConfig</code>. This file includes settings necessary to create the EDA.</p> <pre><code>eda:\n  output_file: docs/validation/eda.md\n  message_column: \"comment_text\" # Column name for input text\n  labels_columns: [\"toxic\"] # List of column names for label(s)\n  get_data:\n    module: \"tmlc.components.get_data\"\n    func: \"get_data\"\n    kwargs:\n      file_path: \"data/toxic_multi_class.csv\" # File path for input data\n  split_data:\n    module: \"tmlc.components.split_data\"\n    func: \"split_data\"\n    kwargs:\n      train_ratio: 0.8 # Train split\n      val_ratio: 0.1 # Validation split\n      test_ratio: 0.1 # Test split\n      random_state: 42 # Random seed for splitting data\n      labels_columns: [\"toxic\"] # List of column names for label(s)\n  transformer_models:\n    - pretrainedmodel:\n        path: bert-base-cased\n      tokenizer:\n        model_name: bert-base-uncased\n        path: bert-base-cased\n        max_length: 128\n        kwargs:\n          add_special_tokens: true\n          padding: \"max_length\"\n          truncation: true\n    - pretrainedmodel:\n        model_name: distilbert-base-uncased\n        path: distilbert-base-uncased\n      tokenizer:\n        model_name: distilbert-base-uncased\n        path: distilbert-base-uncased\n        max_length: 128\n        kwargs:\n          add_special_tokens: true\n          padding: \"max_length\"\n          truncation: true\n    - pretrainedmodel:\n        path: bert-base-uncased\n      tokenizer:\n        model_name: bert-base-uncased\n        path: bert-base-uncased\n        max_length: 128\n        kwargs:\n          add_special_tokens: true\n          padding: \"max_length\"\n          truncation: true\n  classifiers:\n      - clf:\n          module: sklearn.ensemble\n          func: RandomForestClassifier\n        hyperparams:\n          n_estimators: [50, 100, 200]\n          max_depth: [2, 5, 10]\n</code></pre>"},{"location":"user_guide/introduction/","title":"Full Training and Deployment Guide","text":"<p>This extensive documentation guide combines the training and deployment of the multi-label text classification models using the <code>TextMultiLabelClassificationModelWrapperPythonModel</code> wrapper class.</p>"},{"location":"user_guide/introduction/#overview","title":"Overview","text":"<p>The guide will cover the following steps:</p> <ol> <li>Preparing the dataset</li> <li>Training script</li> <li>Creating a model wrapper</li> <li>Saving and registering the model</li> </ol>"},{"location":"user_guide/introduction/#1-preparing-the-dataset","title":"1. Preparing the Dataset","text":"<p>Before training the model, ensure that the dataset is formatted properly. The dataset should be in a CSV format with the following columns:</p> <ul> <li><code>id</code>: Unique identifier for each data point. Is optional</li> <li><code>comment_text</code>: Text data for classification.</li> <li>One column for each label, with binary values (0 or 1) indicating the presence or absence of the label.</li> </ul> <p>For more information on how to preprocess the data, refer to the Data Requirements user guide.</p>"},{"location":"user_guide/introduction/#2-training-script","title":"2. Training script","text":"<p>Train the multi-label text classification model using the provided training script. Adjust the model and training parameters as needed.</p> <p>For more information on training the model, refer to the Training user guide.</p>"},{"location":"user_guide/introduction/#3-creating-a-model-wrapper","title":"3. Creating a Model Wrapper","text":"<p>Once the model has been trained, create an instance of the <code>TextMultiLabelClassificationModelWrapperPythonModel</code> class to wrap the model for deployment.</p> <pre><code>wrapper = TextMultiLabelClassificationModelWrapperPythonModel(\n    model_path=\"path/to/onnx/model\",\n    tokenizer_config=tokenizer_config,\n    tokenizer_path=\"path/to/tokenizer\",\n)\n</code></pre> <p>For more information on the model wrapper, refer to the Model Wrapper documentation page.</p>"},{"location":"user_guide/introduction/#4-saving-and-registering-the-model","title":"4. Saving and Registering the Model","text":"<p>Save the model wrapper using from <code>tmlc.script.utils.register_model</code> function. This will create an MLflow artifact containing the model and all associated files, including the ONNX model and tokenizer.</p> <p>Once the model is saved as an MLflow artifact, it can be deployed using various deployment options supported by MLflow.</p> <p>For more information on the model saving, refer to the Training documentation page.</p>"},{"location":"user_guide/introduction/#5-training-job","title":"5. Training Job","text":"<p>TODO</p>"},{"location":"user_guide/introduction/#summary","title":"Summary","text":"<p>By following this full training and deployment guide, you can train a multi-label text classification model, create a model wrapper using the <code>TextMultiLabelClassificationModelWrapperPythonModel</code> class, and deploy the model using MLflow. This allows you to efficiently use the model for inference and serve it through various deployment options.</p> <p>For more details on each part of the process, refer to the specific user guides provided in the documentation:</p> <ul> <li>Data Requirements</li> <li>Training</li> <li>Model Wrapper</li> <li>Model Evaluation</li> </ul>"},{"location":"user_guide/model_evaluation/","title":"Evaluating a Text Multi-label Classification Model","text":"<p>In this project, we use a PyTorch Lightning Module for text multi-label classification. To evaluate the model's performance, we focus on several key metrics and their mathematical foundations. The insights provided by these metrics help us understand the strengths and weaknesses of the model.</p>"},{"location":"user_guide/model_evaluation/#metrics","title":"Metrics","text":"<p>The following metrics are crucial for evaluating the performance of our text multi-label classification model:</p>"},{"location":"user_guide/model_evaluation/#1-precision-positive-predictive-value","title":"1. Precision (Positive Predictive Value):","text":"<p>Precision measures the proportion of true alarms among the instances predicted as alarms. High precision is important in this context because it indicates that when the model predicts an alarm, it is likely to be a true alarm. A low precision may lead to a high number of false alarms, which could cause unnecessary concern or action.</p> <p>Insights:</p> <ul> <li>High precision indicates that when the model predicts an alarm, it is likely to be a true alarm.</li> <li>Precision is particularly important in scenarios where false alarms may have significant consequences, such as causing unnecessary concern, wasting resources, or triggering unwanted actions.</li> </ul> <p>Issues:</p> <ul> <li>A model with high precision may have low recall, which can lead to missed alarms.</li> <li>Precision alone does not provide a complete picture of the model's performance, as it does not consider the proportion of true alarms that the model identifies.</li> </ul> <p>Mathematically, precision is defined as:</p> <pre><code>precision = TP / (TP + FP)\n</code></pre>"},{"location":"user_guide/model_evaluation/#2-recall-sensitivity-true-positive-rate","title":"2. Recall (Sensitivity, True Positive Rate):","text":"<p>Recall measures the proportion of true alarms among the actual alarms. High recall is important because it indicates that the model is able to identify most of the true alarms. A low recall may lead to a high number of missed alarms, which could result in potential problems being overlooked.</p> <p>Insights:</p> <ul> <li>High recall indicates that the model is able to identify most of the true alarms.</li> <li>Recall is particularly important in scenarios where missed alarms may have significant consequences, such as overlooking potential problems, safety hazards, or time-sensitive issues.</li> </ul> <p>Issues:</p> <ul> <li>A model with high recall may have low precision, which can lead to a high number of false alarms.</li> <li>Recall alone does not provide a complete picture of the model's performance, as it does not consider the proportion of true alarms among the instances predicted as alarms.</li> </ul> <p>Mathematically, recall is defined as:</p> <pre><code>recall = TP / (TP + FN)\n</code></pre>"},{"location":"user_guide/model_evaluation/#3-f1-score","title":"3. F1 Score:","text":"<p>The F1 Score is the harmonic mean of precision and recall. It provides a balance between the two metrics and is particularly useful when there is an imbalance between the number of alarms and non-alarms in the dataset. A high F1 Score indicates that both precision and recall are high, which is desirable for detecting alarms in messages.</p> <p>Insights:</p> <ul> <li>The F1 Score provides a balance between precision and recall, making it particularly useful when there is an imbalance between the number of alarms and non-alarms in the dataset.</li> <li>A high F1 Score indicates that both precision and recall are high, which is desirable for detecting alarms in messages.</li> </ul> <p>Issues:</p> <ul> <li>The F1 Score assumes equal importance for precision and recall, which may not always be the case. Depending on the application, different trade-offs between precision and recall might be necessary.</li> </ul> <p>Mathematically, the F1 Score is defined as:</p> <pre><code>F1 = 2 * (precision * recall) / (precision + recall)\n</code></pre>"},{"location":"user_guide/model_evaluation/#4-estimating-tpr-fpr-fnr-and-tnr","title":"4. Estimating TPR, FPR, FNR, and TNR","text":"<p>Importance:</p> <ul> <li>True Positive Rate (TPR) is equivalent to recall, and it measures the model's ability to identify true alarms.</li> <li>False Positive Rate (FPR) measures the proportion of false alarms among the instances predicted as alarms. Low FPR is desirable to minimize false alarms.</li> <li>False Negative Rate (FNR) measures the proportion of missed alarms among the actual alarms. - Low FNR is desirable to minimize missed alarms.</li> <li>True Negative Rate (TNR) measures the model's ability to identify true non-alarms. High TNR ensures that non-alarm instances are correctly classified, reducing the chance of false alarms.</li> <li>Estimating these rates, along with their average values and error bars, provides a comprehensive evaluation of the model's performance in terms of alarm detection and classification.</li> </ul> <p>Issues:</p> <ul> <li>These rates alone may not provide a complete picture of the model's performance, as they focus on different aspects.</li> <li>Depending on the application and dataset, some rates may be more important than others. For example, in safety-critical scenarios, a higher TPR (recall) might be prioritized over a lower FPR.</li> </ul> <p>By considering these insights and issues, we can make informed decisions about model development and optimization, ensuring that the model effectively detects and classifies alarms in messages.</p>"},{"location":"user_guide/model_evaluation/#estimating-performance-metrics-using-bootstrapping","title":"Estimating Performance Metrics using Bootstrapping","text":"<p>In this section, we will discuss how bootstrapping is used to estimate the average and error bars for performance metrics such as True Positive Rate (TPR), False Positive Rate (FPR), False Negative Rate (FNR), and True Negative Rate (TNR) in the context of multi-label text classification.</p>"},{"location":"user_guide/model_evaluation/#bootstrapping","title":"Bootstrapping","text":"<p>Bootstrapping is a valuable technique for estimating the average and error bars for performance metrics, such as TPR, FPR, FNR, and TNR, in multi-label text classification. It involves generating multiple bootstrap samples from the original data to estimate the distribution of an estimator without making assumptions about the underlying data distribution. This can be particularly useful when the sample size is small or the underlying distribution is unknown.</p> <p>Bootstrapping estimates the true sampling distribution by averaging over the distributions obtained from the bootstrap samples. In the context of performance metrics estimation, we generate multiple bootstrap samples of the true labels and predicted labels, calculate the TPR, FPR, FNR, and TNR for each sample, and estimate the average and error bars for each rate using the bootstrap samples.</p> <p>The main benefits of using bootstrapping include robust estimation and no distribution assumptions, making it suitable for cases with small sample sizes or unknown distributions. However, bootstrapping can be computationally expensive, sensitive to the quality of the original sample, and does not guarantee improved estimation in all cases. In some instances, other techniques like cross-validation or analytical methods might be more appropriate.</p>"},{"location":"user_guide/model_evaluation/#mathematics-behind-bootstrapping","title":"Mathematics behind Bootstrapping","text":"<p>For a given dataset with <code>n</code> samples, the probability of any sample being included in a bootstrap sample is <code>1 - (1 - 1/n)^n</code>, which approaches <code>1 - 1/e</code> (approximately 63.2%) as <code>n</code> goes to infinity. Consequently, about 63.2% of the original samples will be included in each bootstrap sample on average, while the remaining 36.8% will be left out.</p> <p>The main idea behind bootstrapping is to approximate the true sampling distribution by averaging over the distributions obtained from the bootstrap samples. In the context of performance metrics estimation, we generate multiple bootstrap samples of the true labels and predicted labels, and then calculate the TPR, FPR, FNR, and TNR for each sample. Finally, we estimate the average and error bars for each rate using the bootstrap samples.</p> <p>The error bars are calculated using the specified percentile (e.g., 2.5), such that the lower error bar is the value at the lower percentile, and the upper error bar is the value at the higher percentile (100 - percentile).</p>"},{"location":"user_guide/model_evaluation/#benefits-and-limitations","title":"Benefits and Limitations","text":"<p>The main benefits of using bootstrapping to estimate the error bars of different rates are:</p> <ol> <li>Robust estimation: Bootstrapping provides a more robust estimation of the model's performance by taking into account the variability in the performance metrics.</li> <li>No distribution assumptions: Bootstrapping does not require assumptions about the underlying data distribution, making it suitable for cases with small sample sizes or unknown distributions.</li> </ol> <p>However, bootstrapping also has some limitations:</p> <ol> <li>Computationally expensive: Bootstrapping can be computationally expensive, especially when the sample size is large or the number of iterations is high.</li> <li>Sensitive to the quality of the original sample: If the original sample is not representative of the population, the bootstrapped estimates may be biased.</li> <li>No guarantee of improved estimation: Although bootstrapping can provide a more robust estimation of error bars, there is no guarantee that it will always result in better estimates. In some cases, other techniques like cross-validation or analytical methods might be more appropriate.</li> </ol>"},{"location":"user_guide/model_training/","title":"Model Training","text":"<p>This guide explains how to train the <code>TextMultiLabelClassificationModel</code> using the PyTorch Lightning framework. The model is designed for multi-label text classification tasks, and it leverages the Transformers library for the underlying backbone.</p>"},{"location":"user_guide/model_training/#prerequisites","title":"Prerequisites","text":"<p>Before you start, make sure you have installed the required libraries:</p> <pre><code>poetry install\n</code></pre>"},{"location":"user_guide/model_training/#training-script","title":"Training Script","text":"<p>The model training script is responsible for loading the model, data module, and trainer configuration from a YAML file or a checkpoint file. It sets up the trainer with the specified configuration and trains the model using the given dataset.</p> <p>The training script also logs important metrics during training and testing using MLflow. After training, the model is registered, and its URI is printed for easy access.</p>"},{"location":"user_guide/model_training/#running-the-training-script","title":"Running the Training Script","text":"<p>To run the training script, you'll need to provide a YAML configuration file that includes the necessary hyperparameters for the model, data module, and trainer. You can also provide a checkpoint file to start training from an existing checkpoint.</p> <p>To execute the training script, run the following command in your terminal or command prompt:</p> <pre><code>python tmlc/scripts/train.py --file-path /path/to/config_or_checkpoint.yaml [--check-point True|False]\n</code></pre> <p>Replace <code>filename: checkpoint  # file name for the checkpoint</code> in your YAML configuration file or checkpoint file. If you're starting from an existing checkpoint, set the --check-point flag to True.</p>"},{"location":"user_guide/model_training/#example-configuration","title":"Example Configuration","text":"<p>Here's an example configuration file for the LightningModuleConfig. This file includes settings for the trainer, model, data module, loggers, and callbacks.</p> <pre><code># Example configuration file for LightningModuleConfig\ntrainer_config:\n  seed: 42  # seed for reproducibility\n  kwargs:\n      max_epochs: 1  # maximum number of epochs to train for\n      accelerator: cpu  # use CPU for training\n      log_every_n_steps: 5  # log after every 5 steps\n      enable_progress_bar: True  # show progress bar during training\n  mlflow_config:\n    artifact_folder: model/\n    model_path: model/mode.onnx\n    tokenizer_path: model/tokenizer\n    description: \"This is a description\"\n    tags:\n      one: 1\n\n  callbacks:\n      - module: \"pytorch_lightning.callbacks.early_stopping\"  # early stopping callback to stop training if val_loss doesn't improve\n        func: \"EarlyStopping\"\n        kwargs:\n            monitor: val_loss  # monitor validation loss\n            patience: 5  # number of epochs with no improvement after which training will be stopped\n            mode: min  # minimize the monitored quantity\n      - module: \"pytorch_lightning.callbacks.model_checkpoint\"  # model checkpoint callback to save the best model\n        func: \"ModelCheckpoint\"\n        kwargs:\n            dirpath: 'dirpath'  # directory to save the checkpoint\n            filename: checkpoint  # file name for the checkpoint\n            monitor: val_loss  # monitor validation loss\n            mode: min  # minimize the monitored quantity\n            save_top_k: 1  # save the top 1 models\n            save_last: True  # save the last checkpoint\n\n  loggers:\n      - module: \"lightning.pytorch.loggers\"  # logger to log the experiment to MLFlow\n        func: \"MLFlowLogger\"\n        kwargs:\n          experiment_name: experiment_name  # name of the experiment\n          tracking_uri: \"mlruns\"  # URI of the MLFlow server\n      - module: \"lightning.pytorch.loggers\"  # logger to log the experiment to TensorBoard\n        func: \"TensorBoardLogger\"\n        kwargs:\n          save_dir: tensorboard_logdir  # directory to save the logs\n          name: run_name  # name of the run\n\n  lightning_module_config:\n    model_name: TMLC  # name of the model\n    model:\n      pretrained_model:\n        path: bert-base-cased\n      dropout_prob: 0.1  # dropout probability\n      hidden_size: 768  # size of the hidden layer\n      num_classes: 1  # number of classes\n      calculate_predictions:\n        module: \"tmlc.components.predictions\"  # predictions module\n        func: \"calculate_predictions\"  # calculate predictions function\n    calculate_metrics:\n        module: \"tmlc.components.metrics\"  # metrics module\n        func: \"calculate_metrics\"  # calculate metrics function\n        kwargs:\n          n_iterations: 1000 # Number of iterations for the bootstrap error calculation\n          percentile: 2.5 # Lower percentile to calculate symetric error bars in the bootstrap error calculation\n    optimizer:\n        module: \"torch.optim\"  # optimizer module\n        func: \"AdamW\"  # optimizer function\n        kwargs:\n          lr: 0.000001  # learning rate\n    define_loss:\n        module: tmlc.components.loss  # loss module\n        func: bceloss_inverse_frequency_weighted  # loss function\n    predict:\n      module: \"torch\"  # module to use for prediction\n      func: \"sigmoid\"  # function to use for prediction\n    calculate_best_thresholds:\n      module: \"tmlc.components.calculate_best_thresholds\"  # module to use for calculating best thresholds\n      func: \"calculate_best_thresholds\"  # function to use for calculating best thresholds\n      kwargs:\n          vmin: 0.1  # minimum value for the threshold\n          vmax: 0.9  # maximum value for the threshold\n          step: 0.05  # step size for the threshold\n\n  data_module_config:\n    state_file: \"state.bin\" # Location of state file\n    dataset:\n      batch_size: 5 # Batch size for data loader\n      tokenizer_config:\n        model_name: \"bert-base-cased\"\n        path: \"bert-base-cased\"\n        max_length: 128\n        output_keys: [\"input_ids\", \"attention_mask\"]\n        kwargs:\n          add_special_tokens: true\n          padding: \"max_length\"\n          truncation: true\n      kwargs:\n        num_workers: 8 # Number of workers for data loader\n    load_data:\n      module: \"tmlc.components.load_data\"\n      func: \"load_data\"\n      kwargs:\n        file_path: \"data/toxic_multi_class.csv\" # File path for input data\n    process_data:\n      module: \"tmlc.components.process_data\"\n      func: \"process_data\"\n      kwargs:\n        text_column: \"comment_text\" # Column name for input text\n        labels_columns: [\"toxic\"] # List of column names for label(s)\n    split:\n      module: \"tmlc.components.split_data\"\n      func: \"split_data\"\n      kwargs:\n        train_ratio: 0.8 # Train split\n        val_ratio: 0.1 # Validation split\n        test_ratio: 0.1 # Test split\n        random_state: 42 # Random seed for splitting data\n        labels_columns: [\"toxic\"] # List of column names for label(s)\n</code></pre>"},{"location":"user_guide/model_training/#configuration-explanation","title":"Configuration Explanation","text":"<p>The configuration file is organized into several sections:</p> <ol> <li><code>trainer_config</code>: Contains settings for the training process, including seed, maximum epochs, and MLflow configuration.</li> <li><code>callbacks</code>: Configures early stopping and model checkpoint callbacks.</li> <li><code>loggers</code>: Sets up loggers for MLflow and TensorBoard.</li> <li><code>lightning_module_config</code>: Defines the model, optimizer, loss function, and other settings related to the model itself.</li> <li><code>data_module_config</code>: Specifies data-related settings such as dataset, tokenizer, data loading, and data splitting.</li> </ol> <p>Each section includes several parameters, which are briefly explained in the comments in the example configuration above. For more details on each parameter, please refer to the documentation of the respective library or module (e.g., PyTorch Lightning, torch.optim, etc.).</p> <p>When creating your own configuration file, ensure to include all the required sections and parameters, adjusting their values as needed for your specific use case. This configuration file should then be passed to the training script to initialize and train the <code>TextMultiLabelClassificationModel</code>.</p>"},{"location":"user_guide/model_training/#saving-the-model","title":"Saving the Model","text":"<p>In the model training script <code>tmlc/scripts/train.py</code> saves the model, it also registers it to MLflow. The model saving process occurs mainly within the <code>register_model</code> function. This function takes the trained model and the trainer configuration as inputs and logs the model to MLflow. It also sets MLflow tags, creates artifacts, and registers the model.</p>"},{"location":"user_guide/model_training/#training-and-mlflow-registry","title":"Training and mlflow registry","text":"<p>This code snippet shows the main ingridients of <code>tmlc/scripts/train.py</code> script. This how it trains a machine learning model and register it with MLflow, which is a tool for managing and tracking experiments in machine learning. Here's a step-by-step breakdown of what's happening:</p> <ol> <li><code>load_model_data_trainer_config</code> function is called to load a pre-trained model, data module and trainer configuration.</li> <li><code>to_partial_functions_dictionary</code> function is called twice to convert the logger and callback classes into partial functions.</li> <li><code>setup_trainer</code> function is called to set up the trainer object for training the model.</li> <li>A new MLflow run is started using the <code>mlflow.start_run</code> function to log all the relevant information about the training run to the MLflow tracking server.</li> <li>The <code>run_id</code> variable is set to the ID of the current run.</li> <li>The <code>previous</code> run associated with the <code>MLFlowLogger</code> is deleted to ensure that the current run is the only one associated with the logger.</li> <li>The <code>_run_id</code> attribute of the <code>MLFlowLogger</code> is set to the current run ID.</li> <li>The <code>fit</code> method of the <code>trainer</code> object is called to train the model on the provided data.</li> <li>The <code>test</code> method of the <code>trainer</code> object is called to evaluate the trained model on the test data.</li> <li>The <code>register_model</code> function is called to register the trained model with MLflow, and the resulting URI is stored in the <code>model_uri</code> variable.</li> </ol> <p>It is important to note that this code assumes that the MLflow tracking server is set up and running, and that the relevant experiment ID and logging parameters are configured in the config object.</p> <pre><code>model, datamodule, config = load_model_data_trainer_config(file_path=file_path, check_point=check_point)\nloggers = to_partial_functions_dictionary(config.loggers)\ncallbacks = to_partial_functions_dictionary(config.callbacks)\n\ntrainer = setup_trainer(config=config, loggers=loggers, callbacks=callbacks)\n\nwith mlflow.start_run(experiment_id=loggers[\"MLFlowLogger\"].experiment_id) as run:\n    run_id = run.info.run_id\n    loggers[\"MLFlowLogger\"].experiment.delete_run(loggers[\"MLFlowLogger\"]._run_id)\n    loggers[\"MLFlowLogger\"]._run_id = run_id\n\n    trainer.fit(model, datamodule=datamodule)\n    trainer.test(datamodule=datamodule)\n    model_uri = register_model(model, config)\n</code></pre> <p>To understand the saved model continue reading Model Wrapper.</p>"},{"location":"user_guide/model_wrapper/","title":"Model Wrapper","text":"<p>This documentation page provides an extensive overview of the <code>TextMultiLabelClassificationModelWrapperPythonModel</code> class, which is a wrapper for multi-label text classification models in the project.</p>"},{"location":"user_guide/model_wrapper/#overview","title":"Overview","text":"<p>The <code>TextMultiLabelClassificationModelWrapperPythonModel</code> class wraps an ONNX model for multi-label text classification tasks. The class provides methods to predict class probabilities and labels for input text data. It also handles the process of loading the model and tokenizer configurations.</p>"},{"location":"user_guide/model_wrapper/#usage","title":"Usage","text":"<p>To use the <code>TextMultiLabelClassificationModelWrapperPythonModel</code> class, you need to provide the following arguments:</p> <ul> <li><code>model_path</code> (str): Path to the ONNX model file.</li> <li><code>tokenizer_config</code> (TokenizerConfig): Tokenizer configuration object.</li> <li><code>tokenizer_path</code> (str): Path to the tokenizer model file.</li> <li><code>thresholds</code> (torch.Tensor): Threshold values for class probabilities.</li> </ul>"},{"location":"user_guide/model_wrapper/#example","title":"Example","text":"<pre><code>wrapper = TextMultiLabelClassificationModelWrapperPythonModel(\n    model_path=\"path/to/onnx/model\",\n    tokenizer_config=tokenizer_config,\n    tokenizer_path=\"path/to/tokenizer\",\n    thresholds=torch.tensor([0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n)\n</code></pre>"},{"location":"user_guide/model_wrapper/#implementation-details","title":"Implementation Details","text":"<p>The <code>TextMultiLabelClassificationModelWrapperPythonModel</code> class inherits from the <code>mlflow.pyfunc.PythonModel</code> class. This allows for easy integration with MLflow for logging, registering, and serving models.</p> <p>The class uses ONNX Runtime (<code>onnxruntime</code>) for inference, which provides a fast and efficient way to run ONNX models across different platforms and devices.</p> <p>The tokenizer is configured using the TokenizerConfig class. The tokenizer configuration and path are set in the <code>_set_tokenizer_config</code> method.</p>"},{"location":"validation/eda/","title":"EDA","text":"<p>Multi-label text classification is a task in which a message is classified into multiple categories or labels. To achieve accurate classification, it is essential to perform exploratory data analysis (EDA) to gain insights, identify patterns and relationships, and validate assumptions. In this article, we will discuss the important EDA plots and figures for multi-label text classification, including label distribution, co-occurrence matrix, average sentence length distribution, n-gram frequency distribution, sentiment distribution, lexical diversity distribution, correlation between label properties, label similarity matrix, and reduced label embeddings.</p>"},{"location":"validation/eda/#introduction","title":"Introduction:","text":"<p>Multi-label text classification is a complex task that requires an understanding of the relationships between labels and the text features. EDA is a crucial step in this process as it helps to identify potential issues with the dataset and guide feature engineering and model selection decisions. In this article, we will discuss the important EDA plots and figures for multi-label text classification.</p>"},{"location":"validation/eda/#label-distribution","title":"Label Distribution:","text":"<p>The Label Distribution plot shows the distribution of labels in the dataset. It is a histogram that displays the frequency of each label in the dataset. This plot helps to understand the balance of the dataset and identify potential label imbalance. Imbalanced datasets can be a challenge for machine learning models as they tend to favor the majority class and may not be able to learn the minority class well. If the Label Distribution plot shows a significant imbalance in the dataset, one possible solution is to use techniques like data augmentation or sampling methods to balance the dataset and improve model performance.</p> <p> Caption: Distribution of labels in the dataset.</p>"},{"location":"validation/eda/#label-co-occurrence-heatmap","title":"Label Co-Occurrence Heatmap:","text":"<p>The co-occurrence matrix shows the frequency with which pairs of labels occur together in the dataset. Analyzing the co-occurrence matrix can reveal the relationships between different labels and the prevalence of certain label combinations. By examining the co-occurrence matrix, we can also identify potential issues in our data. For instance, if a label is frequently co-occurring with many other labels, it could mean that the label is too broad or ambiguous and may need to be further refined or split into multiple labels. In summary, the co-occurrence matrix provides important insights into the relationships between labels in our dataset and can help guide our decision-making process when building a multi-label classifier.</p> <p> Caption: Heatmap of label co-occurrence in the dataset.</p>"},{"location":"validation/eda/#average-sentence-length-distribution","title":"Average Sentence Length Distribution:","text":"<p>The Average Sentence Length Distribution plot shows the distribution of average sentence lengths for each label. This plot can give insight into how complex the messages are for each label and whether there is a significant difference in the complexity of messages across labels. If the messages for each label have significantly different levels of complexity, this could impact the performance of a multi-label classifier. Additionally, this plot can also help identify labels with messages that have particularly long or short average sentence lengths. This information can be used to identify labels that may need further investigation or refinement to improve the accuracy of the classifier.</p> <p> Caption: Distribution of average sentence lengths for messages with the \"toxic\" label.</p> <p> Caption: Distribution of message lengths for messages with the \"toxic\" label.</p> <p> Caption: Distribution of average sentence lengths for messages with the \"not_toxic\" label.</p> <p> Caption: Distribution of message lengths for messages with the \"not_toxic\" label.</p>"},{"location":"validation/eda/#n-gram-frequency-distribution","title":"N-gram Frequency Distribution:","text":"<p>N-gram frequency distribution is a plot that shows the frequency of occurrence of each n-gram in the text. In the case of a multi-label classifier that uses messages to classify them, n-gram frequency distribution helps in identifying the important n-grams that are relevant to each label. If some labels have similar n-gram frequency distributions, it might indicate that the messages in those labels share some common vocabulary or phrases. On the other hand, if some labels have very distinct n-gram frequency distributions, it might suggest that the messages in those labels are very different in terms of their language and content. Overall, the n-gram frequency distribution plot helps in identifying the important n-grams in the text and their relevance to different labels.</p> <p> Caption: Frequency distribution of 1-grams (triplets of adjacent words) for messages with the \"toxic\" label.</p> <p> Caption: Frequency distribution of 2-grams (triplets of adjacent words) for messages with the \"toxic\" label.</p> <p> Caption: Frequency distribution of 3-grams (triplets of adjacent words) for messages with the \"toxic\" label.</p> <p> Caption: Frequency distribution of 1-grams (triplets of adjacent words) for messages with the \"not_toxic\" label.</p> <p> Caption: Frequency distribution of 2-grams (triplets of adjacent words) for messages with the \"not_toxic\" label.</p> <p> Caption: Frequency distribution of 3-grams (triplets of adjacent words) for messages with the \"not_toxic\" label.</p>"},{"location":"validation/eda/#sentiment-distribution","title":"Sentiment Distribution:","text":"<p>The Sentiment Distribution plot is an important visualization that displays the distribution of sentiment across different labels. A positive sentiment score indicates that the message expresses positive emotions or opinions, while a negative score indicates negative emotions or opinions. Additionally, a score of 0 indicates a neutral sentiment. This plot provides a clear understanding of the sentiment distribution for each label, which can help us identify the types of messages associated with that label and gain insights into the overall sentiment of the dataset. For example, if a particular label has a high proportion of positive sentiment messages, it may indicate that messages associated with that label tend to be more positive in nature. This information can be useful for training a multi-label classifier. Furthermore, the overall sentiment distribution can also help to identify any biases towards certain types of messages, which can impact the accuracy of the model.</p> <p> Caption: Distribution of sentiment scores for messages with the \"toxic\" label.</p> <p> Caption: Distribution of sentiment scores for messages with the \"not_toxic\" label.</p>"},{"location":"validation/eda/#lexical-diversity-distribution","title":"Lexical Diversity Distribution:","text":"<p>Another important visualization in EDA is the Lexical Diversity Distribution plot, which displays the distribution of lexical diversity scores. Lexical diversity is the ratio of unique words to the total number of words in each message, and a higher score indicates a greater variety of vocabulary and potentially more complex language. This visualization provides insights into the language complexity of each label and the level of difficulty of the classification task. However, it is important to note that a high lexical diversity score does not necessarily mean that the language is more difficult to classify, as it could simply reflect the use of more specialized vocabulary in a particular domain. If one label consistently has a higher lexical diversity score compared to others, it could indicate that it is more challenging to classify due to the complexity of its language. Therefore, this plot can be used to gain a better understanding of the language characteristics of each label and potentially identify any labels that may require special attention in the classification process.</p> <p> Caption: Distribution of lexical diversity scores for messages with the \"toxic\" label.</p> <p> Caption: Distribution of lexical diversity scores for messages with the \"not_toxic\" label.</p>"},{"location":"validation/eda/#correlation-between-label-properties","title":"Correlation Between Label Properties:","text":"<p>The Correlation Between Label Properties plot shows the correlation between different label properties, such as message length, sentiment, and lexical diversity, for each label. This plot helps us understand how label properties are related and identify any unique or shared correlations between labels. Interpreting the Correlation Between Label Properties plot can provide valuable insights into the relationships between different label properties. For example, if there is a high positive correlation between message length and lexical diversity score for a particular label, it suggests that longer messages are likely to have higher lexical diversity. On the other hand, if there is a high negative correlation between average sentence length and sentiment score for a particular label, it suggests that shorter sentences are likely to have more positive sentiment. Understanding the relationships between different label properties can be useful for improving the accuracy of a multi-label classifier by improving the feature representation for the classifier or sentiment analysis component.</p> <p> Caption: Heatmap of correlation coefficients between different label properties for messages with the \"toxic\" label.</p> <p> Caption: Heatmap of correlation coefficients between different label properties for messages with the \"not_toxic\" label.</p>"},{"location":"validation/eda/#label-similarity-matrix","title":"Label Similarity Matrix:","text":"<p>The Label Similarity Matrix plot visualizes the pairwise similarity between different labels in the dataset based on their feature representations. This plot helps us understand how labels are related and identify any clusters or patterns in the label space. It can also help identify potential label redundancy and inform label merging or splitting decisions. If two labels have a high similarity score, it may indicate that they are redundant and can be merged into a single label without significant loss of information. Conversely, if two labels have a low similarity score, it may indicate that they are distinct and should be kept separate.</p> <p> Caption: Heatmap of label similarity scores based on feature representations.</p>"},{"location":"validation/eda/#reduced-label-embeddings","title":"Reduced Label Embeddings","text":"<p>The Reduced Label Embeddings plot is a visualization of the low-dimensional embeddings of the labels obtained through dimensionality reduction techniques such as PCA or t-SNE. This plot helps to identify any patterns or clusters in the label space and visualize the relationships between the different labels in a 2D or 3D space.</p> <p>By evaluating the Reduced Label Embeddings plot, we can gain a better understanding of the distribution of the labels and make informed decisions during the feature engineering and model selection phases of the multi-label classification task.</p> <p>The Reduced Label Embeddings plot represents a reduced dimensional representation of the original label embeddings learned by the BERT model. This plot can be used to analyze and understand how the BERT model has learned to represent and cluster the different labels in the reduced dimensional space.</p> <p>PCA and t-SNE are two popular techniques used for dimensionality reduction. In the PCA plot, we can observe the direction and strength of the correlations between the different label embeddings. The t-SNE plot, on the other hand, highlights the non-linear structure of the embeddings, enabling us to visualize the clusters and their separability.</p> <p>Analyzing the Reduced Label Embeddings plot can provide valuable insights into the effectiveness of the BERT model in classifying the messages based on their multi-label classification. If the labels are well-separated in the reduced dimensional space, it indicates that the BERT model has learned a meaningful representation of the labels and can efficiently distinguish between them. However, if the labels are close to each other, it can indicate that the model is having difficulty distinguishing between them, and more training or fine-tuning may be necessary.</p> <p> Caption: Scatterplot of label embeddings in a reduced dimensional space obtained through.</p>"},{"location":"validation/eda/#initial-modeling-analysis","title":"Initial modeling analysis","text":"Task Model Classifier Hyperparameters Metrics toxic bert-base-cased RandomForestClassifier {\"bootstrap\": true, \"ccp_alpha\": 0.0, \"class_weight\": null, \"criterion\": \"gini\", \"max_depth\": 2, \"max_features\": \"sqrt\", \"max_leaf_nodes\": null, \"max_samples\": null, \"min_impurity_decrease\": 0.0, \"min_samples_leaf\": 1, \"min_samples_split\": 2, \"min_weight_fraction_leaf\": 0.0, \"n_estimators\": 50, \"n_jobs\": null, \"oob_score\": false, \"random_state\": null, \"verbose\": 0, \"warm_start\": false} {\"val_f1\": 0.8000000000000002, \"test_f1\": 0.9090909090909091}"},{"location":"validation/eda/#summary","title":"Summary","text":"<p>In conclusion, analyzing the Reduced Label Embeddings in PCA and t-SNE spaces can help in understanding the distribution of the labels in the reduced dimensional space and can provide valuable insights into the effectiveness of the BERT model in classifying the messages based on their multi-label classification.</p> <p>Overall, by evaluating the different plots and figures presented in this EDA, we can gain a comprehensive understanding of the multi-label text classification dataset, including label distribution, label co-occurrence, language complexity, label similarity, and the effectiveness of the BERT model. These insights can help guide our decisions in feature engineering and model selection and ultimately lead to the development of more accurate and effective multi-label classifiers.</p> <p>It is worth noting that the list of plots and figures presented in this EDA is not exhaustive, and there may be other plots and analyses that can provide additional insights into the dataset. However, the set of plots and figures presented here covers a wide range of aspects that are crucial for the development of multi-label classifiers for text data.</p> <p>In conclusion, Exploratory Data Analysis is an essential step in multi-label text classification, and the plots and figures presented in this article can help data scientists gain a better understanding of their dataset, validate assumptions, and make informed decisions in the feature engineering and model selection phases.</p>"},{"location":"validation/model/","title":"Transformer Model for Text Multi-Label Classification","text":"<p>BERT (Bidirectional Encoder Representations from Transformers) for example is a powerful pre-trained language model that has demonstrated outstanding performance in a wide range of natural language processing tasks. In this documentation, we'll discuss why using Transformer models for multi-label classification is a good idea and how to optimize the model for the best performance.</p>"},{"location":"validation/model/#why-transformer-for-multi-label-classification","title":"Why Transformer for Multi-Label Classification?","text":"<ol> <li> <p>Contextualized Word Representations: Transformers learns contextualized word representations, which enables it to understand the meaning of a word based on its surrounding context. This is crucial for multi-label classification, as the model needs to comprehend the context of each label within a given input.</p> </li> <li> <p>Transfer Learning: Transformers are pre-trained on a large corpus of text, which allows it to learn general language understanding. This knowledge can be fine-tuned on specific tasks, like multi-label classification, with smaller amounts of labeled data, thus reducing the need for extensive labeled datasets.</p> </li> <li> <p>Bidirectional Context:</p> </li> </ol> <p>Transformers are designed to process input text in both directions, capturing context from the left and the right. This bidirectional context allows the model to better understand the relationships between labels in multi-label classification tasks.</p>"},{"location":"validation/model/#optimizer-and-loss-function","title":"Optimizer and Loss Function","text":"<p>When fine-tuning a Transformer model for multi-label classification, it's essential to choose the right optimizer and loss function. In this section, we will discuss different options for optimizers and loss functions, and suggest experiments to validate the effectiveness of these strategies.</p>"},{"location":"validation/model/#optimizers","title":"Optimizers","text":"<ol> <li> <p>AdamW: AdamW is an extension of the popular Adam optimizer with weight decay (L2 regularization). It's the recommended optimizer for fine-tuning Transformer models, as it effectively combines the advantages of Adam with weight decay, leading to better generalization performance.</p> </li> <li> <p>Adam: Adam is a widely-used optimizer that combines the benefits of momentum and adaptive learning rates. While it's not specifically designed for Transformer fine-tuning, it can be a viable option to explore.</p> </li> <li> <p>SGD with Momentum: Stochastic Gradient Descent (SGD) with momentum is a classic optimization algorithm that can also be considered for fine-tuning Transformer models. However, it may require more careful tuning of hyperparameters like the learning rate and momentum.</p> </li> </ol>"},{"location":"validation/model/#optimizer-experiments","title":"Optimizer Experiments","text":"<p>To evaluate the effectiveness of different optimizers, you can perform the following experiments:</p> <ol> <li>Train the Transformer model with each optimizer (AdamW, Adam, and SGD with Momentum) using the same learning rate and other hyperparameters.</li> <li>Monitor the training loss, validation loss, and performance metrics (e.g., F1 score) over time.</li> <li>Compare the results to determine which optimizer yields the best performance and convergence rate.</li> </ol>"},{"location":"validation/model/#loss-functions","title":"Loss Functions","text":"<ol> <li> <p>Binary Cross-Entropy Loss (BCELoss): The most commonly used loss function for multi-label classification is BCELoss, as it can handle multiple labels per instance. It measures the difference between predicted probabilities and true binary labels for each class.</p> </li> <li> <p>Weighted Binary Cross-Entropy Loss (Weighted BCELoss): This is an extension of BCELoss that incorporates class weights. It's useful when dealing with imbalanced datasets, as it assigns higher weights to underrepresented classes, thus helping the model pay more attention to those classes.</p> </li> </ol>"},{"location":"validation/model/#loss-function-experiments","title":"Loss Function Experiments","text":"<p>To evaluate the effectiveness of different loss functions, you can perform the following experiments:</p> <ol> <li>Train the Transformer model using BCELoss and Weighted BCELoss, keeping the optimizer and other hyperparameters constant.</li> <li>Monitor the training loss, validation loss, and performance metrics (e.g., F1 score) over time.</li> <li>Compare the results to determine which loss function provides the best performance and handles class imbalance, if present.</li> </ol>"},{"location":"validation/model/#class-weights","title":"Class Weights","text":"<p>Class weights can be used to handle imbalanced datasets by assigning higher weights to underrepresented classes. Here are some strategies for calculating class weights:</p> <ol> <li> <p>Inverse Frequency: Calculate the inverse of the number of samples per class. This gives higher weight to underrepresented classes.</p> </li> <li> <p>Inverse Square Root Frequency: Calculate the inverse of the square root of the number of samples per class. This approach provides a smoother distribution of weights, reducing the impact of extreme imbalances.</p> </li> <li> <p>Normalized Weights: Normalize the weights calculated using either inverse frequency or inverse square root frequency, so that their sum is equal to the total number of classes. This ensures that the overall contribution of class weights to the loss function remains constant.</p> </li> </ol>"},{"location":"validation/model/#class-weight-experiments","title":"Class Weight Experiments","text":"<p>To evaluate the effectiveness of different class weight strategies, you can perform the following experiments:</p> <ol> <li>Train the Transformer model using Weighted BCELoss with each class weight strategy (Inverse Frequency, Inverse Square Root Frequency, and Normalized Weights), keeping the optimizer and other hyperparameters constant.</li> <li>Monitor the training loss, validation loss, and performance metrics (e.g., F1 score) over time.</li> <li>Compare the results to determine which class weight strategy provides the best performance and handles class imbalance effectively.</li> </ol>"},{"location":"validation/model/#threshold-optimization","title":"Threshold Optimization","text":"<p>For multi-label classification, we need to set a threshold for each label to determine if the label is present or not. Here are some strategies for threshold optimization:</p> <ol> <li> <p>Fixed Threshold: Use a fixed threshold (e.g., 0.5) for all labels. This is a simple approach but may not be optimal for all labels.</p> </li> <li> <p>Per-Label Threshold: Optimize the threshold for each label separately on the validation set to maximize a performance metric like the F1 score.</p> </li> <li> <p>Optimization Algorithm: Use an optimization algorithm, like grid search or Bayesian optimization, to search for the best set of thresholds on the validation set that maximize a performance metric.</p> </li> </ol>"},{"location":"validation/model/#threshold-optimization-experiments","title":"Threshold Optimization Experiments","text":"<p>To evaluate the effectiveness of different threshold optimization strategies, you can perform the following experiments:</p> <ol> <li>Train the Transformer model using the same optimizer, loss function, and hyperparameters.</li> <li>Calculate the logits for the validation dataset.</li> <li>Convert logits to probabilities using the sigmoid function.</li> <li>Apply each threshold optimization strategy (Fixed Threshold, Per-Label Threshold, and Optimization Algorithm) and calculate the performance metrics (e.g., F1 score) for each strategy.</li> <li>Compare the results to determine which threshold optimization strategy provides the best performance.</li> <li>Class weights can be calculated once on the training set or on each batch.</li> </ol>"},{"location":"validation/model/#estimating-the-number-of-records-for-multi-label-classification-using-transformer","title":"Estimating the Number of Records for Multi-Label Classification using Transformer","text":"<p>Estimating the number of records required to train a Transformer model for multi-label classification can be challenging. In this section, we will discuss different strategies to estimate the number of records and suggest experiments to validate these strategies.</p>"},{"location":"validation/model/#strategies-for-estimating-the-number-of-records","title":"Strategies for Estimating the Number of Records","text":"<ol> <li> <p>Transfer Learning Advantage: Leverage the fact that Transformer already possesses significant general language understanding. Start with a smaller number of records and gradually increase the number of records in your experiments.</p> </li> <li> <p>Label Distribution: Analyze the distribution of labels and identify rare or underrepresented labels. Increase the number of records for such labels to achieve better performance.</p> </li> <li> <p>Task Complexity: Consider the complexity of the task when estimating the number of records. If the task has clear patterns and relationships between labels, you might need fewer records. On the other hand, if the task is more complex with subtle or intricate relationships, you might require more records.</p> </li> <li> <p>Data Quality: Take into account the quality of your data. High-quality, clean data with minimal noise can lead to better model performance with fewer records. Conversely, noisy data or data with inconsistencies might require more records.</p> </li> <li> <p>Rule of Thumb: As a rule of thumb, you can consider having at least 10-20 times the number of records as the number of labels for a starting point. This can serve as a baseline for further experimentation.</p> </li> </ol>"},{"location":"validation/model/#evaluating-task-complexity","title":"Evaluating Task Complexity","text":"<p>Evaluating task complexity for a Transformer model in multi-label classification of messages is important to understand the required resources, data, and model architecture. Task complexity is a measure of the difficulty and intricacy of the relationships between input messages and labels. In this section, we will discuss various factors and methods to evaluate task complexity for Transformer-based multi-label classification of messages.</p>"},{"location":"validation/model/#factors-affecting-task-complexity","title":"Factors Affecting Task Complexity","text":"<ol> <li> <p>Number of Labels: A higher number of labels often leads to increased task complexity, as the model needs to learn more intricate relationships between input messages and labels.</p> </li> <li> <p>Label Distribution: Imbalanced distribution of labels can make the task more complex, as the model might struggle to learn patterns for underrepresented classes.</p> </li> <li> <p>Label Relationships: The presence of hierarchical relationships, label correlations, or label dependencies can increase task complexity, as the model must learn these relationships to make accurate predictions.</p> </li> <li> <p>Linguistic Complexity: The complexity of the language used in the messages, such as the use of idioms, slang, or domain-specific terminology, can affect task complexity. Understanding and generalizing from such language constructs may require more training data and a deeper understanding of the context.</p> </li> </ol>"},{"location":"validation/model/#methods-to-evaluate-task-complexity","title":"Methods to Evaluate Task Complexity","text":"<ol> <li> <p>Exploratory Data Analysis (EDA): Perform EDA on the dataset to gain insights into label distribution, relationships, and linguistic complexity. Analyze the frequency of different labels, co-occurrence of labels, and the distribution of message lengths. This can help you understand the complexity of relationships between input messages and labels.</p> </li> <li> <p>Text Embeddings Visualization: Use Transformer's pre-trained embeddings to convert input messages into fixed-size vectors. Apply dimensionality reduction techniques, such as PCA or t-SNE, to visualize the embeddings in a lower-dimensional space. If the embeddings corresponding to different labels are well-separated, the task complexity might be lower. On the other hand, if embeddings overlap significantly, the task complexity might be higher.</p> </li> <li> <p>Benchmark Models: Train and evaluate benchmark models, such as logistic regression, decision trees, or support vector machines, using features extracted from Transformer's pre-trained embeddings. By comparing the performance of these models, you can gain insights into the complexity of the task and the potential benefit of fine-tuning Transformer for your specific problem.</p> </li> <li> <p>Incremental Fine-tuning: Fine-tune the Transformer model incrementally, starting with a small subset of the data and gradually increasing the number of training examples. Monitor the model's performance and observe how it improves as more data is added. If the performance plateaus quickly, the task complexity might be lower. On the other hand, if the performance keeps improving with more data, the task complexity might be higher.</p> </li> </ol> <p>In summary, evaluating task complexity for a Transformer model in multi-label classification of messages involves analyzing various factors and applying different techniques. By considering these factors and conducting experiments, you can gain a better understanding of the task complexity and tailor your approach accordingly.</p>"}]}